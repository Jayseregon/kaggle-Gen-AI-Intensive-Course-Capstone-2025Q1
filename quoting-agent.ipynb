{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0487523",
   "metadata": {},
   "source": [
    "# GenAI Quoting Agent for AI & Geospatial Projects\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a GenAI-powered quoting agent for AI-infused geospatial projects. The agent automates requirements extraction, services recommendation, quoting, and QA review using multi-agent orchestration, vector search, and structured output.\n",
    "\n",
    "**Use Case:**\n",
    "\n",
    "- Automate the process of generating accurate, transparent, and auditable quotes for complex geospatial/AI projects.\n",
    "- Reduce manual effort, improve consistency, and enable rapid iteration for both technical and non-technical stakeholders.\n",
    "\n",
    "**GenAI Capabilities Demonstrated:**\n",
    "\n",
    "- Structured output/JSON mode (Pydantic schemas, controlled generation)\n",
    "- Agents (CrewAI multi-agent orchestration)\n",
    "- Embeddings & Vector Store (Google embeddings + Chroma for RAG)\n",
    "- Retrieval Augmented Generation (RAG) for internal service catalog\n",
    "- Function Calling (custom tools for quote calculation and Google search)\n",
    "- GenAI Evaluation (QA agent for quote review)\n",
    "\n",
    "**Workflow Overview:**\n",
    "\n",
    "1. Load and preprocess service standards and client persona data.\n",
    "2. Create a vector store of internal service offerings for semantic search.\n",
    "3. Simulate a client conversation and extract requirements using an agent.\n",
    "4. Research best practices and technologies using GenAI-powered web search.\n",
    "5. Retrieve and recommend relevant internal services using vector search (RAG).\n",
    "6. Calculate a detailed quote, including custom work and overhead, using a function-calling agent.\n",
    "7. Run a QA agent to review the quote for accuracy, consistency, and alignment with requirements.\n",
    "8. Generate a Statement of Work (SOW) document from the structured quote and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Jeremie Bitsch\n",
    "\n",
    "**Date:** [2025-04-17]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4cf45",
   "metadata": {},
   "source": [
    "## Setting the environment\n",
    "\n",
    "The following first cells are dedicated to setting up the environment in a way that ensures compatibility both on Kaggle and in local development. We start by defining a utility function `is_kaggle_environment()` which detects whether the code is running inside a Kaggle notebook.\n",
    "\n",
    "Depending on the environment, the notebook then handles package installation, API key loading, and dataset access differently.\n",
    "\n",
    "Additionally, a utility function `delay_before_task()` is defined to introduce a short pause between tasks execution in the crews, in addition to setting `max_rpm`. This is particularly useful for avoiding API rate limits when using Google Gemini on the free tier.\n",
    "\n",
    "By choosing to structure the environment setup in this way, the notebook ensures reproducibility and smooth execution regardless of where it is run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183df951",
   "metadata": {},
   "source": [
    "#### `is_kaggle_environment()` utility function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dca4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Utility function to detect if the code is running in a Kaggle notebook environment\n",
    "def is_kaggle_environment():\n",
    "    \"\"\"Detect if running in a Kaggle notebook environment.\"\"\"\n",
    "    # Check for Kaggle-specific paths\n",
    "    if os.path.exists(\"/kaggle/input\") or os.path.exists(\"/kaggle/working\"):\n",
    "        return True\n",
    "\n",
    "    # In case the first check fails, second check for Kaggle environment variables\n",
    "    if os.environ.get(\"KAGGLE_URL_BASE\") or os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dc71a",
   "metadata": {},
   "source": [
    "#### Package installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd4255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if is_kaggle_environment():\n",
    "    !pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
    "    !pip install -qU crewai chromadb langchain langchain-chroma langchain-google-genai langgraph-prebuilt google-genai google-generativeai\n",
    "\n",
    "    import kagglehub\n",
    "    from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b09b94",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045fdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not in Kaggle, load environment variables from a .env file\n",
    "if not is_kaggle_environment():\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c6eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayseregon/programming/kaggle/kaggle-Gen-AI-Intensive-Course-Capstone-2025Q1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Type, Optional, Literal\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from datetime import date\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import markdown as md\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from textwrap import dedent\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d50791",
   "metadata": {},
   "source": [
    "#### API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86beb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Google API key depending on the environment\n",
    "if is_kaggle_environment():\n",
    "    # from kaggle secrets\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "else:\n",
    "    # from .env file\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f757f",
   "metadata": {},
   "source": [
    "#### `delay_before_task()` utility function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a utility function to use as callback in the crews, introduce delays between tasks\n",
    "# This is useful to avoid hitting the API rate limits when using Gemini free tier\n",
    "def delay_before_task() -> None:\n",
    "    delay_seconds = 10\n",
    "    print(\n",
    "        f\"\\\\n--- Task Completed. Waiting {delay_seconds} seconds before next task --- \\\\n\"\n",
    "    )\n",
    "    time.sleep(delay_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f1f12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1 - Data Preparation\n",
    "\n",
    "### 1.1 - Load the internal service standards and client persona data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c09e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets depending on the environment\n",
    "if is_kaggle_environment():\n",
    "    # Download supporting dataset from Kaggle\n",
    "    json_data = kagglehub.dataset_download(\n",
    "        \"jeremiebitsch/capstone-2025-quote-standards-v2\"\n",
    "    )\n",
    "    standards_dataset_path = Path(json_data) / \"quote-standards.json\"\n",
    "    client_personas_path = Path(json_data) / \"client-personas.json\"\n",
    "else:\n",
    "    # Use data files directly for local development\n",
    "    standards_dataset_path = Path(\"capstone-2025-quote-standards-v2.json\")\n",
    "    client_personas_path = Path(\"capstone-2025-client-personas-v2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2370e",
   "metadata": {},
   "source": [
    "### 1.2 - Flatten the service catalog for vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727bc532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON dataset\n",
    "with standards_dataset_path.open(\"r\") as f:\n",
    "    standards_dataset = json.load(f)\n",
    "\n",
    "\n",
    "# Utility function to flatten and transform the json dataset\n",
    "def _flatten_json_data(data: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"Convert nested JSON service data into a list of Document objects.\"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for service_category, services in data[\"services\"].items():\n",
    "        for service in services:\n",
    "            # Create metadata keeping relevant fields only\n",
    "            metadata = {\n",
    "                \"uid\": str(uuid.uuid4()),\n",
    "                \"name\": service.get(\"name\", \"\"),\n",
    "                \"label\": service.get(\"label\", \"\"),\n",
    "                \"price\": service.get(\"price\", 0),\n",
    "                \"parentCategory\": service_category,\n",
    "                \"subCategory\": service.get(\"category\", \"\"),\n",
    "                \"developmentTime\": service.get(\"developmentTime\", 0),\n",
    "                \"description\": service.get(\"description\", \"\"),\n",
    "            }\n",
    "\n",
    "            # Combine fields to create the document page content\n",
    "            page_content = f\"\"\"\n",
    "            Service: {service.get(\"label\", \"\")}\n",
    "            Parent Category: {service_category}\n",
    "            Sub Category: {service.get(\"category\", \"\")}\n",
    "            Price: ${service.get(\"price\", 0)}\n",
    "            Description: {service.get(\"description\", \"\")}\n",
    "            Development Time: {service.get(\"developmentTime\", 0)} hours\n",
    "            Deliverables: {\", \".join(service.get(\"deliverables\", []))}\n",
    "            Use Cases: {\", \".join(service.get(\"useCases\", []))}\n",
    "            \"\"\"\n",
    "\n",
    "            # Create a Document object\n",
    "            doc = Document(page_content=page_content, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Create the document objects from json input\n",
    "docs = _flatten_json_data(standards_dataset)\n",
    "\n",
    "# Inspect the number of documents created\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4421a72",
   "metadata": {},
   "source": [
    "### 1.3 - Apply text splitting only if necessary.\n",
    "\n",
    "This step is mainly included to illustrate the usual steps before adding documents to the vectorstore. In our case, each are below 1000 characters and text splitting can be ignored.\n",
    "Still, we keep this step to ensure consistency with the rest of the code and to allow for future changes in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37efa6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "            Service: Satellite Image Analysis with AI\n",
      "            Parent Category: geospatialAI\n",
      "            Sub Category: ai\n",
      "            Price: $1200\n",
      "            Description: Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection\n",
      "            Development Time: 36 hours\n",
      "            Deliverables: Custom trained model on your imagery, Web dashboard for uploading and analyzing new images, API for automated processing pipelines, Technical documentation and training\n",
      "            Use Cases: Agricultural crop health monitoring, Urban development tracking, Disaster damage assessment, Environmental change monitoring\n",
      "            ' metadata={'uid': 'c16a8992-4a04-4e1b-b989-5fde3a679ead', 'name': 'SatelliteImageAnalysis', 'label': 'Satellite Image Analysis with AI', 'price': 1200, 'parentCategory': 'geospatialAI', 'subCategory': 'ai', 'developmentTime': 36, 'description': 'Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection'}\n"
     ]
    }
   ],
   "source": [
    "# Collect all documents lenghts\n",
    "docs_length = [len(doc.page_content) for doc in docs]\n",
    "\n",
    "# Split documents only if they are not all under 100 chars\n",
    "if not all(i < 1000 for i in docs_length):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Sample doc for sanity check\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc56421",
   "metadata": {},
   "source": [
    "## Step 2 - Vector Store Creation\n",
    "\n",
    "Create a Chroma vector store using Google Generative AI embeddings for semantic search over the service catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2162b7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the embedding function\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\", google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "chroma_store = Chroma(\n",
    "    collection_name=\"kaggle_capstone\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "\n",
    "# Collect the ids from metadata\n",
    "ids = [doc.metadata[\"uid\"] for doc in docs]\n",
    "\n",
    "# Add documents to the store\n",
    "chroma_store.add_documents(documents=docs, ids=ids)\n",
    "\n",
    "# For sanity, check the collection is not empty\n",
    "chroma_store._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea1023",
   "metadata": {},
   "source": [
    "## Step 3 - Load the simulated conversation between the user and the agent\n",
    "\n",
    "For testing and demonstration purposes, 3 differents personas are available:\n",
    "\n",
    "- `wildfire_watch_na`: a persona for a wildfire watch service in North America\n",
    "- `urban_traffic_ai`: a persona for an urban traffic monitoring and prediction service\n",
    "- `agri_insights`: a persona for crop health monitoring and yield prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b33738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'agent',\n",
       "  'message': 'Hello! Thank you for contacting us. Could you please tell me your name and the company you represent?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"Hi, my name is Jane Smith, and I'm from Wildfire Watch North America.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Nice to meet you, Jane. How can we help Wildfire Watch North America today?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"We're looking for a quote for a geospatial project focused on wildfire monitoring across Canada and the US. We need to build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"Okay, that's a critical Geospatial AI project. What specific data sources are you considering for both the historical database and the ongoing monitoring?\"},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'We plan to use satellite imagery like MODIS, VIIRS, Landsat, and Sentinel, along with historical fire perimeter data, weather data, and potentially topographical data.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"That's a robust set of sources. For the predictive aspect, what key factors are you hoping the AI model will help predict? Fire spread, ignition risk, or something else?\"},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'Primarily ignition risk and potential spread patterns. We also need near real-time monitoring capabilities.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'What is the desired output for this system? A web dashboard for visualization, an API for integration, or detailed reports?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'We need a web dashboard for our analysts and an API for integrating alerts into our existing systems. Regular risk assessment reports would also be valuable.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Do you have specific performance requirements for the predictive model, perhaps in terms of accuracy or lead time for predictions?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"We're aiming for high accuracy in identifying high-risk zones, maybe around 85-90% precision, and timely alerts for new ignitions.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Understood. And what is your approximate budget for this initial phase, covering the database build, data gathering, and initial model development?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'Our budget is around $40,000 for this phase.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"Thank you, Jane. That gives us a clear picture. We'll start developing a proposal for Wildfire Watch North America.\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function to load the conversation from the dataset\n",
    "def load_conversation(\n",
    "    persona_id: Literal[\"wildfire_watch_na\", \"urban_traffic_ai\", \"agri_insights\"],\n",
    "    dataset_path: str | Path,\n",
    ") -> List[Dict[str, str]]:\n",
    "    if not isinstance(dataset_path, Path):\n",
    "        dataset_path = Path(dataset_path)\n",
    "\n",
    "    with dataset_path.open(\"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for persona in data[\"personas\"]:\n",
    "        if persona[\"id\"] == persona_id:\n",
    "            return persona[\"conversation\"]\n",
    "\n",
    "\n",
    "conversation = load_conversation(\n",
    "    persona_id=\"wildfire_watch_na\", dataset_path=client_personas_path\n",
    ")\n",
    "\n",
    "# Display the conversation\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd401a",
   "metadata": {},
   "source": [
    "## Step 4 - Define the data models for the crews' tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4298e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalRequirements(BaseModel):\n",
    "    accuracy: Optional[str] = Field(\n",
    "        None, description=\"Required accuracy or performance metrics.\"\n",
    "    )\n",
    "    outputs: Optional[List[str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of desired output formats (e.g., 'Web dashboard', 'API', 'Report').\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RequirementsOutput(BaseModel):\n",
    "    client_name: Optional[str] = Field(None, description=\"Name of the client contact.\")\n",
    "    client_company: Optional[str] = Field(\n",
    "        None, description=\"Name of the client's company.\"\n",
    "    )\n",
    "    project_type: Optional[str] = Field(\n",
    "        None, description=\"Type of project (e.g., Geospatial AI, Data Automation).\"\n",
    "    )\n",
    "    main_objective: Optional[str] = Field(\n",
    "        None, description=\"The primary goal of the project.\"\n",
    "    )\n",
    "    data_sources: List[str] = Field(\n",
    "        default_factory=list, description=\"List of data sources mentioned.\"\n",
    "    )\n",
    "    features_needed: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of required features or functionalities.\",\n",
    "    )\n",
    "    technical_requirements: Optional[TechnicalRequirements] = Field(\n",
    "        None, description=\"Specific technical needs.\"\n",
    "    )\n",
    "    budget: Optional[str] = Field(None, description=\"Client's stated budget or range.\")\n",
    "    timeline_expectation: Optional[str] = Field(\n",
    "        None, description=\"Any mention of desired timeline or deadlines.\"\n",
    "    )\n",
    "    potential_risks: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Potential challenges or risks identified from the conversation.\",\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise Markdown summary (1-2 paragraphs) highlighting the core project request and key constraints.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RecommendedService(BaseModel):\n",
    "    name: str = Field(..., description=\"Internal name/ID of the service.\")\n",
    "    label: str = Field(..., description=\"Customer-facing label of the service.\")\n",
    "    price: float = Field(..., description=\"Price of the service.\")\n",
    "    development_time_hours: int = Field(\n",
    "        ..., description=\"Estimated development time in hours.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ServicePackage(BaseModel):\n",
    "    name: str = Field(..., description=\"A descriptive name for the proposed package.\")\n",
    "    services_included: List[str] = Field(\n",
    "        ..., description=\"List of labels of the services included in the package.\"\n",
    "    )\n",
    "    total_estimated_price: float = Field(\n",
    "        ..., description=\"Total calculated price for the package.\"\n",
    "    )\n",
    "    total_estimated_time_hours: int = Field(\n",
    "        ..., description=\"Total calculated development time in hours for the package.\"\n",
    "    )\n",
    "    notes: Optional[str] = Field(\n",
    "        None, description=\"Brief notes about the package coverage.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class BudgetAnalysis(BaseModel):\n",
    "    client_budget: Optional[str] = Field(\n",
    "        ..., description=\"The client's budget extracted from requirements.\"\n",
    "    )\n",
    "    estimated_cost: float = Field(\n",
    "        ..., description=\"The total estimated cost of the proposed package.\"\n",
    "    )\n",
    "    within_budget: bool = Field(\n",
    "        ..., description=\"Whether the estimated cost is within the client's budget.\"\n",
    "    )\n",
    "    comments: Optional[str] = Field(None, description=\"Comments on the budget fit.\")\n",
    "\n",
    "\n",
    "class ServiceAnalysisOutput(BaseModel):\n",
    "    recommended_services: List[RecommendedService] = Field(\n",
    "        ..., description=\"List of relevant services found in the vector store.\"\n",
    "    )\n",
    "    service_package: ServicePackage = Field(\n",
    "        ..., description=\"Details of the proposed service package.\"\n",
    "    )\n",
    "    customization_notes: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of requirements potentially needing custom work.\",\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Markdown explanation justifying the service selection, cost/time, budget comparison, and customization needs.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class WebResearchOutput(BaseModel):\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"Brief overview of key findings regarding best practices and technologies.\",\n",
    "    )\n",
    "    best_practices_insights: str = Field(\n",
    "        ...,\n",
    "        description=\"Actionable recommendations and notes on relevant tools/libraries found.\",\n",
    "    )\n",
    "    potential_challenges: Optional[str] = Field(\n",
    "        None, description=\"Potential challenges identified from search results, if any.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteCalculationDetails(BaseModel):\n",
    "    \"\"\"Detailed breakdown of the quote price calculation.\"\"\"\n",
    "\n",
    "    sum_base_service_prices: float = Field(\n",
    "        ..., description=\"Sum of prices for all recommended standard services.\"\n",
    "    )\n",
    "    total_estimated_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"Total estimated hours including base, custom work, and lifecycle.\",\n",
    "    )\n",
    "    base_hourly_rate: int = Field(\n",
    "        ..., description=\"The hourly rate used for development cost calculation.\"\n",
    "    )\n",
    "    additional_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"Hours allocated beyond standard service hours (for custom work, lifecycle, etc.).\",\n",
    "    )\n",
    "    additional_development_cost: float = Field(\n",
    "        ..., description=\"Calculated cost for the additional hours.\"\n",
    "    )\n",
    "    subtotal_before_overhead: float = Field(\n",
    "        ...,\n",
    "        description=\"Total cost before applying the overhead buffer (Base Services + Additional Development).\",\n",
    "    )\n",
    "    overhead_buffer_percentage: float = Field(\n",
    "        ..., description=\"The overhead buffer percentage applied.\"\n",
    "    )\n",
    "    overhead_amount: float = Field(\n",
    "        ..., description=\"The calculated monetary amount of the overhead buffer.\"\n",
    "    )\n",
    "    final_total_price: float = Field(\n",
    "        ..., description=\"The final calculated total price for the quote.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteSummary(BaseModel):\n",
    "    project_name: str\n",
    "    client_name: Optional[str]\n",
    "    client_company: Optional[str]\n",
    "    quote_id: str\n",
    "    quote_date: str\n",
    "    total_price: float\n",
    "    total_estimated_hours: int\n",
    "    estimated_start_date: str\n",
    "    estimated_completion_date: str\n",
    "    estimation_justification: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Brief justification for the estimated hours and overhead buffer applied.\",\n",
    "    )\n",
    "    price_calculation_details: Optional[QuoteCalculationDetails] = Field(\n",
    "        None,\n",
    "        description=\"Detailed breakdown of the price calculation from the QuoteCalculatorTool.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteLineItem(BaseModel):\n",
    "    service_name: str\n",
    "    description: str\n",
    "    price: float\n",
    "    estimated_hours: int\n",
    "\n",
    "\n",
    "class PaymentMilestone(BaseModel):\n",
    "    milestone: str\n",
    "    percentage: int\n",
    "    amount: float\n",
    "    due_date: str\n",
    "\n",
    "\n",
    "class QuoteDetailsOutput(BaseModel):\n",
    "    quote_summary: QuoteSummary\n",
    "    line_items: List[QuoteLineItem]\n",
    "    payment_schedule: List[PaymentMilestone]\n",
    "\n",
    "\n",
    "class QAIssue(BaseModel):\n",
    "    type: str = Field(\n",
    "        ...,\n",
    "        description=\"Category of the issue (e.g., pricing, timeline, scope, requirements_mismatch, clarity, consistency).\",\n",
    "    )\n",
    "    severity: str = Field(..., description=\"Severity level (e.g., high, medium, low).\")\n",
    "    description: str = Field(\n",
    "        ..., description=\"Specific description of the issue found.\"\n",
    "    )\n",
    "    recommendation: str = Field(..., description=\"Specific instruction for correction.\")\n",
    "\n",
    "\n",
    "class QAImprovementSuggestion(BaseModel):\n",
    "    area: str = Field(\n",
    "        ...,\n",
    "        description=\"Area for improvement (e.g., Executive Summary, Timeline Detail).\",\n",
    "    )\n",
    "    suggestion: str = Field(..., description=\"Specific suggestion.\")\n",
    "\n",
    "\n",
    "class QAReviewOutput(BaseModel):\n",
    "    overall_status: str = Field(\n",
    "        ..., description=\"Overall assessment status: 'approved' or 'needs_revision'.\"\n",
    "    )\n",
    "    issues: List[QAIssue] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of identified issues if status is 'needs_revision'.\",\n",
    "    )\n",
    "    improvement_suggestions: List[QAImprovementSuggestion] = Field(\n",
    "        default_factory=list, description=\"Optional suggestions for improvement.\"\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ..., description=\"A brief Markdown summary of the review findings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class AnalysisOutput(BaseModel):\n",
    "    \"\"\"Consolidated output from the Data Analysis Crew.\"\"\"\n",
    "\n",
    "    requirements: RequirementsOutput = Field(\n",
    "        ..., description=\"Extracted client requirements and project context.\"\n",
    "    )\n",
    "    research: WebResearchOutput = Field(\n",
    "        ..., description=\"Web research findings on technologies and best practices.\"\n",
    "    )\n",
    "    service_analysis: ServiceAnalysisOutput = Field(\n",
    "        ..., description=\"Analysis of internal services, pricing, and budget fit.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0c942",
   "metadata": {},
   "source": [
    "## Step 5 - Defining custom tools, and input schemas models, to be used by the agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86da37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreSearchToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Vector Store Search tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"The search query to find relevant services in the vector store.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class GoogleSearhToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Google Search tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ..., description=\"The search query to be used for Google Search.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteCalculatorToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Quote Calculator tool.\"\"\"\n",
    "\n",
    "    estimated_total_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"The final estimated total development hours for the project, adjusted for complexity and custom work.\",\n",
    "    )\n",
    "    recommended_services: List[RecommendedService] = Field(\n",
    "        ..., description=\"List of recommended services with their base prices.\"\n",
    "    )\n",
    "    overhead_buffer_percentage: float = Field(\n",
    "        ...,\n",
    "        description=\"Overhead buffer percentage (e.g., 0.15 for 15%) estimated based on project risk and complexity.\",\n",
    "    )\n",
    "    base_hourly_rate: int = Field(\n",
    "        100, description=\"The base hourly rate for development time.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Defining the Vector Store Search tool, allowing to search services and return consistant results\n",
    "class VectorStoreSearchTool(BaseTool):\n",
    "    name: str = \"VectorStoreSearch\"\n",
    "    description: str = \"A tool to search for relevant services in the vector store.\"\n",
    "    args_schema: Type[BaseModel] = VectorStoreSearchToolInput\n",
    "    vector_store: Chroma\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use the vector store to find relevant documents.\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(f\"Querying Vector Store with: {query}\")\n",
    "            results = self.vector_store.similarity_search_with_score(query, k=10)\n",
    "\n",
    "            # Format results for the agent\n",
    "            formatted_results = []\n",
    "            for doc, score in results:\n",
    "                metadata = doc.metadata\n",
    "                formatted_results.append(\n",
    "                    f\"Result (Score: {score:.4f}):\\n\"\n",
    "                    f\"  Name: {metadata.get('name', 'N/A')}\\n\"\n",
    "                    f\"  Label: {metadata.get('label', 'N/A')}\\n\"\n",
    "                    f\"  Price: ${metadata.get('price', 'N/A')}\\n\"\n",
    "                    f\"  Development Time: {metadata.get('developmentTime', 'N/A')} hours\\n\"\n",
    "                    f\"  Category: {metadata.get('parentCategory', 'N/A')} / {metadata.get('subCategory', 'N/A')}\\n\"\n",
    "                    f\"  Description: {metadata.get('description', 'N/A')}\\n\"\n",
    "                )\n",
    "            if not formatted_results:\n",
    "                return \"No relevant services found in the vector store for the query.\"\n",
    "\n",
    "            return \"\\n-----\\n\".join(formatted_results)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error searching vector store: {e}\")\n",
    "\n",
    "\n",
    "# Defining the Google search tool\n",
    "class GoogleSearchTool(BaseTool):\n",
    "    name: str = \"GoogleSearch\"\n",
    "    description: str = \"A tool to perform Google searches.\"\n",
    "    args_schema: Type[BaseModel] = GoogleSearhToolInput\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use Google Search to find relevant information.\"\"\"\n",
    "\n",
    "        try:\n",
    "            client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "            config_with_search = types.GenerateContentConfig(\n",
    "                tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    "            )\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=query,\n",
    "                config=config_with_search,\n",
    "            )\n",
    "\n",
    "            return [\n",
    "                (\"\\n-----\\n\").join([i.text for i in res.content.parts if i.text])\n",
    "                for res in response.candidates\n",
    "            ][0]\n",
    "        except Exception as e:\n",
    "            return f\"Error performing Google search: {e}\"\n",
    "\n",
    "\n",
    "# Defining the Quote Calculator tool used to ensure consistency in the quote\n",
    "class QuoteCalculatorTool(BaseTool):\n",
    "    name: str = \"QuoteCalculator\"\n",
    "    description: str = (\n",
    "        \"Calculates the final project quote price and provides a detailed breakdown. \"\n",
    "        \"Input: estimated total hours, list of recommended services (with base prices), overhead buffer percentage, and base hourly rate. \"\n",
    "        \"Output: A structured object containing base service costs, additional development costs, overhead amount, and the final total price.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = QuoteCalculatorToolInput\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        estimated_total_hours: int,\n",
    "        recommended_services: List[Any],\n",
    "        overhead_buffer_percentage: float,\n",
    "        base_hourly_rate: int = 100,\n",
    "    ) -> QuoteCalculationDetails:\n",
    "        \"\"\"Calculate the final quote price and return a detailed breakdown.\"\"\"\n",
    "        try:\n",
    "            if not 0.0 <= overhead_buffer_percentage <= 0.3:\n",
    "                print(\n",
    "                    f\"Warning: Clamping overhead buffer {overhead_buffer_percentage}.\"\n",
    "                )\n",
    "                overhead_buffer_percentage = max(\n",
    "                    0.0, min(0.3, overhead_buffer_percentage)\n",
    "                )\n",
    "\n",
    "            # Calculate sum_base_service_prices and sum_base_service_hours\n",
    "            sum_base_service_prices = 0.0\n",
    "            sum_base_service_hours = 0\n",
    "\n",
    "            for service in recommended_services:\n",
    "                price = 0.0\n",
    "                hours = 0\n",
    "                if isinstance(service, dict):\n",
    "                    price = service.get(\"price\", 0.0)\n",
    "                elif hasattr(service, \"price\") and hasattr(\n",
    "                    service, \"development_time_hours\"\n",
    "                ):\n",
    "                    price = getattr(service, \"price\", 0.0)\n",
    "                    hours = getattr(service, \"development_time_hours\", 0)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping unknown service type: {type(service)}\")\n",
    "                    continue\n",
    "                sum_base_service_prices += price\n",
    "                sum_base_service_hours += hours\n",
    "\n",
    "            # Calculate additional hours and cost\n",
    "            additional_hours = max(0, estimated_total_hours - sum_base_service_hours)\n",
    "            additional_development_cost = round(additional_hours * base_hourly_rate, 2)\n",
    "\n",
    "            # Calculate subtotal and overhead amount\n",
    "            subtotal_before_overhead = round(\n",
    "                sum_base_service_prices + additional_development_cost, 2\n",
    "            )\n",
    "            overhead_amount = round(\n",
    "                subtotal_before_overhead * overhead_buffer_percentage, 2\n",
    "            )\n",
    "\n",
    "            # Calculate final price\n",
    "            final_total_price = round(subtotal_before_overhead + overhead_amount, 2)\n",
    "\n",
    "            details = QuoteCalculationDetails(\n",
    "                sum_base_service_prices=sum_base_service_prices,\n",
    "                total_estimated_hours=estimated_total_hours,\n",
    "                base_hourly_rate=base_hourly_rate,\n",
    "                additional_hours=additional_hours,\n",
    "                additional_development_cost=additional_development_cost,\n",
    "                subtotal_before_overhead=subtotal_before_overhead,\n",
    "                overhead_buffer_percentage=overhead_buffer_percentage,\n",
    "                overhead_amount=overhead_amount,\n",
    "                final_total_price=final_total_price,\n",
    "            )\n",
    "\n",
    "            # Prints for verbose output\n",
    "            print(\n",
    "                f\"Calculator Input: Hours={estimated_total_hours}, Services Base={sum_base_service_prices}, Buffer={overhead_buffer_percentage}, Rate={base_hourly_rate}\"\n",
    "            )\n",
    "            print(f\"Calculated Details: {details.model_dump_json(indent=2)}\")\n",
    "\n",
    "            return details\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in QuoteCalculatorTool: {e}\")\n",
    "            print(\n",
    "                f\"Input causing error: hours={estimated_total_hours}, services={recommended_services}, buffer={overhead_buffer_percentage}, rate={base_hourly_rate}\"\n",
    "            )\n",
    "            raise ValueError(f\"Error calculating quote details: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfccdd",
   "metadata": {},
   "source": [
    "## Step 6 - Requirements Extraction & Analysis (Crew 1)\n",
    "\n",
    "The first multi-agent crew pipeline `DataAnalysisCrew`, using CrewAI, processes the client conversation to:\n",
    "\n",
    "- Extract the specific requirements of the project, from the conversation\n",
    "- Research best practices, new technologies and challenges that would better define the project\n",
    "- Query the company vector store for most relevant services matching the requirements\n",
    "- Finally, consolidation of all previous researchs and analysis, crafting a structured output as the foundation for the subsequent quoting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0b21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew 1: Data Analysis & Service Recommendation\n",
    "class DataAnalysisCrew:\n",
    "    def __init__(\n",
    "        self, vector_store: Chroma, conversation: List[Dict[str, str]]\n",
    "    ) -> None:\n",
    "        self.llm = LLM(\n",
    "            model=\"gemini/gemini-2.0-flash\",\n",
    "            temperature=0,\n",
    "            api_key=GOOGLE_API_KEY,\n",
    "            num_retries=5,\n",
    "            request_timeout=80,\n",
    "            seed=42,\n",
    "        )\n",
    "        self.conversation = conversation\n",
    "        self.search_tool = GoogleSearchTool()\n",
    "        self.vector_store_tool = VectorStoreSearchTool(vector_store=vector_store)\n",
    "        self._tasks = {}\n",
    "\n",
    "    # Agent #1 - Context Extraction Agent\n",
    "    def context_extraction_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Project Context and Requirements Extraction Specialist\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Analyze the provided client conversation transcript. Identify and extract key information including:\n",
    "                - Client Name and Company\n",
    "                - Project Type (e.g., geospatial analysis, data automation, AI model development)\n",
    "                - Main Objectives and Goals\n",
    "                - Specific Features or Functionalities required\n",
    "                - Mentioned Data Sources (e.g., satellite imagery types, specific databases)\n",
    "                - Technical Requirements (e.g., accuracy targets, performance needs, output formats like dashboards, APIs)\n",
    "                - Budget constraints or indications\n",
    "                - Timeline expectations (if mentioned)\n",
    "                - Any potential challenges or risks implied\n",
    "                Structure this information clearly into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are a meticulous analyst specializing in understanding client needs from initial conversations.\n",
    "            Your strength lies in parsing dialogue to capture the core requirements, constraints, and context\n",
    "            of a potential project, organizing it into a structured Pydantic object that other team members can easily use.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #2 - Web Search Agent\n",
    "    def web_search_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Geospatial Technology & Market Researcher\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Based on the extracted project requirements (like project type, objectives, data sources, technical needs),\n",
    "                research current best practices, relevant technologies, potential challenges, and industry benchmarks\n",
    "                for similar geospatial projects using the GoogleSearch tool. Provide concise summaries and actionable insights\n",
    "                structured into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are an expert researcher skilled in finding up-to-date information on geospatial technologies,\n",
    "            AI applications in geography, data processing techniques, and market trends. You know how to\n",
    "            efficiently gather relevant case studies, technical documentation, and best practice guides\n",
    "            to inform project planning and estimation, outputting structured findings.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.search_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #3 - Vector Store Retrieval Agent\n",
    "    def service_retrieval_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Internal Service Catalog & Pricing Analyst\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Based on the extracted project requirements, use the VectorStoreSearch tool to find matching\n",
    "                internal services and standards. Identify a potential package of services, calculate the\n",
    "                total estimated price and development time, compare it against the client's budget,\n",
    "                note any requirement gaps needing custom work, and structure all findings into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You have deep knowledge of the company's service offerings, standard components, and pricing structure,\n",
    "            stored within the vector database. You excel at translating client requirements into a combination\n",
    "            of existing services, estimating costs and timelines accurately, identifying customization needs,\n",
    "            and outputting a structured analysis in the specified Pydantic format.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.vector_store_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Task 1: Extract project context and requirements\n",
    "    def extract_requirements_task(self) -> Task:\n",
    "        formatted_conversation = \"\\n\".join(\n",
    "            [f\"{item['speaker']}: {item['message']}\" for item in self.conversation]\n",
    "        )\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Analyze the following client conversation transcript and extract key project details.\n",
    "            Focus on understanding the client's needs, goals, constraints, and any specific mentions\n",
    "            of technology, data, or deliverables.\n",
    "\n",
    "            CONVERSATION:\n",
    "            ---\n",
    "            {formatted_conversation}\n",
    "            ---\n",
    "\n",
    "            Extract the following: Client Name/Company, Project Type, Objectives, Required Features,\n",
    "            Data Sources, Technical Requirements (Accuracy, Outputs), Budget, Timeline hints, Potential Risks.\n",
    "            Also provide a concise Markdown summary of the project request and constraints.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `RequirementsOutput` schema.\n",
    "            This object should contain all the extracted details like client info, project type, objectives,\n",
    "            data sources, features, technical requirements, budget, timeline, risks, and a markdown summary.\n",
    "            Ensure all fields in the Pydantic model are populated accurately based on the conversation.\n",
    "            The final output MUST be only the JSON representation of the populated `RequirementsOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.context_extraction_agent(),\n",
    "            output_pydantic=RequirementsOutput,\n",
    "        )\n",
    "        self._tasks[\"extract_requirements\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 2: Research relevant information on the web\n",
    "    def web_research_task(self) -> Task:\n",
    "        task = Task(\n",
    "            description=\"\"\"\\\n",
    "            Based on the project requirements extracted in Task 1 (referencing the `project_type`, `main_objective`, `data_sources`, `technical_requirements` fields from the previous task's output),\n",
    "            conduct web research using the GoogleSearch tool. **Perform ONE focused search** combining the most critical aspects:\n",
    "            - Best practices for implementing similar geospatial AI projects (e.g., wildfire monitoring AI).\n",
    "            - Relevant technologies (specific libraries like TensorFlow/PyTorch/GeoPandas, platforms like Google Earth Engine/ArcGIS, algorithms for geospatial AI, satellite image processing).\n",
    "\n",
    "            Synthesize the findings from your search into the structured format required.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `WebResearchOutput` schema.\n",
    "            This object must contain:\n",
    "            - `summary`: Brief overview of key findings regarding best practices and technologies.\n",
    "            - `best_practices_insights`: Actionable recommendations and notes on relevant tools/libraries/platforms found (mention specific names if prominent in search results).\n",
    "            - `potential_challenges`: Potential challenges identified from search results, if any.\n",
    "\n",
    "            Focus on information directly applicable to building the quote and SOW. Keep the output concise but informative.\n",
    "            The final output MUST be only the JSON representation of the populated `WebResearchOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.web_search_agent(),\n",
    "            context=[self._tasks[\"extract_requirements\"]],\n",
    "            output_pydantic=WebResearchOutput,\n",
    "        )\n",
    "        self._tasks[\"web_research\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 3: Retrieve relevant services and calculate estimates\n",
    "    def service_retrieval_task(self) -> Task:\n",
    "        task = Task(\n",
    "            description=\"\"\"\\\n",
    "            Analyze the structured project requirements (output object from Task 1).\n",
    "            Use the 'VectorStoreSearch' tool to query the internal service catalog based on keywords derived from the requirements\n",
    "            (e.g., `project_type`, `features_needed`, `data_sources`, `technical_requirements`). Use a query like \"geospatial AI wildfire monitoring database model dashboard API\".\n",
    "\n",
    "            Identify the most relevant existing services from the tool's output. For each relevant service, extract its name, label, price, and development time.\n",
    "\n",
    "            Based on the findings:\n",
    "            1.  Select a combination of services that best address the client's needs to propose as a package.\n",
    "            2.  Calculate the total estimated *base* price and total estimated *base* development time (in hours) for this package by summing the values from the selected services.\n",
    "            3.  List any client requirements (from Task 1 output, e.g., `features_needed`, `technical_requirements.outputs`) that are NOT clearly covered by the selected standard services. These might require custom work.\n",
    "            4.  Write a brief Markdown explanation justifying the service choices, base calculations, and customization needs.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `ServiceAnalysisOutput` schema.\n",
    "            This object must contain:\n",
    "            - `recommended_services`: A list of `RecommendedService` objects for services found and deemed relevant, with their details (name, label, price, development_time_hours).\n",
    "            - `service_package`: A `ServicePackage` object detailing the proposed package including a descriptive name (e.g., \"Wildfire Monitoring AI - Phase 1\"), included service labels, calculated total *base* price, and calculated total *base* time.\n",
    "            - `customization_notes`: A list of strings describing requirements potentially needing custom work.\n",
    "            - `explanation`: A Markdown string summarizing the analysis and justifications.\n",
    "\n",
    "            Ensure all calculations (total base price, total base time) are performed correctly.\n",
    "            The final output MUST be only the JSON representation of the populated `ServiceAnalysisOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.service_retrieval_agent(),\n",
    "            context=[self._tasks[\"extract_requirements\"]],\n",
    "            output_pydantic=ServiceAnalysisOutput,\n",
    "        )\n",
    "        self._tasks[\"service_retrieval\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 4: Consolidate Analysis Output\n",
    "    def consolidate_analysis_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"\"\"\\\n",
    "            Consolidate the outputs from the previous three tasks (Requirements Extraction, Web Research, Service Analysis)\n",
    "            into a single structured `AnalysisOutput` object. You have access to the outputs of these tasks via context.\n",
    "            Retrieve the `RequirementsOutput` object from Task 1 context.\n",
    "            Retrieve the `WebResearchOutput` object from Task 2 context.\n",
    "            Retrieve the `ServiceAnalysisOutput` object (which no longer contains budget analysis) from Task 3 context (your own previous output).\n",
    "            Combine these three objects into the fields (`requirements`, `research`, `service_analysis`) of the `AnalysisOutput` Pydantic model.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `AnalysisOutput` schema.\n",
    "            This object MUST contain the full outputs from Task 1, Task 2, and Task 3 nested within the appropriate fields.\n",
    "            The `service_analysis` field will reflect the structure without the preliminary budget check.\n",
    "            The final output MUST be only the JSON representation of the populated `AnalysisOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.service_retrieval_agent(),\n",
    "            context=[\n",
    "                self._tasks[\"extract_requirements\"],\n",
    "                self._tasks[\"web_research\"],\n",
    "                self._tasks[\"service_retrieval\"],\n",
    "            ],\n",
    "            output_pydantic=AnalysisOutput,\n",
    "        )\n",
    "\n",
    "    def crew(self) -> Crew:\n",
    "        \"\"\"Assemble the crew with all agents and tasks.\"\"\"\n",
    "        return Crew(\n",
    "            agents=[\n",
    "                self.context_extraction_agent(),\n",
    "                self.web_search_agent(),\n",
    "                self.service_retrieval_agent(),\n",
    "            ],\n",
    "            tasks=[\n",
    "                self.extract_requirements_task(),\n",
    "                self.web_research_task(),\n",
    "                self.service_retrieval_task(),\n",
    "                self.consolidate_analysis_task(),\n",
    "            ],\n",
    "            process=Process.sequential,\n",
    "            verbose=False,\n",
    "            step_callback=delay_before_task,\n",
    "            max_rpm=15,\n",
    "            memory=True,\n",
    "            embedder={\n",
    "                \"provider\": \"google\",\n",
    "                \"config\": {\n",
    "                    \"api_key\": GOOGLE_API_KEY,\n",
    "                    \"model\": \"models/text-embedding-004\",\n",
    "                },\n",
    "            },\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b24a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Context and Requirements Extraction Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Analyze the following client conversation transcript and extract key project details.\n",
      "            Focus on understanding the client's needs, goals, constraints, and any specific mentions\n",
      "            of technology, data, or deliverables.\n",
      "\n",
      "            CONVERSATION:\n",
      "            ---\n",
      "            agent: Hello! Thank you for contacting us. Could you please tell me your name and the company you represent?\n",
      "customer: Hi, my name is Jane Smith, and I'm from Wildfire Watch North America.\n",
      "agent: Nice to meet you, Jane. How can we help Wildfire Watch North America today?\n",
      "customer: We're looking for a quote for a geospatial project focused on wildfire monitoring across Canada and the US. We need to build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management.\n",
      "agent: Okay, that's a critical Geospatial AI project. What specific data sources are you considering for both the historical database and the ongoing monitoring?\n",
      "customer: We plan to use satellite imagery like MODIS, VIIRS, Landsat, and Sentinel, along with historical fire perimeter data, weather data, and potentially topographical data.\n",
      "agent: That's a robust set of sources. For the predictive aspect, what key factors are you hoping the AI model will help predict? Fire spread, ignition risk, or something else?\n",
      "customer: Primarily ignition risk and potential spread patterns. We also need near real-time monitoring capabilities.\n",
      "agent: What is the desired output for this system? A web dashboard for visualization, an API for integration, or detailed reports?\n",
      "customer: We need a web dashboard for our analysts and an API for integrating alerts into our existing systems. Regular risk assessment reports would also be valuable.\n",
      "agent: Do you have specific performance requirements for the predictive model, perhaps in terms of accuracy or lead time for predictions?\n",
      "customer: We're aiming for high accuracy in identifying high-risk zones, maybe around 85-90% precision, and timely alerts for new ignitions.\n",
      "agent: Understood. And what is your approximate budget for this initial phase, covering the database build, data gathering, and initial model development?\n",
      "customer: Our budget is around $40,000 for this phase.\n",
      "agent: Thank you, Jane. That gives us a clear picture. We'll start developing a proposal for Wildfire Watch North America.\n",
      "            ---\n",
      "\n",
      "            Extract the following: Client Name/Company, Project Type, Objectives, Required Features,\n",
      "            Data Sources, Technical Requirements (Accuracy, Outputs), Budget, Timeline hints, Potential Risks.\n",
      "            Also provide a concise Markdown summary of the project request and constraints.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Context and Requirements Extraction Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"client_name\": \"Jane Smith\",\n",
      "  \"client_company\": \"Wildfire Watch North America\",\n",
      "  \"project_type\": \"Geospatial AI project\",\n",
      "  \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "  \"data_sources\": [\n",
      "    \"MODIS satellite imagery\",\n",
      "    \"VIIRS satellite imagery\",\n",
      "    \"Landsat satellite imagery\",\n",
      "    \"Sentinel satellite imagery\",\n",
      "    \"Historical fire perimeter data\",\n",
      "    \"Weather data\",\n",
      "    \"Topographical data\"\n",
      "  ],\n",
      "  \"features_needed\": [\n",
      "    \"Near real-time monitoring capabilities\",\n",
      "    \"Predict ignition risk\",\n",
      "    \"Predict potential spread patterns\",\n",
      "    \"Web dashboard for analysts\",\n",
      "    \"API for integrating alerts into existing systems\",\n",
      "    \"Regular risk assessment reports\"\n",
      "  ],\n",
      "  \"technical_requirements\": {\n",
      "    \"accuracy\": \"85-90% precision in identifying high-risk zones\",\n",
      "    \"outputs\": [\n",
      "      \"Web dashboard\",\n",
      "      \"API\",\n",
      "      \"Regular risk assessment reports\"\n",
      "    ]\n",
      "  },\n",
      "  \"budget\": \"$40,000\",\n",
      "  \"timeline_expectation\": null,\n",
      "  \"potential_risks\": [\n",
      "    \"Achieving desired accuracy with the AI model\",\n",
      "    \"Timely data acquisition and processing\",\n",
      "    \"Integration with existing systems\"\n",
      "  ],\n",
      "  \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a database of historical wildfire events, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The system should provide near real-time monitoring, a web dashboard for analysts, and an API for integration. The target accuracy is 85-90% precision in identifying high-risk zones. The budget for the initial phase is $40,000.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Based on the project requirements extracted in Task 1 (referencing the `project_type`, `main_objective`, `data_sources`, `technical_requirements` fields from the previous task's output),\n",
      "            conduct web research using the GoogleSearch tool. **Perform ONE focused search** combining the most critical aspects:\n",
      "            - Best practices for implementing similar geospatial AI projects (e.g., wildfire monitoring AI).\n",
      "            - Relevant technologies (specific libraries like TensorFlow/PyTorch/GeoPandas, platforms like Google Earth Engine/ArcGIS, algorithms for geospatial AI, satellite image processing).\n",
      "\n",
      "            Synthesize the findings from your search into the structured format required.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to research best practices and relevant technologies for building a geospatial AI solution for wildfire monitoring and prediction, focusing on the specified data sources and technical requirements. I will use GoogleSearch to find information on this topic.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mGoogleSearch\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"geospatial AI wildfire monitoring best practices satellite imagery analysis TensorFlow PyTorch Google Earth Engine ArcGIS\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "It appears you're asking about best practices for using geospatial AI for wildfire monitoring, specifically utilizing satellite imagery analysis with tools like TensorFlow, PyTorch, Google Earth Engine, and ArcGIS. Here's a breakdown of key aspects:\n",
      "\n",
      "**1. The Importance of Early Wildfire Detection**.\n",
      "*   Wildfires can spread rapidly, causing immense damage to ecosystems, homes, and human life. Early detection is crucial for:\n",
      "    *   Reducing response times, enabling firefighters to contain fires before they spread.\n",
      "    *   Minimizing damage to forests, infrastructure and homes.\n",
      "    *   Protecting human and animal lives through timely evacuations.\n",
      "    *   Decreasing economic losses.\n",
      "\n",
      "**2. How AI and Geospatial Data are Transforming Wildfire Monitoring:**.\n",
      "*   **AI-powered systems:** These systems use cameras, gas sensors, satellites, drones, and infrared imaging to detect fires quickly and accurately.\n",
      "*   **Real-time analysis:** AI models analyze environmental data in real time to identify smoke, flames, and heat signatures, sending alerts to emergency responders within minutes.\n",
      "*   **Satellite Monitoring:** Satellites detect heat anomalies and track fire movement across large areas, providing real-time data for emergency response and evacuation planning. They also assess environmental conditions to predict fire-prone areas based on wind patterns, humidity and drought.\n",
      "*   **Geospatial Analysis:** This helps process satellite imagery, analyze geospatial data and identify fire-prone areas with accuracy. AI models can detect temperature anomalies, track vegetation dryness and predict potential wildfire outbreaks before they escalate.\n",
      "\n",
      "**3. Key Technologies and Techniques:**.\n",
      "*   **Satellite Imagery:**\n",
      "    *   Use of near-infrared and shortwave infrared imagery to detect vegetation changes and surface temperature shifts, which are key indicators of wildfires.\n",
      "    *   Satellites can detect heat signatures using infrared and thermal imaging to identify temperature spikes that signal early stages of a wildfire.\n",
      "    *   Continuous satellite imaging helps track fire movement, predict spread, and improve containment strategies.\n",
      "*   **AI/Machine Learning Models:**\n",
      "    *   **Convolutional Neural Networks (CNNs):** Analyzing satellite imagery to identify wildfires with high accuracy. Trained on satellite images to distinguish between fire-affected and unaffected areas.\n",
      "    *   **Deep Learning:** Used for processing satellite images to identify transmission lines affected by burn scars.\n",
      "    *   AI models are trained on historical wildfire data, satellite imagery, and sensor readings to identify subtle changes in smoke movement, fire intensity, and heat emissions.\n",
      "*   **Platforms & Tools:**\n",
      "    *   **Google Earth Engine (GEE):** Facilitates retrieval of geospatial data and provides access to public datasets and real-time information using a cloud-based platform. It allows for analysis of satellite imagery and geospatial data at a planetary scale.\n",
      "    *   **ArcGIS:** Used to visualize data and produce maps.\n",
      "    *   **TensorFlow and PyTorch:** Deep learning frameworks used for predictive modeling.\n",
      "    *   **FlyPix AI:** Provides AI-based geospatial analysis solutions for wildfire detection and prevention, enabling authorities to process satellite imagery, analyze geospatial data, and identify fire-prone areas.\n",
      "*   **Data Integration:** Combining satellite and drone data creates a multi-layered monitoring system. Satellites offer broad-area detection and meteorological context, while drones pinpoint fire hotspots in difficult-to-reach locations.\n",
      "\n",
      "**4. Best Practices & Strategies**.\n",
      "*   **Early Detection Systems:** Leverage real-time monitoring to detect wildfires in their earliest stages.\n",
      "*   **AI-Driven Geospatial Analysis:** Use AI to process satellite imagery, analyze geospatial data, and identify fire-prone areas.\n",
      "*   **Machine Learning for Accuracy:** Train AI models on historical data to improve the accuracy of wildfire detection systems, minimizing false alarms and increasing early warnings.\n",
      "*   **Integration of Systems:** Integrate geospatial intelligence with existing wildfire monitoring systems like satellite networks and aerial surveillance to enhance early detection.\n",
      "*   **Multi-layered monitoring:** Combine satellite imaging and drone surveillance. Satellites can identify large-scale fire threats, while drones monitor difficult-to-reach locations.\n",
      "*   **Predictive analytics:** Use AI algorithms to combine satellite and drone data to forecast fire progression and enhance response strategies.\n",
      "*   **Community Involvement:** Engage local communities in awareness training and preventive measures.\n",
      "*   **Continuous Improvement:** Regularly monitor, train, test, and validate wildfire AI models with additional live datasets.\n",
      "*   **Collaboration:** Foster interdisciplinary cooperation and public engagement to translate Earth Observation data into effective action.\n",
      "\n",
      "**5. Specific Applications & Models**.\n",
      "*   **Wildfire and Smoke Classification:** Automatically classify wildfires and smoke using imagery from aerial drones and ground-based camera systems.\n",
      "*   **Wildfire Delineation:** Accurately map the spread of active fires using satellite imagery, aiding in risk assessment, evacuation planning, and resource allocation.\n",
      "*   **Burn Scar Mapping:** Use machine learning to map the aftermath of wildfires, enabling emergency responders to assess damage and plan recovery efforts.\n",
      "*   **Damage Assessment:** Analyze the severity of damage from drone and aerial imagery to identify critical areas and prioritize response efforts.\n",
      "\n",
      "**6. Challenges and Future Directions**.\n",
      "*   **Better Satellite Coverage:** Improving satellite coverage for more effective wildfire monitoring.\n",
      "*   **Smarter AI Tools:** Developing smarter AI tools for improved analysis and prediction.\n",
      "*   **Easier Access to Data:** Making fire data more accessible to various stakeholders.\n",
      "*   **Investment in Infrastructure:** Investing in resilient infrastructure to support wildfire monitoring and response efforts.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"This research focuses on best practices for geospatial AI in wildfire monitoring, emphasizing satellite imagery analysis using tools like TensorFlow, PyTorch, Google Earth Engine, and ArcGIS. Key areas include early detection, AI-driven analysis, and predictive modeling. The integration of satellite and drone data is crucial for comprehensive monitoring. Challenges include data accessibility and the need for smarter AI tools.\",\n",
      "  \"best_practices_insights\": \"Utilize Convolutional Neural Networks (CNNs) for satellite imagery analysis. Employ Google Earth Engine (GEE) for data retrieval and analysis. Integrate TensorFlow and PyTorch for predictive modeling. Combine satellite data with drone imagery for multi-layered monitoring. Focus on early detection systems and AI-driven geospatial analysis. Train AI models on historical data to improve accuracy. Consider FlyPix AI for geospatial analysis solutions.\",\n",
      "  \"potential_challenges\": \"Achieving desired accuracy with AI models, ensuring timely data acquisition and processing, and integrating with existing systems are potential challenges. Other challenges include improving satellite coverage, developing smarter AI tools, and making fire data more accessible.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Analyze the structured project requirements (output object from Task 1).\n",
      "            Use the 'VectorStoreSearch' tool to query the internal service catalog based on keywords derived from the requirements\n",
      "            (e.g., `project_type`, `features_needed`, `data_sources`, `technical_requirements`). Use a query like \"geospatial AI wildfire monitoring database model dashboard API\".\n",
      "\n",
      "            Identify the most relevant existing services from the tool's output. For each relevant service, extract its name, label, price, and development time.\n",
      "\n",
      "            Based on the findings:\n",
      "            1.  Select a combination of services that best address the client's needs to propose as a package.\n",
      "            2.  Calculate the total estimated *base* price and total estimated *base* development time (in hours) for this package by summing the values from the selected services.\n",
      "            3.  List any client requirements (from Task 1 output, e.g., `features_needed`, `technical_requirements.outputs`) that are NOT clearly covered by the selected standard services. These might require custom work.\n",
      "            4.  Write a brief Markdown explanation justifying the service choices, base calculations, and customization needs.\n",
      "            \u001b[00m\n",
      "Querying Vector Store with: geospatial AI wildfire monitoring database model dashboard API\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mOkay, I need to analyze the project requirements and use the `VectorStoreSearch` tool to find matching internal services and standards. Then, I'll identify a potential package of services, calculate the total estimated price and development time, compare it against the client's budget, note any requirement gaps needing custom work, and structure all findings into the required Pydantic format.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mVectorStoreSearch\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"geospatial AI wildfire monitoring database model dashboard API\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Result (Score: 0.8233):\n",
      "  Name: GeospatialModelDeploymentMonitoring\n",
      "  Label: Geospatial AI Model Deployment & Monitoring\n",
      "  Price: $1000\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / devops\n",
      "  Description: Setup and configuration of infrastructure for deploying, monitoring (performance, drift, data quality), and managing the lifecycle of geospatial AI models in production environments.\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8500):\n",
      "  Name: AIIntegration\n",
      "  Label: AI Integration for Geospatial\n",
      "  Price: $900\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / ai\n",
      "  Description: Seamless integration with major AI providers (OpenAI, Google, Anthropic) customized for processing geospatial data with sophisticated content understanding\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8589):\n",
      "  Name: ChangeDetection\n",
      "  Label: AI-Powered Change Detection\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Temporal analysis of geospatial data to identify and classify changes over time\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8728):\n",
      "  Name: GeospatialLLM\n",
      "  Label: Geospatial Large Language Models\n",
      "  Price: $1400\n",
      "  Development Time: 40 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Fine-tuned LLMs for geospatial content understanding, query processing, and location-based insights generation\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8938):\n",
      "  Name: SpatialPredictiveModeling\n",
      "  Label: Spatial Predictive Modeling\n",
      "  Price: $1500\n",
      "  Development Time: 40 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: AI models that incorporate spatial relationships to predict future conditions or optimize resource allocation\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8991):\n",
      "  Name: GeoDataWrangling\n",
      "  Label: AI-Assisted Geospatial Data Wrangling\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / geospatial\n",
      "  Description: Intelligent ETL pipelines for geospatial data using GeoPandas, Shapely, and ML-enhanced data cleaning\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9049):\n",
      "  Name: GeoWorkflows\n",
      "  Label: Intelligent Geospatial Workflows\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / automation\n",
      "  Description: AI-optimized pipelines for processing, analyzing and visualizing geospatial data with smart validation and error handling\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9082):\n",
      "  Name: ReactDashboard\n",
      "  Label: Geospatial React Dashboard\n",
      "  Price: $1400\n",
      "  Development Time: 40 hours\n",
      "  Category: webDevelopment / frontend\n",
      "  Description: Interactive React-based dashboards for visualizing and interacting with geospatial data, featuring responsive design and modular components\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9183):\n",
      "  Name: SatelliteImageAnalysis\n",
      "  Label: Satellite Image Analysis with AI\n",
      "  Price: $1200\n",
      "  Development Time: 36 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9189):\n",
      "  Name: MultiAgentSystems\n",
      "  Label: Geospatial Multi-Agent Systems\n",
      "  Price: $1400\n",
      "  Development Time: 44 hours\n",
      "  Category: automations / ai\n",
      "  Description: Orchestrated AI agent networks specifically designed for complex geospatial workflows with built-in spatial reasoning capabilities\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"recommended_services\": [\n",
      "    {\n",
      "      \"name\": \"GeoDataWrangling\",\n",
      "      \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"price\": 950.0,\n",
      "      \"development_time_hours\": 28\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SatelliteImageAnalysis\",\n",
      "      \"label\": \"Satellite Image Analysis with AI\",\n",
      "      \"price\": 1200.0,\n",
      "      \"development_time_hours\": 36\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SpatialPredictiveModeling\",\n",
      "      \"label\": \"Spatial Predictive Modeling\",\n",
      "      \"price\": 1500.0,\n",
      "      \"development_time_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ReactDashboard\",\n",
      "      \"label\": \"Geospatial React Dashboard\",\n",
      "      \"price\": 1400.0,\n",
      "      \"development_time_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"AIIntegration\",\n",
      "      \"label\": \"AI Integration for Geospatial\",\n",
      "      \"price\": 900.0,\n",
      "      \"development_time_hours\": 28\n",
      "    }\n",
      "  ],\n",
      "  \"service_package\": {\n",
      "    \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "    \"services_included\": [\n",
      "      \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"Satellite Image Analysis with AI\",\n",
      "      \"Spatial Predictive Modeling\",\n",
      "      \"Geospatial React Dashboard\",\n",
      "      \"AI Integration for Geospatial\"\n",
      "    ],\n",
      "    \"total_estimated_price\": 5950.0,\n",
      "    \"total_estimated_time_hours\": 172,\n",
      "    \"notes\": \"This package provides the core components for wildfire monitoring and prediction, including data processing, AI model development, visualization, and API integration.\"\n",
      "  },\n",
      "  \"customization_notes\": [\n",
      "    \"Achieving the desired 85-90% precision in identifying high-risk zones may require custom AI model fine-tuning and optimization.\",\n",
      "    \"Integration with specific existing systems will require custom API development and configuration.\",\n",
      "    \"Development of regular risk assessment reports will require custom report generation logic and formatting.\"\n",
      "  ],\n",
      "  \"explanation\": \"The recommended service package includes AI-Assisted Geospatial Data Wrangling for preparing the data from various sources, Satellite Image Analysis with AI for extracting relevant information from satellite imagery, Spatial Predictive Modeling for building the wildfire risk prediction model, Geospatial React Dashboard for visualizing the results and providing a user interface, and AI Integration for Geospatial to provide an API for integration with existing systems. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization will likely be needed to achieve the desired accuracy, integrate with existing systems, and generate the required risk assessment reports.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Consolidate the outputs from the previous three tasks (Requirements Extraction, Web Research, Service Analysis)\n",
      "            into a single structured `AnalysisOutput` object. You have access to the outputs of these tasks via context.\n",
      "            Retrieve the `RequirementsOutput` object from Task 1 context.\n",
      "            Retrieve the `WebResearchOutput` object from Task 2 context.\n",
      "            Retrieve the `ServiceAnalysisOutput` object (which no longer contains budget analysis) from Task 3 context (your own previous output).\n",
      "            Combine these three objects into the fields (`requirements`, `research`, `service_analysis`) of the `AnalysisOutput` Pydantic model.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Web dashboard for analysts\",\n",
      "      \"API for integrating alerts into existing systems\",\n",
      "      \"Regular risk assessment reports\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard\",\n",
      "        \"API\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy with the AI model\",\n",
      "      \"Timely data acquisition and processing\",\n",
      "      \"Integration with existing systems\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a database of historical wildfire events, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The system should provide near real-time monitoring, a web dashboard for analysts, and an API for integration. The target accuracy is 85-90% precision in identifying high-risk zones. The budget for the initial phase is $40,000.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"This research focuses on best practices for geospatial AI in wildfire monitoring, emphasizing satellite imagery analysis using tools like TensorFlow, PyTorch, Google Earth Engine, and ArcGIS. Key areas include early detection, AI-driven analysis, and predictive modeling. The integration of satellite and drone data is crucial for comprehensive monitoring. Challenges include data accessibility and the need for smarter AI tools.\",\n",
      "    \"best_practices_insights\": \"Utilize Convolutional Neural Networks (CNNs) for satellite imagery analysis. Employ Google Earth Engine (GEE) for data retrieval and analysis. Integrate TensorFlow and PyTorch for predictive modeling. Combine satellite data with drone imagery for multi-layered monitoring. Focus on early detection systems and AI-driven geospatial analysis. Train AI models on historical data to improve accuracy. Consider FlyPix AI for geospatial analysis solutions.\",\n",
      "    \"potential_challenges\": \"Achieving desired accuracy with AI models, ensuring timely data acquisition and processing, and integrating with existing systems are potential challenges. Other challenges include improving satellite coverage, developing smarter AI tools, and making fire data more accessible.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for wildfire monitoring and prediction, including data processing, AI model development, visualization, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Achieving the desired 85-90% precision in identifying high-risk zones may require custom AI model fine-tuning and optimization.\",\n",
      "      \"Integration with specific existing systems will require custom API development and configuration.\",\n",
      "      \"Development of regular risk assessment reports will require custom report generation logic and formatting.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes AI-Assisted Geospatial Data Wrangling for preparing the data from various sources, Satellite Image Analysis with AI for extracting relevant information from satellite imagery, Spatial Predictive Modeling for building the wildfire risk prediction model, Geospatial React Dashboard for visualizing the results and providing a user interface, and AI Integration for Geospatial to provide an API for integration with existing systems. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization will likely be needed to achieve the desired accuracy, integrate with existing systems, and generate the required risk assessment reports.\"\n",
      "  }\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kickoff Crew 1\n",
    "analysis_crew = DataAnalysisCrew(vector_store=chroma_store, conversation=conversation)\n",
    "analysis_result = analysis_crew.crew().kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c5af4",
   "metadata": {},
   "source": [
    "## Step 7 - Quote Structuring & QA (Crew 2)\n",
    "\n",
    "The second multi-agent crew pipeline `QuoteGenerationCrew`, using CrewAI, takes the structured analysis and:\n",
    "\n",
    "- Builds a detailed project quote using function-calling and structured output\n",
    "- Reviews the quote for accuracy, consistency, and alignment with requirements, outputting a structured QA review\n",
    "\n",
    "This ensures the quote is both comprehensive and quality-checked before SOW generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62a359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew 2: Quote Structuring & QA\n",
    "class QuoteGenerationCrew:\n",
    "    def __init__(self, analysis_output: AnalysisOutput) -> None:\n",
    "        self.llm = LLM(\n",
    "            model=\"gemini/gemini-2.0-flash\",\n",
    "            temperature=0,\n",
    "            api_key=GOOGLE_API_KEY,\n",
    "            num_retries=5,\n",
    "            request_timeout=80,\n",
    "            seed=42,\n",
    "        )\n",
    "        self.analysis_output = analysis_output\n",
    "        self.today_date_obj = date.today()\n",
    "        self.today_date_str = self.today_date_obj.strftime(\"%Y-%m-%d\")\n",
    "        self._tasks = {}\n",
    "        self.quote_calculator_tool = QuoteCalculatorTool()\n",
    "\n",
    "    # Agent #4 - Quote Structuring Agent\n",
    "    def quote_structuring_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Quote Data Structuring Specialist\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Synthesize information from the provided `AnalysisOutput` object (containing requirements, research, and service analysis)\n",
    "                to structure the detailed project quote data into the `QuoteDetailsOutput` Pydantic format.\n",
    "                Estimate adjusted development hours and overhead buffer based on project specifics.\n",
    "                Use the QuoteCalculator tool to calculate the final price.\n",
    "                Perform all necessary calculations for dates and milestones accurately.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are an expert in translating project analysis into structured quote data.\n",
    "            You take the consolidated analysis (requirements, research, service details, pricing, budget analysis)\n",
    "            and meticulously populate a detailed quote structure.\n",
    "            First, you estimate the realistic total development hours by starting from the sum of base service hours, and only moderately increasing it (typically by 10-40%) to account for project-specific customizations, integration, and full lifecycle (planning, testing, deployment, PM).\n",
    "            Avoid drastic upscaling unless the project is highly custom or risky. Justify any increase clearly.\n",
    "            Second, you estimate an appropriate overhead buffer percentage (between 0.10 and 0.20) based on project risks and complexity.\n",
    "            Third, you use the QuoteCalculator tool with these estimates and the base service prices to get the final project price.\n",
    "            Finally, you populate the quote summary, line items, and payment schedule, performing date/milestone calculations precisely. Your output is a clean Pydantic object.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.quote_calculator_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #5 - Quality Assurance Agent\n",
    "    def qa_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Quote Quality Assurance Engineer\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Review the structured quote data (`QuoteDetailsOutput`) against the initial analysis (`AnalysisOutput`).\n",
    "                Check for accuracy in scope, deliverables, pricing, timeline, budget alignment, and consistency, based *only* on the provided inputs.\n",
    "                Output a structured QA review (`QAReviewOutput`) indicating 'approved' or 'needs_revision' with specific issues if any.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are a meticulous QA specialist focusing on data integrity in technical proposals.\n",
    "            You cross-reference the structured quote data against the original analysis data provided to you.\n",
    "            You ensure the quote accurately reflects the client's needs, aligns with the proposed services,\n",
    "            stays within budget, and is free of calculation errors or inconsistencies based *only* on the provided context.\n",
    "            You output a structured review Pydantic object. You do NOT have delegation capabilities in this setup.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "        )\n",
    "\n",
    "    # Task 4: Structure the quote data\n",
    "    def quote_structuring_task(self) -> Task:\n",
    "        analysis_json = self.analysis_output.model_dump_json(indent=2)\n",
    "        base_hours_sum = sum(\n",
    "            s.development_time_hours\n",
    "            for s in self.analysis_output.service_analysis.recommended_services\n",
    "        )\n",
    "\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Synthesize information from the provided `AnalysisOutput` JSON below to structure the project quote data.\n",
    "            Use the data within this JSON as the sole source for requirements, research, and service analysis.\n",
    "            **Assume today's date is {self.today_date_str} for all calculations.**\n",
    "\n",
    "            ANALYSIS OUTPUT:\n",
    "            ```json\n",
    "            {analysis_json}\n",
    "            ```\n",
    "\n",
    "            **Actions:**\n",
    "            1.  **Estimate Adjusted Hours & Justify:** Start with base hours ({base_hours_sum}).\n",
    "                - If the project is mostly standard, increase by 10-25% for lifecycle (planning, testing, deployment, PM).\n",
    "                - If there are moderate customizations or integrations, increase by 25-40%.\n",
    "                - Only increase more if the project is highly custom or risky, and justify clearly.\n",
    "                - Arrive at `estimated_total_hours`. Justify the increase (e.g., \"Increased base hours by X% for custom work and lifecycle activities...\").\n",
    "            2.  **Estimate Overhead Buffer & Justify:** Estimate `overhead_buffer_percentage` (0.10-0.20) based on risks/complexity. Justify briefly.\n",
    "            3.  **Calculate Price Breakdown:** Use `QuoteCalculator` tool with `estimated_total_hours`, `recommended_services` (from analysis output), `overhead_buffer_percentage`, `base_hourly_rate` (100). The tool will return a `QuoteCalculationDetails` object.\n",
    "            4.  **Extract Final Price:** Get the `final_total_price` from the `QuoteCalculationDetails` object returned by the tool. Use this for milestone calculations.\n",
    "            5.  **Generate Quote ID:** Create Quote ID: Q-{self.today_date_str}-001.\n",
    "            6.  **Extract Client Info:** Get `client_name`, `client_company`.\n",
    "            7.  **Determine Project Name:** Create a suitable `project_name`.\n",
    "            8.  **Calculate Dates:**\n",
    "                *   `today_obj = date.today()` \n",
    "                *   `quote_date_str = today_obj.strftime('%Y-%m-%d')`\n",
    "                *   `start_date_obj = today_obj + timedelta(days=7)`\n",
    "                *   `estimated_start_date = start_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `weeks_needed = math.ceil(estimated_total_hours / 40)` (Use `estimated_total_hours` from step 1)\n",
    "                *   `completion_date_obj = start_date_obj + timedelta(weeks=weeks_needed)`\n",
    "                *   `estimated_completion_date = completion_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `midpoint1_weeks = math.ceil(weeks_needed / 3)`\n",
    "                *   `midpoint1_date_obj = start_date_obj + timedelta(weeks=midpoint1_weeks)`\n",
    "                *   `midpoint1_date_str = midpoint1_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `midpoint2_weeks = math.ceil(weeks_needed * 2 / 3)`\n",
    "                *   `midpoint2_date_obj = start_date_obj + timedelta(weeks=midpoint2_weeks)`\n",
    "                *   `midpoint2_date_str = midpoint2_date_obj.strftime('%Y-%m-%d')`\n",
    "            9.  **Create Line Items:** Create `QuoteLineItem` objects from `recommended_services` (base details).\n",
    "            10.  **Create Payment Milestones (4 Milestones):**\n",
    "                *   Milestone 1: \"Project Kick-off\", 30%, amount = `total_price` * 0.30, due_date = `estimated_start_date`\n",
    "                *   Milestone 2: \"Mid-Project Review 1\", 30%, amount = `total_price` * 0.30, due_date = `midpoint1_date_str`\n",
    "                *   Milestone 3: \"Mid-Project Review 2\", 30%, amount = `total_price` * 0.30, due_date = `midpoint2_date_str`\n",
    "                *   Milestone 4: \"Project Completion & Handover\", 10%, amount = `total_price` * 0.10, due_date = `estimated_completion_date`\n",
    "                *   Create `PaymentMilestone` objects for each. Ensure amounts are calculated correctly (handle potential rounding if needed, though direct multiplication is usually fine).\n",
    "            11. **Populate Summary:** Populate `QuoteSummary` using extracted info, project name, quote ID, `quote_date_str` (from step 7), final `total_price`, `estimated_total_hours`, calculated dates (`estimated_start_date`, `estimated_completion_date`), the combined `estimation_justification`, **and the `overhead_buffer_percentage` estimated in step 2.**\n",
    "            12. **Assemble Output:** Combine `QuoteSummary`, `line_items`, and `payment_schedule` into `QuoteDetailsOutput`.\n",
    "\n",
    "            Ensure all estimations are justified, tool is called correctly, all dates use the current year ({self.today_date_obj.year}), milestones sum to 100%, and data mapping is accurate.\n",
    "            \"\"\",\n",
    "            expected_output=f\"\"\"\\\n",
    "            A Pydantic object conforming to the `QuoteDetailsOutput` schema.\n",
    "            - `quote_summary.quote_date` must be today's date ({self.today_date_str}).\n",
    "            - `quote_summary.estimated_start_date` and `quote_summary.estimated_completion_date` must be calculated correctly based on today's date and `total_estimated_hours`.\n",
    "            - `quote_summary.estimation_justification` must be populated.\n",
    "            - **`quote_summary.price_calculation_details` must be populated with the full `QuoteCalculationDetails` object returned by the QuoteCalculator tool.**\n",
    "            - **`quote_summary.total_price` must match `quote_summary.price_calculation_details.final_total_price`.**\n",
    "            - **`quote_summary.total_estimated_hours` must match `quote_summary.price_calculation_details.total_estimated_hours`.**\n",
    "            - `payment_schedule` must contain 4 milestones (30%, 30%, 30%, 10%) with correctly calculated amounts based on the final total price.\n",
    "            - All dates must reflect the current year ({self.today_date_obj.year}).\n",
    "\n",
    "            The final output MUST be only the JSON representation of this populated object.\n",
    "            \"\"\",\n",
    "            agent=self.quote_structuring_agent(),\n",
    "            output_pydantic=QuoteDetailsOutput,\n",
    "        )\n",
    "        self._tasks[\"quote_structuring\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 5: Review and QA the quote data\n",
    "    def qa_review_task(self) -> Task:\n",
    "        analysis_json = self.analysis_output.model_dump_json(indent=2)\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Review the structured quote data (`QuoteDetailsOutput` object from Task 4 context) for quality and accuracy against the provided `AnalysisOutput` JSON below.\n",
    "            **You MUST use the provided `AnalysisOutput` JSON and the `QuoteDetailsOutput` from Task 4 context for your review.**\n",
    "\n",
    "            ANALYSIS OUTPUT (Source of Truth):\n",
    "            ```json\n",
    "            {analysis_json}\n",
    "            ```\n",
    "\n",
    "            QUOTE DETAILS (To Be Reviewed - from Task 4 context):\n",
    "            *(The `QuoteDetailsOutput` JSON from the previous task will be available here in the actual execution context)*\n",
    "\n",
    "            **Verify the following based *only* on comparing Task 4 output (`QuoteDetailsOutput`) against the `AnalysisOutput` JSON and the logic described for Task 4:**\n",
    "                - **Completeness:**\n",
    "                    - Does `line_items` reflect `recommended_services`?\n",
    "                    - **Is `quote_summary.price_calculation_details` present and fully populated?**\n",
    "                - **Plausibility & Justification:**\n",
    "                    - Is `quote_summary.total_estimated_hours` significantly higher than base hours (sum of `estimated_hours` in `line_items`)?\n",
    "                    - Is `estimation_justification` present and reasonable?\n",
    "                    - **Does `price_calculation_details.overhead_buffer_percentage` fall within the expected range (e.g., 0.10 to 0.20)?**\n",
    "                - **Internal Consistency:**\n",
    "                    - **Does `quote_summary.total_price` exactly match `quote_summary.price_calculation_details.final_total_price`?**\n",
    "                    - **Does `quote_summary.total_estimated_hours` exactly match `quote_summary.price_calculation_details.total_estimated_hours`?**\n",
    "                    - **Verify the calculation within `price_calculation_details`: Does `(details.subtotal_before_overhead * (1 + details.overhead_buffer_percentage))` approximately equal `details.final_total_price` (allowing for minor rounding)?**\n",
    "                    - **Verify subtotal: Does `details.sum_base_service_prices + details.additional_development_cost` approximately equal `details.subtotal_before_overhead`?**\n",
    "                - **Accuracy (Milestones/Dates):**\n",
    "                    - Does `payment_schedule` contain exactly 4 milestones?\n",
    "                    - Do percentages sum to 100?\n",
    "                    - Are amounts calculated correctly based on `quote_summary.total_price`?\n",
    "                    - Does `quote_summary.quote_date` match today's date ({self.today_date_str})?\n",
    "                    - Is `estimated_completion_date` calculated correctly based on `quote_summary.total_estimated_hours`? (Verify logic as before). Are dates in the correct year?\n",
    "                - **Budget Alignment:** (Check remains the same - compare `quote_summary.total_price` to parsed `budget`)\n",
    "                - **Consistency:** Are `client_name`, `client_company`, `project_name` consistent?\n",
    "                - **Clarity:** Are `line_items` descriptions reasonable? Is `payment_schedule` logical?\n",
    "\n",
    "            **Output Format:**\n",
    "            - If issues found (budget exceeded, date mismatch, milestone error, missing/poor justification, inconsistency), create `QAIssue` objects. Set `overall_status` to 'needs_revision'.\n",
    "            - If satisfactory, set `overall_status` to 'approved'.\n",
    "            - Optional `QAImprovementSuggestion`s.\n",
    "            - Brief Markdown `summary`.\n",
    "            - Base findings strictly on comparison.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `QAReviewOutput` schema.\n",
    "            - `overall_status`: 'approved' or 'needs_revision'.\n",
    "                        - `issues`: List of `QAIssue` objects if status is 'needs_revision'. Issues must detail problems based *only* on the context (e.g., budget comparison failure, date calculation mismatch, milestone errors, **inconsistent price breakdown**, missing justification).\n",
    "            - `improvement_suggestions`: Optional list.\n",
    "            - `summary`: Brief Markdown summary.\n",
    "\n",
    "            The final output MUST be only the JSON representation of the populated `QAReviewOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.qa_agent(),\n",
    "            context=[self._tasks[\"quote_structuring\"]],\n",
    "            output_pydantic=QAReviewOutput,\n",
    "        )\n",
    "        return task\n",
    "\n",
    "    def crew(self) -> Crew:\n",
    "        \"\"\"Assemble the crew with all agents and tasks.\"\"\"\n",
    "        return Crew(\n",
    "            agents=[\n",
    "                self.quote_structuring_agent(),\n",
    "                self.qa_agent(),\n",
    "            ],\n",
    "            tasks=[self.quote_structuring_task(), self.qa_review_task()],\n",
    "            process=Process.sequential,\n",
    "            verbose=False,\n",
    "            step_callback=delay_before_task,\n",
    "            max_rpm=15,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "250fb967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Synthesize information from the provided `AnalysisOutput` JSON below to structure the project quote data.\n",
      "            Use the data within this JSON as the sole source for requirements, research, and service analysis.\n",
      "            **Assume today's date is 2025-04-16 for all calculations.**\n",
      "\n",
      "            ANALYSIS OUTPUT:\n",
      "            ```json\n",
      "            {\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Web dashboard for analysts\",\n",
      "      \"API for integrating alerts into existing systems\",\n",
      "      \"Regular risk assessment reports\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard\",\n",
      "        \"API\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy with the AI model\",\n",
      "      \"Timely data acquisition and processing\",\n",
      "      \"Integration with existing systems\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a database of historical wildfire events, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The system should provide near real-time monitoring, a web dashboard for analysts, and an API for integration. The target accuracy is 85-90% precision in identifying high-risk zones. The budget for the initial phase is $40,000.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"This research focuses on best practices for geospatial AI in wildfire monitoring, emphasizing satellite imagery analysis using tools like TensorFlow, PyTorch, Google Earth Engine, and ArcGIS. Key areas include early detection, AI-driven analysis, and predictive modeling. The integration of satellite and drone data is crucial for comprehensive monitoring. Challenges include data accessibility and the need for smarter AI tools.\",\n",
      "    \"best_practices_insights\": \"Utilize Convolutional Neural Networks (CNNs) for satellite imagery analysis. Employ Google Earth Engine (GEE) for data retrieval and analysis. Integrate TensorFlow and PyTorch for predictive modeling. Combine satellite data with drone imagery for multi-layered monitoring. Focus on early detection systems and AI-driven geospatial analysis. Train AI models on historical data to improve accuracy. Consider FlyPix AI for geospatial analysis solutions.\",\n",
      "    \"potential_challenges\": \"Achieving desired accuracy with AI models, ensuring timely data acquisition and processing, and integrating with existing systems are potential challenges. Other challenges include improving satellite coverage, developing smarter AI tools, and making fire data more accessible.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for wildfire monitoring and prediction, including data processing, AI model development, visualization, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Achieving the desired 85-90% precision in identifying high-risk zones may require custom AI model fine-tuning and optimization.\",\n",
      "      \"Integration with specific existing systems will require custom API development and configuration.\",\n",
      "      \"Development of regular risk assessment reports will require custom report generation logic and formatting.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes AI-Assisted Geospatial Data Wrangling for preparing the data from various sources, Satellite Image Analysis with AI for extracting relevant information from satellite imagery, Spatial Predictive Modeling for building the wildfire risk prediction model, Geospatial React Dashboard for visualizing the results and providing a user interface, and AI Integration for Geospatial to provide an API for integration with existing systems. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization will likely be needed to achieve the desired accuracy, integrate with existing systems, and generate the required risk assessment reports.\"\n",
      "  }\n",
      "}\n",
      "            ```\n",
      "\n",
      "            **Actions:**\n",
      "            1.  **Estimate Adjusted Hours & Justify:** Start with base hours (172).\n",
      "                - If the project is mostly standard, increase by 10-25% for lifecycle (planning, testing, deployment, PM).\n",
      "                - If there are moderate customizations or integrations, increase by 25-40%.\n",
      "                - Only increase more if the project is highly custom or risky, and justify clearly.\n",
      "                - Arrive at `estimated_total_hours`. Justify the increase (e.g., \"Increased base hours by X% for custom work and lifecycle activities...\").\n",
      "            2.  **Estimate Overhead Buffer & Justify:** Estimate `overhead_buffer_percentage` (0.10-0.20) based on risks/complexity. Justify briefly.\n",
      "            3.  **Calculate Price Breakdown:** Use `QuoteCalculator` tool with `estimated_total_hours`, `recommended_services` (from analysis output), `overhead_buffer_percentage`, `base_hourly_rate` (100). The tool will return a `QuoteCalculationDetails` object.\n",
      "            4.  **Extract Final Price:** Get the `final_total_price` from the `QuoteCalculationDetails` object returned by the tool. Use this for milestone calculations.\n",
      "            5.  **Generate Quote ID:** Create Quote ID: Q-2025-04-16-001.\n",
      "            6.  **Extract Client Info:** Get `client_name`, `client_company`.\n",
      "            7.  **Determine Project Name:** Create a suitable `project_name`.\n",
      "            8.  **Calculate Dates:**\n",
      "                *   `today_obj = date.today()` \n",
      "                *   `quote_date_str = today_obj.strftime('%Y-%m-%d')`\n",
      "                *   `start_date_obj = today_obj + timedelta(days=7)`\n",
      "                *   `estimated_start_date = start_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `weeks_needed = math.ceil(estimated_total_hours / 40)` (Use `estimated_total_hours` from step 1)\n",
      "                *   `completion_date_obj = start_date_obj + timedelta(weeks=weeks_needed)`\n",
      "                *   `estimated_completion_date = completion_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `midpoint1_weeks = math.ceil(weeks_needed / 3)`\n",
      "                *   `midpoint1_date_obj = start_date_obj + timedelta(weeks=midpoint1_weeks)`\n",
      "                *   `midpoint1_date_str = midpoint1_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `midpoint2_weeks = math.ceil(weeks_needed * 2 / 3)`\n",
      "                *   `midpoint2_date_obj = start_date_obj + timedelta(weeks=midpoint2_weeks)`\n",
      "                *   `midpoint2_date_str = midpoint2_date_obj.strftime('%Y-%m-%d')`\n",
      "            9.  **Create Line Items:** Create `QuoteLineItem` objects from `recommended_services` (base details).\n",
      "            10.  **Create Payment Milestones (4 Milestones):**\n",
      "                *   Milestone 1: \"Project Kick-off\", 30%, amount = `total_price` * 0.30, due_date = `estimated_start_date`\n",
      "                *   Milestone 2: \"Mid-Project Review 1\", 30%, amount = `total_price` * 0.30, due_date = `midpoint1_date_str`\n",
      "                *   Milestone 3: \"Mid-Project Review 2\", 30%, amount = `total_price` * 0.30, due_date = `midpoint2_date_str`\n",
      "                *   Milestone 4: \"Project Completion & Handover\", 10%, amount = `total_price` * 0.10, due_date = `estimated_completion_date`\n",
      "                *   Create `PaymentMilestone` objects for each. Ensure amounts are calculated correctly (handle potential rounding if needed, though direct multiplication is usually fine).\n",
      "            11. **Populate Summary:** Populate `QuoteSummary` using extracted info, project name, quote ID, `quote_date_str` (from step 7), final `total_price`, `estimated_total_hours`, calculated dates (`estimated_start_date`, `estimated_completion_date`), the combined `estimation_justification`, **and the `overhead_buffer_percentage` estimated in step 2.**\n",
      "            12. **Assemble Output:** Combine `QuoteSummary`, `line_items`, and `payment_schedule` into `QuoteDetailsOutput`.\n",
      "\n",
      "            Ensure all estimations are justified, tool is called correctly, all dates use the current year (2025), milestones sum to 100%, and data mapping is accurate.\n",
      "            \u001b[00m\n",
      "Calculator Input: Hours=230, Services Base=5950.0, Buffer=0.15, Rate=100\n",
      "Calculated Details: {\n",
      "  \"sum_base_service_prices\": 5950.0,\n",
      "  \"total_estimated_hours\": 230,\n",
      "  \"base_hourly_rate\": 100,\n",
      "  \"additional_hours\": 230,\n",
      "  \"additional_development_cost\": 23000.0,\n",
      "  \"subtotal_before_overhead\": 28950.0,\n",
      "  \"overhead_buffer_percentage\": 0.15,\n",
      "  \"overhead_amount\": 4342.5,\n",
      "  \"final_total_price\": 33292.5\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mOkay, I will follow the instructions to create the `QuoteDetailsOutput` object. I will start by estimating the adjusted development hours and overhead buffer, then use the `QuoteCalculator` tool to calculate the final price. After that, I will calculate the dates, create line items and payment milestones, and finally assemble the output.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mQuoteCalculator\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"estimated_total_hours\\\": 230, \\\"recommended_services\\\": [{\\\"name\\\": \\\"GeoDataWrangling\\\", \\\"label\\\": \\\"AI-Assisted Geospatial Data Wrangling\\\", \\\"price\\\": 950.0, \\\"development_time_hours\\\": 28}, {\\\"name\\\": \\\"SatelliteImageAnalysis\\\", \\\"label\\\": \\\"Satellite Image Analysis with AI\\\", \\\"price\\\": 1200.0, \\\"development_time_hours\\\": 36}, {\\\"name\\\": \\\"SpatialPredictiveModeling\\\", \\\"label\\\": \\\"Spatial Predictive Modeling\\\", \\\"price\\\": 1500.0, \\\"development_time_hours\\\": 40}, {\\\"name\\\": \\\"ReactDashboard\\\", \\\"label\\\": \\\"Geospatial React Dashboard\\\", \\\"price\\\": 1400.0, \\\"development_time_hours\\\": 40}, {\\\"name\\\": \\\"AIIntegration\\\", \\\"label\\\": \\\"AI Integration for Geospatial\\\", \\\"price\\\": 900.0, \\\"development_time_hours\\\": 28}], \\\"overhead_buffer_percentage\\\": 0.15, \\\"base_hourly_rate\\\": 100}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "sum_base_service_prices=5950.0 total_estimated_hours=230 base_hourly_rate=100 additional_hours=230 additional_development_cost=23000.0 subtotal_before_overhead=28950.0 overhead_buffer_percentage=0.15 overhead_amount=4342.5 final_total_price=33292.5\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"quote_summary\": {\n",
      "    \"project_name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"quote_id\": \"Q-2025-04-16-001\",\n",
      "    \"quote_date\": \"2025-04-16\",\n",
      "    \"total_price\": 33292.5,\n",
      "    \"total_estimated_hours\": 230,\n",
      "    \"estimated_start_date\": \"2025-04-23\",\n",
      "    \"estimated_completion_date\": \"2025-10-08\",\n",
      "    \"estimation_justification\": \"Increased base hours by 33% to account for custom AI model fine-tuning, integration with existing systems, development of custom risk assessment reports, and project lifecycle activities (planning, testing, deployment, project management).\",\n",
      "    \"price_calculation_details\": {\n",
      "      \"sum_base_service_prices\": 5950.0,\n",
      "      \"total_estimated_hours\": 230,\n",
      "      \"base_hourly_rate\": 100,\n",
      "      \"additional_hours\": 230,\n",
      "      \"additional_development_cost\": 23000.0,\n",
      "      \"subtotal_before_overhead\": 28950.0,\n",
      "      \"overhead_buffer_percentage\": 0.15,\n",
      "      \"overhead_amount\": 4342.5,\n",
      "      \"final_total_price\": 33292.5\n",
      "    }\n",
      "  },\n",
      "  \"line_items\": [\n",
      "    {\n",
      "      \"service_name\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"description\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"price\": 950.0,\n",
      "      \"estimated_hours\": 28\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"Satellite Image Analysis with AI\",\n",
      "      \"description\": \"Satellite Image Analysis with AI\",\n",
      "      \"price\": 1200.0,\n",
      "      \"estimated_hours\": 36\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"Spatial Predictive Modeling\",\n",
      "      \"description\": \"Spatial Predictive Modeling\",\n",
      "      \"price\": 1500.0,\n",
      "      \"estimated_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"Geospatial React Dashboard\",\n",
      "      \"description\": \"Geospatial React Dashboard\",\n",
      "      \"price\": 1400.0,\n",
      "      \"estimated_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"AI Integration for Geospatial\",\n",
      "      \"description\": \"AI Integration for Geospatial\",\n",
      "      \"price\": 900.0,\n",
      "      \"estimated_hours\": 28\n",
      "    }\n",
      "  ],\n",
      "  \"payment_schedule\": [\n",
      "    {\n",
      "      \"milestone\": \"Project Kick-off\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-04-23\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Mid-Project Review 1\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-06-04\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Mid-Project Review 2\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-08-06\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Project Completion & Handover\",\n",
      "      \"percentage\": 10,\n",
      "      \"amount\": 3329.25,\n",
      "      \"due_date\": \"2025-10-08\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Quality Assurance Engineer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Review the structured quote data (`QuoteDetailsOutput` object from Task 4 context) for quality and accuracy against the provided `AnalysisOutput` JSON below.\n",
      "            **You MUST use the provided `AnalysisOutput` JSON and the `QuoteDetailsOutput` from Task 4 context for your review.**\n",
      "\n",
      "            ANALYSIS OUTPUT (Source of Truth):\n",
      "            ```json\n",
      "            {\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Web dashboard for analysts\",\n",
      "      \"API for integrating alerts into existing systems\",\n",
      "      \"Regular risk assessment reports\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard\",\n",
      "        \"API\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy with the AI model\",\n",
      "      \"Timely data acquisition and processing\",\n",
      "      \"Integration with existing systems\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a database of historical wildfire events, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The system should provide near real-time monitoring, a web dashboard for analysts, and an API for integration. The target accuracy is 85-90% precision in identifying high-risk zones. The budget for the initial phase is $40,000.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"This research focuses on best practices for geospatial AI in wildfire monitoring, emphasizing satellite imagery analysis using tools like TensorFlow, PyTorch, Google Earth Engine, and ArcGIS. Key areas include early detection, AI-driven analysis, and predictive modeling. The integration of satellite and drone data is crucial for comprehensive monitoring. Challenges include data accessibility and the need for smarter AI tools.\",\n",
      "    \"best_practices_insights\": \"Utilize Convolutional Neural Networks (CNNs) for satellite imagery analysis. Employ Google Earth Engine (GEE) for data retrieval and analysis. Integrate TensorFlow and PyTorch for predictive modeling. Combine satellite data with drone imagery for multi-layered monitoring. Focus on early detection systems and AI-driven geospatial analysis. Train AI models on historical data to improve accuracy. Consider FlyPix AI for geospatial analysis solutions.\",\n",
      "    \"potential_challenges\": \"Achieving desired accuracy with AI models, ensuring timely data acquisition and processing, and integrating with existing systems are potential challenges. Other challenges include improving satellite coverage, developing smarter AI tools, and making fire data more accessible.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for wildfire monitoring and prediction, including data processing, AI model development, visualization, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Achieving the desired 85-90% precision in identifying high-risk zones may require custom AI model fine-tuning and optimization.\",\n",
      "      \"Integration with specific existing systems will require custom API development and configuration.\",\n",
      "      \"Development of regular risk assessment reports will require custom report generation logic and formatting.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes AI-Assisted Geospatial Data Wrangling for preparing the data from various sources, Satellite Image Analysis with AI for extracting relevant information from satellite imagery, Spatial Predictive Modeling for building the wildfire risk prediction model, Geospatial React Dashboard for visualizing the results and providing a user interface, and AI Integration for Geospatial to provide an API for integration with existing systems. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization will likely be needed to achieve the desired accuracy, integrate with existing systems, and generate the required risk assessment reports.\"\n",
      "  }\n",
      "}\n",
      "            ```\n",
      "\n",
      "            QUOTE DETAILS (To Be Reviewed - from Task 4 context):\n",
      "            *(The `QuoteDetailsOutput` JSON from the previous task will be available here in the actual execution context)*\n",
      "\n",
      "            **Verify the following based *only* on comparing Task 4 output (`QuoteDetailsOutput`) against the `AnalysisOutput` JSON and the logic described for Task 4:**\n",
      "                - **Completeness:**\n",
      "                    - Does `line_items` reflect `recommended_services`?\n",
      "                    - **Is `quote_summary.price_calculation_details` present and fully populated?**\n",
      "                - **Plausibility & Justification:**\n",
      "                    - Is `quote_summary.total_estimated_hours` significantly higher than base hours (sum of `estimated_hours` in `line_items`)?\n",
      "                    - Is `estimation_justification` present and reasonable?\n",
      "                    - **Does `price_calculation_details.overhead_buffer_percentage` fall within the expected range (e.g., 0.10 to 0.20)?**\n",
      "                - **Internal Consistency:**\n",
      "                    - **Does `quote_summary.total_price` exactly match `quote_summary.price_calculation_details.final_total_price`?**\n",
      "                    - **Does `quote_summary.total_estimated_hours` exactly match `quote_summary.price_calculation_details.total_estimated_hours`?**\n",
      "                    - **Verify the calculation within `price_calculation_details`: Does `(details.subtotal_before_overhead * (1 + details.overhead_buffer_percentage))` approximately equal `details.final_total_price` (allowing for minor rounding)?**\n",
      "                    - **Verify subtotal: Does `details.sum_base_service_prices + details.additional_development_cost` approximately equal `details.subtotal_before_overhead`?**\n",
      "                - **Accuracy (Milestones/Dates):**\n",
      "                    - Does `payment_schedule` contain exactly 4 milestones?\n",
      "                    - Do percentages sum to 100?\n",
      "                    - Are amounts calculated correctly based on `quote_summary.total_price`?\n",
      "                    - Does `quote_summary.quote_date` match today's date (2025-04-16)?\n",
      "                    - Is `estimated_completion_date` calculated correctly based on `quote_summary.total_estimated_hours`? (Verify logic as before). Are dates in the correct year?\n",
      "                - **Budget Alignment:** (Check remains the same - compare `quote_summary.total_price` to parsed `budget`)\n",
      "                - **Consistency:** Are `client_name`, `client_company`, `project_name` consistent?\n",
      "                - **Clarity:** Are `line_items` descriptions reasonable? Is `payment_schedule` logical?\n",
      "\n",
      "            **Output Format:**\n",
      "            - If issues found (budget exceeded, date mismatch, milestone error, missing/poor justification, inconsistency), create `QAIssue` objects. Set `overall_status` to 'needs_revision'.\n",
      "            - If satisfactory, set `overall_status` to 'approved'.\n",
      "            - Optional `QAImprovementSuggestion`s.\n",
      "            - Brief Markdown `summary`.\n",
      "            - Base findings strictly on comparison.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Quality Assurance Engineer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"overall_status\": \"needs_revision\",\n",
      "  \"issues\": [\n",
      "    {\n",
      "      \"type\": \"BudgetAlignment\",\n",
      "      \"severity\": \"high\",\n",
      "      \"description\": \"The total price of the quote ($33292.5) exceeds the budget specified in the analysis ($40,000) by a significant margin. While it is *under* budget, this is unexpected and requires justification. The additional development cost of $23000 is not clearly justified based on the analysis.\",\n",
      "      \"recommendation\": \"Provide a clear justification for the additional development cost and explain why the quote is significantly under budget. Consider adding scope to meet the budget or reducing the additional development cost.\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"PriceCalculationInconsistency\",\n",
      "      \"severity\": \"medium\",\n",
      "      \"description\": \"The `additional_hours` field in `price_calculation_details` is equal to the `total_estimated_hours`. This implies that *all* hours are being billed at the 'additional' rate, which is inconsistent with the base service prices. The base hourly rate is also not used in the calculation.\",\n",
      "      \"recommendation\": \"Clarify the calculation of `additional_development_cost`. It should be based on the *additional* hours beyond those already accounted for in the base service prices. The base hourly rate should be applied to the base service hours.\"\n",
      "    }\n",
      "  ],\n",
      "  \"improvement_suggestions\": [\n",
      "    {\n",
      "      \"area\": \"Justification\",\n",
      "      \"suggestion\": \"Provide a more detailed breakdown of the additional development cost, specifying which tasks or resources contribute to this cost.\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The quote requires revision due to budget alignment concerns and inconsistencies in the price calculation. The additional development cost needs better justification, and the calculation of total price should be reviewed to ensure it accurately reflects the base service prices and any additional costs.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Crew 1 output\n",
    "analysis_data: AnalysisOutput = analysis_result.pydantic\n",
    "\n",
    "# Kickoff Crew 2\n",
    "quote_crew = QuoteGenerationCrew(analysis_output=analysis_data)\n",
    "quote_result = quote_crew.crew().kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd2ce7",
   "metadata": {},
   "source": [
    "## Step 8 - Statement of Work (SOW) Generation\n",
    "\n",
    "Generate a professional SOW document from the structured quote and analysis. The SOW covers:\n",
    "\n",
    "- Project overview, objectives, and summary\n",
    "- In-scope and out-of-scope services and deliverables\n",
    "- Key features, technical specifications, and data sources\n",
    "- Acceptance criteria for each deliverable\n",
    "- Timeline with estimated start, completion, and milestones\n",
    "- Detailed price breakdown and payment schedule\n",
    "- Roles and responsibilities for provider and client\n",
    "- Assumptions, change management process, reporting, and disclaimer\n",
    "\n",
    "The SOW is auto-filled using structured data, ensuring clarity and consistency for client approval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19580b6f",
   "metadata": {},
   "source": [
    "### 8.1 - Create the SOW template\n",
    "\n",
    "The template is formatted as markdown, for consistent output, but also include HTML tags to bypass formatting issues within Kaggle notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e847a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOW_TEMPLATE = dedent(\"\"\"\\\n",
    "    # Statement of Work\n",
    "\n",
    "    ## 1. Introduction\n",
    "    This Statement of Work (SOW) is made and entered into as of {today_date}, by and between {provider_name} (\"Provider\"), located at {provider_address}, and {client_company} (\"Client\"). This document outlines the scope, deliverables, timeline, and costs for the {project_name} project.\n",
    "\n",
    "    ## 2. Project Overview & Objectives\n",
    "    **Project Name:** {project_name} <br>\n",
    "    **Main Objective:** {main_objective} <br>\n",
    "    **Project Summary:**\n",
    "    *{project_summary}*\n",
    "\n",
    "    ## 3. Scope of Work\n",
    "    ### 3.1. In-Scope Services & Deliverables\n",
    "    The following services, based on the approved quote (ID: {quote_id}), are included:\n",
    "    {line_items_markdown}\n",
    "\n",
    "    Key features to be delivered include:\n",
    "    {features_markdown}\n",
    "\n",
    "    {customization_notes_section}\n",
    "\n",
    "    ### 3.2. Out-of-Scope\n",
    "    The following are explicitly excluded:\n",
    "    *   Ongoing maintenance beyond a standard 30-day warranty period post-completion.\n",
    "    *   Hardware procurement or setup.\n",
    "    *   Data acquisition or licensing costs unless specified in line items.\n",
    "    *   Training beyond basic handover documentation.\n",
    "    *   Work not explicitly listed in section 3.1.\n",
    "\n",
    "    ## 4. Technical Specifications\n",
    "    *   **Key Technologies:**\n",
    "        {key_technologies}\n",
    "    *   **Data Sources:**\n",
    "        {data_sources_list}\n",
    "    *   **Performance Requirements:** Accuracy target: {accuracy_target}.\n",
    "    *   **Outputs:**\n",
    "        {outputs_list}\n",
    "\n",
    "    ## 5. Deliverables & Acceptance Criteria\n",
    "    Major deliverables correspond to the services listed in 3.1.\n",
    "    {acceptance_criteria_markdown}\n",
    "\n",
    "    ## 6. Project Timeline\n",
    "    *   **Estimated Start Date:** {start_date}\n",
    "    *   **Estimated Completion Date:** {completion_date}\n",
    "    *   **Total Estimated Effort:** Approximately {total_hours} hours.\n",
    "\n",
    "    ## 7. Roles & Responsibilities\n",
    "    *   **Provider ({provider_name}):** Project management, development, testing, deployment, documentation.\n",
    "    *   **Client ({client_company}):** Provide timely access to data sources ({data_sources_list}), provide subject matter expertise, assign a primary point of contact ({client_name}), participate in reviews, provide timely feedback/approvals.\n",
    "\n",
    "    ## 8. Pricing & Payment Schedule\n",
    "    *   **Total Project Price:** ${total_price:.2f}\n",
    "\n",
    "    ### 8.1. Price Breakdown\n",
    "    {price_breakdown_markdown}\n",
    "\n",
    "    ### 8.2. Payment Schedule\n",
    "    {payment_schedule_markdown}\n",
    "\n",
    "    ## 9. Assumptions\n",
    "    *   Client will provide access to specified data sources within 1 week of project start.\n",
    "    *   Client point of contact will be available for weekly check-ins and provide feedback within 2 business days.\n",
    "    *   Scope is fixed as defined in Section 3.1.\n",
    "    *   Data quality from provided sources is sufficient for model training.\n",
    "    *   {add_assumptions_from_research}\n",
    "\n",
    "    ## 10. Change Management\n",
    "    Any requested changes to scope, deliverables, or timeline must be submitted via a formal Change Request process. Approved changes may impact cost and schedule.\n",
    "\n",
    "    ## 11. Reporting\n",
    "    Provider will provide brief weekly status updates via email and hold bi-weekly review meetings with the Client.\n",
    "\n",
    "    ## 12. Disclaimer\n",
    "    Disclaimer: This Statement of Work was generated with AI assistance based on the approved quote (Quote ID: {quote_id}). It requires final human review and approval by both Provider and Client before execution.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bed0b",
   "metadata": {},
   "source": [
    "### Step 8.2 - Definie the function that generate the SOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c210851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sow(\n",
    "    quote_details: QuoteDetailsOutput,\n",
    "    analysis: AnalysisOutput,\n",
    "    provider_info: Dict[str, str],\n",
    ") -> str:\n",
    "    \"\"\"Generates the SOW Markdown document from structured data.\"\"\"\n",
    "\n",
    "    # Prepare template data\n",
    "    qs = quote_details.quote_summary\n",
    "    req = analysis.requirements\n",
    "    sa = analysis.service_analysis\n",
    "    res = analysis.research\n",
    "    details = qs.price_calculation_details\n",
    "    additional_development_cost = details.additional_development_cost\n",
    "    overhead_amount = details.overhead_amount\n",
    "    buffer_percentage_used = details.overhead_buffer_percentage\n",
    "    total_estimated_hours = details.total_estimated_hours\n",
    "\n",
    "    # Format Price Breakdown using HTML table\n",
    "    price_breakdown_lines = [\"<table>\"]\n",
    "    price_breakdown_lines.append(\n",
    "        \"<tr><th>Item</th><th>Estimated Cost</th><th>Notes</th></tr>\"\n",
    "    )\n",
    "\n",
    "    # Add rows for services\n",
    "    for item in quote_details.line_items:\n",
    "        price_to_display = item.price\n",
    "        price_breakdown_lines.append(\n",
    "            f\"<tr><td>Standard Service: {item.service_name}</td><td>${price_to_display:.2f}</td><td>Base price for standard offering</td></tr>\"\n",
    "        )\n",
    "\n",
    "    # Add additional development cost row if necessary\n",
    "    if additional_development_cost > 0:\n",
    "        price_breakdown_lines.append(\n",
    "            f\"<tr><td>Additional Development & Lifecycle</td><td>${additional_development_cost:.2f}</td><td>Custom work, testing, PM, etc. ({total_estimated_hours} hrs)</td></tr>\"\n",
    "        )\n",
    "\n",
    "    # Add overhead row if necessary\n",
    "    if overhead_amount > 0:\n",
    "        price_breakdown_lines.append(\n",
    "            f\"<tr><td>Overhead & Contingency ({buffer_percentage_used * 100:.0f}%)</td><td>${overhead_amount:.2f}</td><td>Project complexity & risk buffer</td></tr>\"\n",
    "        )\n",
    "\n",
    "    # Add price breakdown by service\n",
    "    price_breakdown_lines.append(\n",
    "        f\"<tr><th>Total Estimated Project Price</th><th>${qs.total_price:.2f}</th><td></td></tr>\"\n",
    "    )\n",
    "    price_breakdown_lines.append(\"</table>\")\n",
    "    price_breakdown_md = \"\\n\".join(price_breakdown_lines)\n",
    "\n",
    "    line_items = \"\\n\".join(\n",
    "        [\n",
    "            f\"<li><b>{item.service_name}</b>: {item.description} (Base: ${item.price:.2f}, {item.estimated_hours} hrs)</li>\"\n",
    "            for item in quote_details.line_items\n",
    "        ]\n",
    "    )\n",
    "    line_items_md = f\"\\n<ul>\\n{line_items}\\n</ul>\\n\"\n",
    "\n",
    "    # Features as HTML list\n",
    "    features_html = \"<ul>\\n\"\n",
    "    features_html += \"\\n\".join(\n",
    "        [f\"<li>{feature}</li>\" for feature in req.features_needed]\n",
    "    )\n",
    "    features_html += \"\\n</ul>\"\n",
    "\n",
    "    # Payment schedule as HTML table\n",
    "    payment_schedule_lines = [\"<table>\"]\n",
    "    payment_schedule_lines.append(\n",
    "        \"<tr><th>Milestone</th><th>Percentage</th><th>Amount</th><th>Due Date</th></tr>\"\n",
    "    )\n",
    "\n",
    "    # Add rows for each milestone\n",
    "    for milestone in quote_details.payment_schedule:\n",
    "        payment_schedule_lines.append(\n",
    "            f\"<tr><td>{milestone.milestone}</td><td>{milestone.percentage}%</td><td>${milestone.amount:.2f}</td><td>{milestone.due_date}</td></tr>\"\n",
    "        )\n",
    "\n",
    "    payment_schedule_lines.append(\"</table>\")\n",
    "    payment_schedule_md = \"\\n\".join(payment_schedule_lines)\n",
    "\n",
    "    # Acceptance criteria as HTML list\n",
    "    acceptance_criteria_lines = [\"<ul>\"]\n",
    "\n",
    "    for i, item in enumerate(quote_details.line_items):\n",
    "        feature_text = req.features_needed[i] if i < len(req.features_needed) else \"N/A\"\n",
    "        acceptance_criteria_lines.append(\n",
    "            f\"<li><b>{item.service_name} Deliverable</b>: Corresponds to service {item.service_name}.<br>\"\n",
    "            f\"<ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to '{feature_text}' as described.</li></ul></li>\"\n",
    "        )\n",
    "\n",
    "    acceptance_criteria_lines.append(\"</ul>\")\n",
    "    acceptance_criteria_html = \"\\n\".join(acceptance_criteria_lines)\n",
    "\n",
    "    # Key technologies\n",
    "    key_tech = (\n",
    "        res.best_practices_insights\n",
    "        if res.best_practices_insights\n",
    "        else \"Standard web/cloud technologies as appropriate\"\n",
    "    )\n",
    "\n",
    "    # Assumptions from research\n",
    "    add_assumptions = (\n",
    "        f\"Potential challenges identified during research include: {res.potential_challenges}\"\n",
    "        if res.potential_challenges\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    # Populate Template\n",
    "    sow_content = SOW_TEMPLATE.format(\n",
    "        today_date=date.today().strftime(\"%Y-%m-%d\"),\n",
    "        provider_name=provider_info.get(\"name\", \"Acme Inc.\"),\n",
    "        provider_address=provider_info.get(\n",
    "            \"address\", \"123 Business Street, City, Country\"\n",
    "        ),\n",
    "        client_company=qs.client_company or \"N/A\",\n",
    "        project_name=qs.project_name,\n",
    "        main_objective=req.main_objective or \"N/A\",\n",
    "        project_summary=req.summary or \"\",\n",
    "        quote_id=qs.quote_id,\n",
    "        line_items_markdown=line_items_md,\n",
    "        features_markdown=features_html,\n",
    "        customization_notes_section=(\n",
    "            f\"<p><em>Includes custom development considerations for: {', '.join(sa.customization_notes)}</em></p>\"\n",
    "            if sa.customization_notes\n",
    "            else \"\"\n",
    "        ),\n",
    "        key_technologies=key_tech.strip(),\n",
    "        data_sources_list=\", \".join(req.data_sources) if req.data_sources else \"N/A\",\n",
    "        accuracy_target=req.technical_requirements.accuracy\n",
    "        if req.technical_requirements\n",
    "        else \"N/A\",\n",
    "        outputs_list=\", \".join(req.technical_requirements.outputs)\n",
    "        if req.technical_requirements and req.technical_requirements.outputs\n",
    "        else \"N/A\",\n",
    "        acceptance_criteria_markdown=acceptance_criteria_html,\n",
    "        start_date=qs.estimated_start_date,\n",
    "        completion_date=qs.estimated_completion_date,\n",
    "        total_hours=qs.total_estimated_hours,\n",
    "        client_name=qs.client_name or \"N/A\",\n",
    "        total_price=qs.total_price,\n",
    "        price_breakdown_markdown=price_breakdown_md,\n",
    "        payment_schedule_markdown=payment_schedule_md,\n",
    "        add_assumptions_from_research=add_assumptions.strip(),\n",
    "    )\n",
    "\n",
    "    return sow_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3b2e6",
   "metadata": {},
   "source": [
    "### 8.3 - SOW Generation Execution\n",
    "\n",
    "The function `create_final_sow` orchestrates the final SOW generation process:\n",
    "\n",
    "- Takes the outputs from both crews\n",
    "- Validates that outputs are present and of the correct types\n",
    "- Calls `generate_sow` to produce the final SOW markdown document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b80925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_sow(quote_result: Any, analysis_data: AnalysisOutput) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generates the final Statement of Work (SOW) if the quote passes QA.\n",
    "\n",
    "    Args:\n",
    "        quote_result: The output object from the QuoteGenerationCrew.\n",
    "        analysis_data: The output object from the DataAnalysisCrew.\n",
    "\n",
    "    Returns:\n",
    "        The generated SOW as a markdown string, or None if QA failed or an error occurred.\n",
    "    \"\"\"\n",
    "    provider_details = {\n",
    "        \"name\": \"Acme Inc.\",\n",
    "        \"address\": \"123 Your Street, ABCity, EDCountry\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Ensure quote_result and its tasks_output are valid\n",
    "        if not (\n",
    "            quote_result\n",
    "            and hasattr(quote_result, \"tasks_output\")\n",
    "            and len(quote_result.tasks_output) == 2\n",
    "        ):\n",
    "            raise ValueError(\"Invalid quote_result structure.\")\n",
    "\n",
    "        quote_details_task = quote_result.tasks_output[0]\n",
    "        qa_output_task = quote_result.tasks_output[-1]\n",
    "\n",
    "        # Check task outputs against pydantic models\n",
    "        if not (\n",
    "            hasattr(qa_output_task, \"pydantic\")\n",
    "            and isinstance(qa_output_task.pydantic, QAReviewOutput)\n",
    "            and hasattr(quote_details_task, \"pydantic\")\n",
    "            and isinstance(quote_details_task.pydantic, QuoteDetailsOutput)\n",
    "        ):\n",
    "            raise ValueError(\"Unexpected output types from quote generation tasks.\")\n",
    "\n",
    "        # qa_data: QAReviewOutput = qa_output_task.pydantic\n",
    "        quote_data: QuoteDetailsOutput = quote_details_task.pydantic\n",
    "\n",
    "        # if qa_data.overall_status == \"approved\":\n",
    "        final_sow = generate_sow(\n",
    "            quote_details=quote_data,\n",
    "            analysis=analysis_data,\n",
    "            provider_info=provider_details,\n",
    "        )\n",
    "        return final_sow\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during SOW generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d54ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Statement of Work</h1>\n",
       "<h2>1. Introduction</h2>\n",
       "<p>This Statement of Work (SOW) is made and entered into as of 2025-04-16, by and between Acme Inc. (\"Provider\"), located at 123 Your Street, ABCity, EDCountry, and Wildfire Watch North America (\"Client\"). This document outlines the scope, deliverables, timeline, and costs for the Wildfire Monitoring AI - Phase 1 project.</p>\n",
       "<h2>2. Project Overview &amp; Objectives</h2>\n",
       "<p><strong>Project Name:</strong> Wildfire Monitoring AI - Phase 1 <br>\n",
       "<strong>Main Objective:</strong> Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US. <br>\n",
       "<strong>Project Summary:</strong>\n",
       "<em>Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a database of historical wildfire events, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The system should provide near real-time monitoring, a web dashboard for analysts, and an API for integration. The target accuracy is 85-90% precision in identifying high-risk zones. The budget for the initial phase is $40,000.</em></p>\n",
       "<h2>3. Scope of Work</h2>\n",
       "<h3>3.1. In-Scope Services &amp; Deliverables</h3>\n",
       "<p>The following services, based on the approved quote (ID: Q-2025-04-16-001), are included:</p>\n",
       "<ul>\n",
       "<li><b>AI-Assisted Geospatial Data Wrangling</b>: AI-Assisted Geospatial Data Wrangling (Base: $950.00, 28 hrs)</li>\n",
       "<li><b>Satellite Image Analysis with AI</b>: Satellite Image Analysis with AI (Base: $1200.00, 36 hrs)</li>\n",
       "<li><b>Spatial Predictive Modeling</b>: Spatial Predictive Modeling (Base: $1500.00, 40 hrs)</li>\n",
       "<li><b>Geospatial React Dashboard</b>: Geospatial React Dashboard (Base: $1400.00, 40 hrs)</li>\n",
       "<li><b>AI Integration for Geospatial</b>: AI Integration for Geospatial (Base: $900.00, 28 hrs)</li>\n",
       "</ul>\n",
       "\n",
       "<p>Key features to be delivered include:</p>\n",
       "<ul>\n",
       "<li>Near real-time monitoring capabilities</li>\n",
       "<li>Predict ignition risk</li>\n",
       "<li>Predict potential spread patterns</li>\n",
       "<li>Web dashboard for analysts</li>\n",
       "<li>API for integrating alerts into existing systems</li>\n",
       "<li>Regular risk assessment reports</li>\n",
       "</ul>\n",
       "\n",
       "<p><em>Includes custom development considerations for: Achieving the desired 85-90% precision in identifying high-risk zones may require custom AI model fine-tuning and optimization., Integration with specific existing systems will require custom API development and configuration., Development of regular risk assessment reports will require custom report generation logic and formatting.</em></p>\n",
       "\n",
       "<h3>3.2. Out-of-Scope</h3>\n",
       "<p>The following are explicitly excluded:\n",
       "*   Ongoing maintenance beyond a standard 30-day warranty period post-completion.\n",
       "*   Hardware procurement or setup.\n",
       "*   Data acquisition or licensing costs unless specified in line items.\n",
       "*   Training beyond basic handover documentation.\n",
       "*   Work not explicitly listed in section 3.1.</p>\n",
       "<h2>4. Technical Specifications</h2>\n",
       "<ul>\n",
       "<li><strong>Key Technologies:</strong>\n",
       "    Utilize Convolutional Neural Networks (CNNs) for satellite imagery analysis. Employ Google Earth Engine (GEE) for data retrieval and analysis. Integrate TensorFlow and PyTorch for predictive modeling. Combine satellite data with drone imagery for multi-layered monitoring. Focus on early detection systems and AI-driven geospatial analysis. Train AI models on historical data to improve accuracy. Consider FlyPix AI for geospatial analysis solutions.</li>\n",
       "<li><strong>Data Sources:</strong>\n",
       "    MODIS satellite imagery, VIIRS satellite imagery, Landsat satellite imagery, Sentinel satellite imagery, Historical fire perimeter data, Weather data, Topographical data</li>\n",
       "<li><strong>Performance Requirements:</strong> Accuracy target: 85-90% precision in identifying high-risk zones.</li>\n",
       "<li><strong>Outputs:</strong>\n",
       "    Web dashboard, API, Regular risk assessment reports</li>\n",
       "</ul>\n",
       "<h2>5. Deliverables &amp; Acceptance Criteria</h2>\n",
       "<p>Major deliverables correspond to the services listed in 3.1.</p>\n",
       "<ul>\n",
       "<li><b>AI-Assisted Geospatial Data Wrangling Deliverable</b>: Corresponds to service AI-Assisted Geospatial Data Wrangling.<br><ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to 'Near real-time monitoring capabilities' as described.</li></ul></li>\n",
       "<li><b>Satellite Image Analysis with AI Deliverable</b>: Corresponds to service Satellite Image Analysis with AI.<br><ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to 'Predict ignition risk' as described.</li></ul></li>\n",
       "<li><b>Spatial Predictive Modeling Deliverable</b>: Corresponds to service Spatial Predictive Modeling.<br><ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to 'Predict potential spread patterns' as described.</li></ul></li>\n",
       "<li><b>Geospatial React Dashboard Deliverable</b>: Corresponds to service Geospatial React Dashboard.<br><ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to 'Web dashboard for analysts' as described.</li></ul></li>\n",
       "<li><b>AI Integration for Geospatial Deliverable</b>: Corresponds to service AI Integration for Geospatial.<br><ul><li><b>Acceptance Criteria:</b> Successfully delivers functionality related to 'API for integrating alerts into existing systems' as described.</li></ul></li>\n",
       "</ul>\n",
       "\n",
       "<h2>6. Project Timeline</h2>\n",
       "<ul>\n",
       "<li><strong>Estimated Start Date:</strong> 2025-04-23</li>\n",
       "<li><strong>Estimated Completion Date:</strong> 2025-10-08</li>\n",
       "<li><strong>Total Estimated Effort:</strong> Approximately 230 hours.</li>\n",
       "</ul>\n",
       "<h2>7. Roles &amp; Responsibilities</h2>\n",
       "<ul>\n",
       "<li><strong>Provider (Acme Inc.):</strong> Project management, development, testing, deployment, documentation.</li>\n",
       "<li><strong>Client (Wildfire Watch North America):</strong> Provide timely access to data sources (MODIS satellite imagery, VIIRS satellite imagery, Landsat satellite imagery, Sentinel satellite imagery, Historical fire perimeter data, Weather data, Topographical data), provide subject matter expertise, assign a primary point of contact (Jane Smith), participate in reviews, provide timely feedback/approvals.</li>\n",
       "</ul>\n",
       "<h2>8. Pricing &amp; Payment Schedule</h2>\n",
       "<ul>\n",
       "<li><strong>Total Project Price:</strong> $33292.50</li>\n",
       "</ul>\n",
       "<h3>8.1. Price Breakdown</h3>\n",
       "<table>\n",
       "<tr><th>Item</th><th>Estimated Cost</th><th>Notes</th></tr>\n",
       "<tr><td>Standard Service: AI-Assisted Geospatial Data Wrangling</td><td>$950.00</td><td>Base price for standard offering</td></tr>\n",
       "<tr><td>Standard Service: Satellite Image Analysis with AI</td><td>$1200.00</td><td>Base price for standard offering</td></tr>\n",
       "<tr><td>Standard Service: Spatial Predictive Modeling</td><td>$1500.00</td><td>Base price for standard offering</td></tr>\n",
       "<tr><td>Standard Service: Geospatial React Dashboard</td><td>$1400.00</td><td>Base price for standard offering</td></tr>\n",
       "<tr><td>Standard Service: AI Integration for Geospatial</td><td>$900.00</td><td>Base price for standard offering</td></tr>\n",
       "<tr><td>Additional Development & Lifecycle</td><td>$23000.00</td><td>Custom work, testing, PM, etc. (230 hrs)</td></tr>\n",
       "<tr><td>Overhead & Contingency (15%)</td><td>$4342.50</td><td>Project complexity & risk buffer</td></tr>\n",
       "<tr><th>Total Estimated Project Price</th><th>$33292.50</th><td></td></tr>\n",
       "</table>\n",
       "\n",
       "<h3>8.2. Payment Schedule</h3>\n",
       "<table>\n",
       "<tr><th>Milestone</th><th>Percentage</th><th>Amount</th><th>Due Date</th></tr>\n",
       "<tr><td>Project Kick-off</td><td>30%</td><td>$9987.75</td><td>2025-04-23</td></tr>\n",
       "<tr><td>Mid-Project Review 1</td><td>30%</td><td>$9987.75</td><td>2025-06-04</td></tr>\n",
       "<tr><td>Mid-Project Review 2</td><td>30%</td><td>$9987.75</td><td>2025-08-06</td></tr>\n",
       "<tr><td>Project Completion & Handover</td><td>10%</td><td>$3329.25</td><td>2025-10-08</td></tr>\n",
       "</table>\n",
       "\n",
       "<h2>9. Assumptions</h2>\n",
       "<ul>\n",
       "<li>Client will provide access to specified data sources within 1 week of project start.</li>\n",
       "<li>Client point of contact will be available for weekly check-ins and provide feedback within 2 business days.</li>\n",
       "<li>Scope is fixed as defined in Section 3.1.</li>\n",
       "<li>Data quality from provided sources is sufficient for model training.</li>\n",
       "<li>Potential challenges identified during research include: Achieving desired accuracy with AI models, ensuring timely data acquisition and processing, and integrating with existing systems are potential challenges. Other challenges include improving satellite coverage, developing smarter AI tools, and making fire data more accessible.</li>\n",
       "</ul>\n",
       "<h2>10. Change Management</h2>\n",
       "<p>Any requested changes to scope, deliverables, or timeline must be submitted via a formal Change Request process. Approved changes may impact cost and schedule.</p>\n",
       "<h2>11. Reporting</h2>\n",
       "<p>Provider will provide brief weekly status updates via email and hold bi-weekly review meetings with the Client.</p>\n",
       "<h2>12. Disclaimer</h2>\n",
       "<p>Disclaimer: This Statement of Work was generated with AI assistance based on the approved quote (Quote ID: Q-2025-04-16-001). It requires final human review and approval by both Provider and Client before execution.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the SOW\n",
    "final_sow = create_final_sow(quote_result, analysis_data)\n",
    "\n",
    "# Display as markdown, using display and HTML to bypass Kaggle's markdown limitations\n",
    "display(HTML(md.markdown(final_sow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e27be3",
   "metadata": {},
   "source": [
    "---\n",
    "## Findings, Limitations, and Next Steps\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "- The GenAI quoting agent can automate much of the requirements extraction, service recommendation, and quote generation process for AI-powered geospatial projects.\n",
    "- The use of multi-agent orchestration, vector search, and structured output enables modular, auditable, and extensible workflows.\n",
    "- Automated SOW generation from structured quote and analysis ensures professional, consistent client documentation with minimal manual effort.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- The QA agent currently only reviews and recommends changes. It does not trigger an automatic update or re-run of the quote if issues are found.\n",
    "- There is no feedback loop to update the quote based on QA recommendations. This could be improved by adding a third agent to revise the quote or by making the QA output actionable for a human reviewer.\n",
    "- The QA output could be oriented toward the company team, highlighting what to review before finalizing the quote with the client.\n",
    "- The SOW is generated based on available outputs, even if the QA agent flags issues. A stricter gating mechanism could be considered.\n",
    "\n",
    "**Next Steps / Future Improvements:**\n",
    "\n",
    "- Add a third agent or workflow step to revise the quote based on QA recommendations, enabling a closed feedback loop.\n",
    "- Enhance the QA output to provide actionable checklists for human reviewers.\n",
    "- Expand the system to handle more complex negotiation scenarios or multi-round quoting.\n",
    "- Improve error handling and user experience for non-technical users.\n",
    "- Implement the final solution as fully featured cloud solution using Mastra.ai, within an existing project, a new dedicated project/backend, or even as MCP server. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ec6d9",
   "metadata": {},
   "source": [
    "## References & Acknowledgements\n",
    "\n",
    "- [Kaggle Gen AI Intensive Course Capstone 2025Q1](https://www.kaggle.com/competitions/gen-ai-intensive-course-capstone-2025q1)\n",
    "- [CrewAI](https://docs.crewai.com/introduction)\n",
    "- [LangChain](https://python.langchain.com/docs/introduction/)\n",
    "- [Chroma Vector DB](https://docs.trychroma.com/docs/overview/introduction)\n",
    "- [Google Generative AI](https://ai.google.dev/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
