{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024b909b",
   "metadata": {},
   "source": [
    "local notebook imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045fdd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f00a65",
   "metadata": {},
   "source": [
    "Kaggle env settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd4255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
    "# !pip install -qU crewai chromadb langchain langchain-chroma langchain-google-genai langgraph-prebuilt google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdd732",
   "metadata": {},
   "source": [
    "Kaggle imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c6eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Type, Optional, Literal\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import math\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from textwrap import dedent\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4a5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d3b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_retriable = retry.if_exception_type(\n",
    "    google_exceptions.ResourceExhausted,  # Catches 429 errors\n",
    "    google_exceptions.DeadlineExceeded,\n",
    "    google_exceptions.InternalServerError,\n",
    "    google_exceptions.ServiceUnavailable,  # Catches 503 errors\n",
    ")\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, \"__wrapped__\"):\n",
    "    genai.models.Models.generate_content = retry.Retry(\n",
    "        predicate=is_retriable,\n",
    "        initial=5.0,\n",
    "        maximum=60.0,\n",
    "        multiplier=2.0,\n",
    "        deadline=300.0,\n",
    "    )(genai.models.Models.generate_content)\n",
    "\n",
    "\n",
    "# Create a callback function to introduce delays between tasks\n",
    "def delay_before_task(task_output):\n",
    "    delay_seconds = 10\n",
    "    print(\n",
    "        f\"\\\\n--- Task Completed. Waiting {delay_seconds} seconds before next task --- \\\\n\"\n",
    "    )\n",
    "    time.sleep(delay_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25dfb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "standards_dataset_path = Path(\"capstone-2025-quote-standards-v2.json\")\n",
    "client_personas_path = Path(\"capstone-2025-client-personas-v2.json\")\n",
    "\n",
    "with standards_dataset_path.open(\"r\") as f:\n",
    "    standards_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700ef338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'agent',\n",
       "  'message': 'Hello! Thank you for contacting us. Could you please tell me your name and the company you represent?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"Hi, my name is Jane Smith, and I'm from Wildfire Watch North America.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Nice to meet you, Jane. How can we help Wildfire Watch North America today?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"We're looking for a quote for a geospatial project focused on wildfire monitoring across Canada and the US. We need to build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"Okay, that's a critical Geospatial AI project. What specific data sources are you considering for both the historical database and the ongoing monitoring?\"},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'We plan to use satellite imagery like MODIS, VIIRS, Landsat, and Sentinel, along with historical fire perimeter data, weather data, and potentially topographical data.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"That's a robust set of sources. For the predictive aspect, what key factors are you hoping the AI model will help predict? Fire spread, ignition risk, or something else?\"},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'Primarily ignition risk and potential spread patterns. We also need near real-time monitoring capabilities.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'What is the desired output for this system? A web dashboard for visualization, an API for integration, or detailed reports?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'We need a web dashboard for our analysts and an API for integrating alerts into our existing systems. Regular risk assessment reports would also be valuable.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Do you have specific performance requirements for the predictive model, perhaps in terms of accuracy or lead time for predictions?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': \"We're aiming for high accuracy in identifying high-risk zones, maybe around 85-90% precision, and timely alerts for new ignitions.\"},\n",
       " {'speaker': 'agent',\n",
       "  'message': 'Understood. And what is your approximate budget for this initial phase, covering the database build, data gathering, and initial model development?'},\n",
       " {'speaker': 'customer',\n",
       "  'message': 'Our budget is around $40,000 for this phase.'},\n",
       " {'speaker': 'agent',\n",
       "  'message': \"Thank you, Jane. That gives us a clear picture. We'll start developing a proposal for Wildfire Watch North America.\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_conversation(\n",
    "    persona_id: Literal[\"wildfire_watch_na\", \"urban_traffic_ai\", \"agri_insights\"],\n",
    "    dataset_path: str | Path,\n",
    ") -> List[Dict[str, str]]:\n",
    "    if not isinstance(dataset_path, Path):\n",
    "        dataset_path = Path(dataset_path)\n",
    "\n",
    "    with dataset_path.open(\"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for persona in data[\"personas\"]:\n",
    "        if persona[\"id\"] == persona_id:\n",
    "            return persona[\"conversation\"]\n",
    "\n",
    "\n",
    "conversation = load_conversation(\n",
    "    persona_id=\"wildfire_watch_na\", dataset_path=client_personas_path\n",
    ")\n",
    "\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4230212",
   "metadata": {},
   "source": [
    "### Chroma vectorstore creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727bc532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to flatten and transform the json dataset\n",
    "def _flatten_json_data(data: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"Convert nested JSON service data into a list of Document objects.\"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for service_category, services in data[\"services\"].items():\n",
    "        for service in services:\n",
    "            # Create metadata with relevant fields\n",
    "            metadata = {\n",
    "                \"uid\": str(uuid.uuid4()),\n",
    "                \"name\": service.get(\"name\", \"\"),\n",
    "                \"label\": service.get(\"label\", \"\"),\n",
    "                \"price\": service.get(\"price\", 0),\n",
    "                \"parentCategory\": service_category,\n",
    "                \"subCategory\": service.get(\"category\", \"\"),\n",
    "                \"developmentTime\": service.get(\"developmentTime\", 0),\n",
    "                \"description\": service.get(\"description\", \"\"),\n",
    "            }\n",
    "\n",
    "            # Combine fields for the document page content\n",
    "            page_content = f\"\"\"\n",
    "            Service: {service.get(\"label\", \"\")}\n",
    "            Parent Category: {service_category}\n",
    "            Sub Category: {service.get(\"category\", \"\")}\n",
    "            Price: ${service.get(\"price\", 0)}\n",
    "            Description: {service.get(\"description\", \"\")}\n",
    "            Development Time: {service.get(\"developmentTime\", 0)} hours\n",
    "            Deliverables: {\", \".join(service.get(\"deliverables\", []))}\n",
    "            Use Cases: {\", \".join(service.get(\"useCases\", []))}\n",
    "            \"\"\"\n",
    "\n",
    "            # Create a Document object\n",
    "            doc = Document(page_content=page_content, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Create the document objects from json input\n",
    "docs = _flatten_json_data(standards_dataset)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37efa6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "            Service: Satellite Image Analysis with AI\n",
      "            Parent Category: geospatialAI\n",
      "            Sub Category: ai\n",
      "            Price: $1200\n",
      "            Description: Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection\n",
      "            Development Time: 36 hours\n",
      "            Deliverables: Custom trained model on your imagery, Web dashboard for uploading and analyzing new images, API for automated processing pipelines, Technical documentation and training\n",
      "            Use Cases: Agricultural crop health monitoring, Urban development tracking, Disaster damage assessment, Environmental change monitoring\n",
      "            ' metadata={'uid': 'd8763c20-3557-48b7-911e-dc46b515a606', 'name': 'SatelliteImageAnalysis', 'label': 'Satellite Image Analysis with AI', 'price': 1200, 'parentCategory': 'geospatialAI', 'subCategory': 'ai', 'developmentTime': 36, 'description': 'Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection'}\n"
     ]
    }
   ],
   "source": [
    "# Collect all documents lenghts\n",
    "docs_length = [len(doc.page_content) for doc in docs]\n",
    "\n",
    "# Split documents only if they are not all under 100 chars\n",
    "if not all(i < 1000 for i in docs_length):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Sample doc for sanity check\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2162b7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the embedding function\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\", google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "chroma_store = Chroma(\n",
    "    collection_name=\"kaggle_capstone\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "\n",
    "# Collect the ids from metadata\n",
    "ids = [doc.metadata[\"uid\"] for doc in docs]\n",
    "\n",
    "chroma_store.add_documents(documents=docs, ids=ids)\n",
    "\n",
    "# Sanity check the collection is not empty\n",
    "chroma_store._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd401a",
   "metadata": {},
   "source": [
    "### Output Pydantic schemas definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4298e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalRequirements(BaseModel):\n",
    "    accuracy: Optional[str] = Field(\n",
    "        None, description=\"Required accuracy or performance metrics.\"\n",
    "    )\n",
    "    outputs: Optional[List[str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of desired output formats (e.g., 'Web dashboard', 'API', 'Report').\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RequirementsOutput(BaseModel):\n",
    "    client_name: Optional[str] = Field(None, description=\"Name of the client contact.\")\n",
    "    client_company: Optional[str] = Field(\n",
    "        None, description=\"Name of the client's company.\"\n",
    "    )\n",
    "    project_type: Optional[str] = Field(\n",
    "        None, description=\"Type of project (e.g., Geospatial AI, Data Automation).\"\n",
    "    )\n",
    "    main_objective: Optional[str] = Field(\n",
    "        None, description=\"The primary goal of the project.\"\n",
    "    )\n",
    "    data_sources: List[str] = Field(\n",
    "        default_factory=list, description=\"List of data sources mentioned.\"\n",
    "    )\n",
    "    features_needed: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of required features or functionalities.\",\n",
    "    )\n",
    "    technical_requirements: Optional[TechnicalRequirements] = Field(\n",
    "        None, description=\"Specific technical needs.\"\n",
    "    )\n",
    "    budget: Optional[str] = Field(None, description=\"Client's stated budget or range.\")\n",
    "    timeline_expectation: Optional[str] = Field(\n",
    "        None, description=\"Any mention of desired timeline or deadlines.\"\n",
    "    )\n",
    "    potential_risks: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Potential challenges or risks identified from the conversation.\",\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise Markdown summary (1-2 paragraphs) highlighting the core project request and key constraints.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class RecommendedService(BaseModel):\n",
    "    name: str = Field(..., description=\"Internal name/ID of the service.\")\n",
    "    label: str = Field(..., description=\"Customer-facing label of the service.\")\n",
    "    price: float = Field(..., description=\"Price of the service.\")\n",
    "    development_time_hours: int = Field(\n",
    "        ..., description=\"Estimated development time in hours.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ServicePackage(BaseModel):\n",
    "    name: str = Field(..., description=\"A descriptive name for the proposed package.\")\n",
    "    services_included: List[str] = Field(\n",
    "        ..., description=\"List of labels of the services included in the package.\"\n",
    "    )\n",
    "    total_estimated_price: float = Field(\n",
    "        ..., description=\"Total calculated price for the package.\"\n",
    "    )\n",
    "    total_estimated_time_hours: int = Field(\n",
    "        ..., description=\"Total calculated development time in hours for the package.\"\n",
    "    )\n",
    "    notes: Optional[str] = Field(\n",
    "        None, description=\"Brief notes about the package coverage.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class BudgetAnalysis(BaseModel):\n",
    "    client_budget: Optional[str] = Field(\n",
    "        ..., description=\"The client's budget extracted from requirements.\"\n",
    "    )\n",
    "    estimated_cost: float = Field(\n",
    "        ..., description=\"The total estimated cost of the proposed package.\"\n",
    "    )\n",
    "    within_budget: bool = Field(\n",
    "        ..., description=\"Whether the estimated cost is within the client's budget.\"\n",
    "    )\n",
    "    comments: Optional[str] = Field(None, description=\"Comments on the budget fit.\")\n",
    "\n",
    "\n",
    "class ServiceAnalysisOutput(BaseModel):\n",
    "    recommended_services: List[RecommendedService] = Field(\n",
    "        ..., description=\"List of relevant services found in the vector store.\"\n",
    "    )\n",
    "    service_package: ServicePackage = Field(\n",
    "        ..., description=\"Details of the proposed service package.\"\n",
    "    )\n",
    "    customization_notes: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of requirements potentially needing custom work.\",\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Markdown explanation justifying the service selection, cost/time, budget comparison, and customization needs.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class WebResearchOutput(BaseModel):\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"Brief overview of key findings regarding best practices and technologies.\",\n",
    "    )\n",
    "    best_practices_insights: str = Field(\n",
    "        ...,\n",
    "        description=\"Actionable recommendations and notes on relevant tools/libraries found.\",\n",
    "    )\n",
    "    potential_challenges: Optional[str] = Field(\n",
    "        None, description=\"Potential challenges identified from search results, if any.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteCalculationDetails(BaseModel):\n",
    "    \"\"\"Detailed breakdown of the quote price calculation.\"\"\"\n",
    "\n",
    "    sum_base_service_prices: float = Field(\n",
    "        ..., description=\"Sum of prices for all recommended standard services.\"\n",
    "    )\n",
    "    total_estimated_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"Total estimated hours including base, custom work, and lifecycle.\",\n",
    "    )\n",
    "    base_hourly_rate: int = Field(\n",
    "        ..., description=\"The hourly rate used for development cost calculation.\"\n",
    "    )\n",
    "    additional_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"Hours allocated beyond standard service hours (for custom work, lifecycle, etc.).\",\n",
    "    )\n",
    "    additional_development_cost: float = Field(\n",
    "        ..., description=\"Calculated cost for the additional hours.\"\n",
    "    )\n",
    "    subtotal_before_overhead: float = Field(\n",
    "        ...,\n",
    "        description=\"Total cost before applying the overhead buffer (Base Services + Additional Development).\",\n",
    "    )\n",
    "    overhead_buffer_percentage: float = Field(\n",
    "        ..., description=\"The overhead buffer percentage applied.\"\n",
    "    )\n",
    "    overhead_amount: float = Field(\n",
    "        ..., description=\"The calculated monetary amount of the overhead buffer.\"\n",
    "    )\n",
    "    final_total_price: float = Field(\n",
    "        ..., description=\"The final calculated total price for the quote.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteSummary(BaseModel):\n",
    "    project_name: str\n",
    "    client_name: Optional[str]\n",
    "    client_company: Optional[str]\n",
    "    quote_id: str\n",
    "    quote_date: str\n",
    "    total_price: float\n",
    "    total_estimated_hours: int\n",
    "    estimated_start_date: str\n",
    "    estimated_completion_date: str\n",
    "    estimation_justification: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Brief justification for the estimated hours and overhead buffer applied.\",\n",
    "    )\n",
    "    price_calculation_details: Optional[QuoteCalculationDetails] = Field(\n",
    "        None,\n",
    "        description=\"Detailed breakdown of the price calculation from the QuoteCalculatorTool.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuoteLineItem(BaseModel):\n",
    "    service_name: str\n",
    "    description: str\n",
    "    price: float\n",
    "    estimated_hours: int\n",
    "\n",
    "\n",
    "class PaymentMilestone(BaseModel):\n",
    "    milestone: str\n",
    "    percentage: int\n",
    "    amount: float\n",
    "    due_date: str\n",
    "\n",
    "\n",
    "class QuoteDetailsOutput(BaseModel):\n",
    "    quote_summary: QuoteSummary\n",
    "    line_items: List[QuoteLineItem]\n",
    "    payment_schedule: List[PaymentMilestone]\n",
    "\n",
    "\n",
    "class QAIssue(BaseModel):\n",
    "    type: str = Field(\n",
    "        ...,\n",
    "        description=\"Category of the issue (e.g., pricing, timeline, scope, requirements_mismatch, clarity, consistency).\",\n",
    "    )\n",
    "    severity: str = Field(..., description=\"Severity level (e.g., high, medium, low).\")\n",
    "    description: str = Field(\n",
    "        ..., description=\"Specific description of the issue found.\"\n",
    "    )\n",
    "    recommendation: str = Field(..., description=\"Specific instruction for correction.\")\n",
    "\n",
    "\n",
    "class QAImprovementSuggestion(BaseModel):\n",
    "    area: str = Field(\n",
    "        ...,\n",
    "        description=\"Area for improvement (e.g., Executive Summary, Timeline Detail).\",\n",
    "    )\n",
    "    suggestion: str = Field(..., description=\"Specific suggestion.\")\n",
    "\n",
    "\n",
    "class QAReviewOutput(BaseModel):\n",
    "    overall_status: str = Field(\n",
    "        ..., description=\"Overall assessment status: 'approved' or 'needs_revision'.\"\n",
    "    )\n",
    "    issues: List[QAIssue] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of identified issues if status is 'needs_revision'.\",\n",
    "    )\n",
    "    improvement_suggestions: List[QAImprovementSuggestion] = Field(\n",
    "        default_factory=list, description=\"Optional suggestions for improvement.\"\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ..., description=\"A brief Markdown summary of the review findings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class AnalysisOutput(BaseModel):\n",
    "    \"\"\"Consolidated output from the Data Analysis Crew.\"\"\"\n",
    "\n",
    "    requirements: RequirementsOutput = Field(\n",
    "        ..., description=\"Extracted client requirements and project context.\"\n",
    "    )\n",
    "    research: WebResearchOutput = Field(\n",
    "        ..., description=\"Web research findings on technologies and best practices.\"\n",
    "    )\n",
    "    service_analysis: ServiceAnalysisOutput = Field(\n",
    "        ..., description=\"Analysis of internal services, pricing, and budget fit.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0c942",
   "metadata": {},
   "source": [
    "### Custom tools definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86da37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining crewAI custom tools Pydantic schemas\n",
    "class QuoteCalculatorToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Quote Calculator tool.\"\"\"\n",
    "\n",
    "    estimated_total_hours: int = Field(\n",
    "        ...,\n",
    "        description=\"The final estimated total development hours for the project, adjusted for complexity and custom work.\",\n",
    "    )\n",
    "    recommended_services: List[RecommendedService] = Field(\n",
    "        ..., description=\"List of recommended services with their base prices.\"\n",
    "    )\n",
    "    overhead_buffer_percentage: float = Field(\n",
    "        ...,\n",
    "        description=\"Overhead buffer percentage (e.g., 0.15 for 15%) estimated based on project risk and complexity.\",\n",
    "    )\n",
    "    base_hourly_rate: int = Field(\n",
    "        100, description=\"The base hourly rate for development time.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GoogleSearhToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Google Search tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ..., description=\"The search query to be used for Google Search.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class VectorStoreSearchToolInput(BaseModel):\n",
    "    \"\"\"Input schema for the Vector Store Search tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"The search query to find relevant services in the vector store.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Defining crewAI custom tools\n",
    "class GoogleSearchTool(BaseTool):\n",
    "    name: str = \"GoogleSearch\"\n",
    "    description: str = \"A tool to perform Google searches.\"\n",
    "    args_schema: Type[BaseModel] = GoogleSearhToolInput\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use Google Search to find relevant information.\"\"\"\n",
    "\n",
    "        try:\n",
    "            client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "            config_with_search = types.GenerateContentConfig(\n",
    "                tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    "            )\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=query,\n",
    "                config=config_with_search,\n",
    "            )\n",
    "\n",
    "            return [\n",
    "                (\"\\n-----\\n\").join([i.text for i in res.content.parts if i.text])\n",
    "                for res in response.candidates\n",
    "            ][0]\n",
    "        except Exception as e:\n",
    "            return f\"Error performing Google search: {e}\"\n",
    "\n",
    "\n",
    "class QuoteCalculatorTool(BaseTool):\n",
    "    name: str = \"QuoteCalculator\"\n",
    "    description: str = (\n",
    "        \"Calculates the final project quote price and provides a detailed breakdown. \"\n",
    "        \"Input: estimated total hours, list of recommended services (with base prices), overhead buffer percentage, and base hourly rate. \"\n",
    "        \"Output: A structured object containing base service costs, additional development costs, overhead amount, and the final total price.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = QuoteCalculatorToolInput\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        estimated_total_hours: int,\n",
    "        recommended_services: List[Any],\n",
    "        overhead_buffer_percentage: float,\n",
    "        base_hourly_rate: int = 100,\n",
    "    ) -> QuoteCalculationDetails:\n",
    "        \"\"\"Calculate the final quote price and return a detailed breakdown.\"\"\"\n",
    "        try:\n",
    "            if not 0.0 <= overhead_buffer_percentage <= 0.3:\n",
    "                print(\n",
    "                    f\"Warning: Clamping overhead buffer {overhead_buffer_percentage}.\"\n",
    "                )\n",
    "                overhead_buffer_percentage = max(\n",
    "                    0.0, min(0.3, overhead_buffer_percentage)\n",
    "                )\n",
    "\n",
    "            # Calculate sum_base_service_prices and sum_base_service_hours\n",
    "            sum_base_service_prices = 0.0\n",
    "            sum_base_service_hours = 0\n",
    "\n",
    "            for service in recommended_services:\n",
    "                price = 0.0\n",
    "                hours = 0\n",
    "                if isinstance(service, dict):\n",
    "                    price = service.get(\"price\", 0.0)\n",
    "                elif hasattr(service, \"price\") and hasattr(\n",
    "                    service, \"development_time_hours\"\n",
    "                ):\n",
    "                    price = getattr(service, \"price\", 0.0)\n",
    "                    hours = getattr(service, \"development_time_hours\", 0)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping unknown service type: {type(service)}\")\n",
    "                    continue\n",
    "                sum_base_service_prices += price\n",
    "                sum_base_service_hours += hours\n",
    "\n",
    "            # Calculate additional hours and cost\n",
    "            additional_hours = max(0, estimated_total_hours - sum_base_service_hours)\n",
    "            additional_development_cost = round(additional_hours * base_hourly_rate, 2)\n",
    "\n",
    "            # Calculate subtotal and overhead amount\n",
    "            subtotal_before_overhead = round(\n",
    "                sum_base_service_prices + additional_development_cost, 2\n",
    "            )\n",
    "            overhead_amount = round(\n",
    "                subtotal_before_overhead * overhead_buffer_percentage, 2\n",
    "            )\n",
    "\n",
    "            # Calculate final price\n",
    "            final_total_price = round(subtotal_before_overhead + overhead_amount, 2)\n",
    "\n",
    "            details = QuoteCalculationDetails(\n",
    "                sum_base_service_prices=sum_base_service_prices,\n",
    "                total_estimated_hours=estimated_total_hours,\n",
    "                base_hourly_rate=base_hourly_rate,\n",
    "                additional_hours=additional_hours,\n",
    "                additional_development_cost=additional_development_cost,\n",
    "                subtotal_before_overhead=subtotal_before_overhead,\n",
    "                overhead_buffer_percentage=overhead_buffer_percentage,\n",
    "                overhead_amount=overhead_amount,\n",
    "                final_total_price=final_total_price,\n",
    "            )\n",
    "\n",
    "            # Print for verbose output\n",
    "            print(\n",
    "                f\"Calculator Input: Hours={estimated_total_hours}, Services Base={sum_base_service_prices}, Buffer={overhead_buffer_percentage}, Rate={base_hourly_rate}\"\n",
    "            )\n",
    "            print(f\"Calculated Details: {details.model_dump_json(indent=2)}\")\n",
    "\n",
    "            return details\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in QuoteCalculatorTool: {e}\")\n",
    "            print(\n",
    "                f\"Input causing error: hours={estimated_total_hours}, services={recommended_services}, buffer={overhead_buffer_percentage}, rate={base_hourly_rate}\"\n",
    "            )\n",
    "            raise ValueError(f\"Error calculating quote details: {e}\")\n",
    "\n",
    "\n",
    "class VectorStoreSearchTool(BaseTool):\n",
    "    name: str = \"VectorStoreSearch\"\n",
    "    description: str = \"A tool to search for relevant services in the vector store.\"\n",
    "    args_schema: Type[BaseModel] = VectorStoreSearchToolInput\n",
    "    vector_store: Chroma\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use the vector store to find relevant documents.\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(f\"Querying Vector Store with: {query}\")\n",
    "            results = self.vector_store.similarity_search_with_score(query, k=10)\n",
    "\n",
    "            # Format results for the agent\n",
    "            formatted_results = []\n",
    "            for doc, score in results:\n",
    "                metadata = doc.metadata\n",
    "                formatted_results.append(\n",
    "                    f\"Result (Score: {score:.4f}):\\n\"\n",
    "                    f\"  Name: {metadata.get('name', 'N/A')}\\n\"\n",
    "                    f\"  Label: {metadata.get('label', 'N/A')}\\n\"\n",
    "                    f\"  Price: ${metadata.get('price', 'N/A')}\\n\"\n",
    "                    f\"  Development Time: {metadata.get('developmentTime', 'N/A')} hours\\n\"\n",
    "                    f\"  Category: {metadata.get('parentCategory', 'N/A')} / {metadata.get('subCategory', 'N/A')}\\n\"\n",
    "                    f\"  Description: {metadata.get('description', 'N/A')}\\n\"\n",
    "                )\n",
    "            if not formatted_results:\n",
    "                return \"No relevant services found in the vector store for the query.\"\n",
    "\n",
    "            return \"\\n-----\\n\".join(formatted_results)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error searching vector store: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfccdd",
   "metadata": {},
   "source": [
    "### Crew #1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0b21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew 1: Data Analysis & Service Recommendation\n",
    "class DataAnalysisCrew:\n",
    "    def __init__(\n",
    "        self, vector_store: Chroma, conversation: List[Dict[str, str]]\n",
    "    ) -> None:\n",
    "        self.llm = LLM(\n",
    "            model=\"gemini/gemini-2.0-flash\",\n",
    "            temperature=0,\n",
    "            api_key=GOOGLE_API_KEY,\n",
    "            num_retries=5,\n",
    "            request_timeout=80,\n",
    "            seed=42,\n",
    "        )\n",
    "        self.conversation = conversation\n",
    "        self.search_tool = GoogleSearchTool()\n",
    "        self.vector_store_tool = VectorStoreSearchTool(vector_store=vector_store)\n",
    "        self._tasks = {}\n",
    "\n",
    "    # Agent #1 - Context Extraction Agent\n",
    "    def context_extraction_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Project Context and Requirements Extraction Specialist\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Analyze the provided client conversation transcript. Identify and extract key information including:\n",
    "                - Client Name and Company\n",
    "                - Project Type (e.g., geospatial analysis, data automation, AI model development)\n",
    "                - Main Objectives and Goals\n",
    "                - Specific Features or Functionalities required\n",
    "                - Mentioned Data Sources (e.g., satellite imagery types, specific databases)\n",
    "                - Technical Requirements (e.g., accuracy targets, performance needs, output formats like dashboards, APIs)\n",
    "                - Budget constraints or indications\n",
    "                - Timeline expectations (if mentioned)\n",
    "                - Any potential challenges or risks implied\n",
    "                Structure this information clearly into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are a meticulous analyst specializing in understanding client needs from initial conversations.\n",
    "            Your strength lies in parsing dialogue to capture the core requirements, constraints, and context\n",
    "            of a potential project, organizing it into a structured Pydantic object that other team members can easily use.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #2 - Web Search Agent\n",
    "    def web_search_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Geospatial Technology & Market Researcher\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Based on the extracted project requirements (like project type, objectives, data sources, technical needs),\n",
    "                research current best practices, relevant technologies, potential challenges, and industry benchmarks\n",
    "                for similar geospatial projects using the GoogleSearch tool. Provide concise summaries and actionable insights\n",
    "                structured into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are an expert researcher skilled in finding up-to-date information on geospatial technologies,\n",
    "            AI applications in geography, data processing techniques, and market trends. You know how to\n",
    "            efficiently gather relevant case studies, technical documentation, and best practice guides\n",
    "            to inform project planning and estimation, outputting structured findings.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.search_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #3 - Vector Store Retrieval Agent\n",
    "    def service_retrieval_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Internal Service Catalog & Pricing Analyst\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Based on the extracted project requirements, use the VectorStoreSearch tool to find matching\n",
    "                internal services and standards. Identify a potential package of services, calculate the\n",
    "                total estimated price and development time, compare it against the client's budget,\n",
    "                note any requirement gaps needing custom work, and structure all findings into the required Pydantic format.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You have deep knowledge of the company's service offerings, standard components, and pricing structure,\n",
    "            stored within the vector database. You excel at translating client requirements into a combination\n",
    "            of existing services, estimating costs and timelines accurately, identifying customization needs,\n",
    "            and outputting a structured analysis in the specified Pydantic format.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.vector_store_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Task 1: Extract project context and requirements\n",
    "    def extract_requirements_task(self) -> Task:\n",
    "        formatted_conversation = \"\\n\".join(\n",
    "            [f\"{item['speaker']}: {item['message']}\" for item in self.conversation]\n",
    "        )\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Analyze the following client conversation transcript and extract key project details.\n",
    "            Focus on understanding the client's needs, goals, constraints, and any specific mentions\n",
    "            of technology, data, or deliverables.\n",
    "\n",
    "            CONVERSATION:\n",
    "            ---\n",
    "            {formatted_conversation}\n",
    "            ---\n",
    "\n",
    "            Extract the following: Client Name/Company, Project Type, Objectives, Required Features,\n",
    "            Data Sources, Technical Requirements (Accuracy, Outputs), Budget, Timeline hints, Potential Risks.\n",
    "            Also provide a concise Markdown summary of the project request and constraints.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `RequirementsOutput` schema.\n",
    "            This object should contain all the extracted details like client info, project type, objectives,\n",
    "            data sources, features, technical requirements, budget, timeline, risks, and a markdown summary.\n",
    "            Ensure all fields in the Pydantic model are populated accurately based on the conversation.\n",
    "            The final output MUST be only the JSON representation of the populated `RequirementsOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.context_extraction_agent(),\n",
    "            output_pydantic=RequirementsOutput,\n",
    "        )\n",
    "        self._tasks[\"extract_requirements\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 2: Research relevant information on the web\n",
    "    def web_research_task(self) -> Task:\n",
    "        task = Task(\n",
    "            description=\"\"\"\\\n",
    "            Based on the project requirements extracted in Task 1 (referencing the `project_type`, `main_objective`, `data_sources`, `technical_requirements` fields from the previous task's output),\n",
    "            conduct web research using the GoogleSearch tool. **Perform ONE focused search** combining the most critical aspects:\n",
    "            - Best practices for implementing similar geospatial AI projects (e.g., wildfire monitoring AI).\n",
    "            - Relevant technologies (specific libraries like TensorFlow/PyTorch/GeoPandas, platforms like Google Earth Engine/ArcGIS, algorithms for geospatial AI, satellite image processing).\n",
    "\n",
    "            Synthesize the findings from your search into the structured format required.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `WebResearchOutput` schema.\n",
    "            This object must contain:\n",
    "            - `summary`: Brief overview of key findings regarding best practices and technologies.\n",
    "            - `best_practices_insights`: Actionable recommendations and notes on relevant tools/libraries/platforms found (mention specific names if prominent in search results).\n",
    "            - `potential_challenges`: Potential challenges identified from search results, if any.\n",
    "\n",
    "            Focus on information directly applicable to building the quote and SOW. Keep the output concise but informative.\n",
    "            The final output MUST be only the JSON representation of the populated `WebResearchOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.web_search_agent(),\n",
    "            context=[self._tasks[\"extract_requirements\"]],\n",
    "            output_pydantic=WebResearchOutput,\n",
    "        )\n",
    "        self._tasks[\"web_research\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 3: Retrieve relevant services and calculate estimates\n",
    "    def service_retrieval_task(self) -> Task:\n",
    "        task = Task(\n",
    "            description=\"\"\"\\\n",
    "            Analyze the structured project requirements (output object from Task 1).\n",
    "            Use the 'VectorStoreSearch' tool to query the internal service catalog based on keywords derived from the requirements\n",
    "            (e.g., `project_type`, `features_needed`, `data_sources`, `technical_requirements`). Use a query like \"geospatial AI wildfire monitoring database model dashboard API\".\n",
    "\n",
    "            Identify the most relevant existing services from the tool's output. For each relevant service, extract its name, label, price, and development time.\n",
    "\n",
    "            Based on the findings:\n",
    "            1.  Select a combination of services that best address the client's needs to propose as a package.\n",
    "            2.  Calculate the total estimated *base* price and total estimated *base* development time (in hours) for this package by summing the values from the selected services.\n",
    "            3.  List any client requirements (from Task 1 output, e.g., `features_needed`, `technical_requirements.outputs`) that are NOT clearly covered by the selected standard services. These might require custom work.\n",
    "            4.  Write a brief Markdown explanation justifying the service choices, base calculations, and customization needs.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `ServiceAnalysisOutput` schema.\n",
    "            This object must contain:\n",
    "            - `recommended_services`: A list of `RecommendedService` objects for services found and deemed relevant, with their details (name, label, price, development_time_hours).\n",
    "            - `service_package`: A `ServicePackage` object detailing the proposed package including a descriptive name (e.g., \"Wildfire Monitoring AI - Phase 1\"), included service labels, calculated total *base* price, and calculated total *base* time.\n",
    "            - `customization_notes`: A list of strings describing requirements potentially needing custom work.\n",
    "            - `explanation`: A Markdown string summarizing the analysis and justifications.\n",
    "\n",
    "            Ensure all calculations (total base price, total base time) are performed correctly.\n",
    "            The final output MUST be only the JSON representation of the populated `ServiceAnalysisOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.service_retrieval_agent(),\n",
    "            context=[self._tasks[\"extract_requirements\"]],\n",
    "            output_pydantic=ServiceAnalysisOutput,\n",
    "        )\n",
    "        self._tasks[\"service_retrieval\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 4: Consolidate Analysis Output\n",
    "    def consolidate_analysis_task(self) -> Task:\n",
    "        return Task(\n",
    "            description=\"\"\"\\\n",
    "            Consolidate the outputs from the previous three tasks (Requirements Extraction, Web Research, Service Analysis)\n",
    "            into a single structured `AnalysisOutput` object. You have access to the outputs of these tasks via context.\n",
    "            Retrieve the `RequirementsOutput` object from Task 1 context.\n",
    "            Retrieve the `WebResearchOutput` object from Task 2 context.\n",
    "            Retrieve the `ServiceAnalysisOutput` object (which no longer contains budget analysis) from Task 3 context (your own previous output).\n",
    "            Combine these three objects into the fields (`requirements`, `research`, `service_analysis`) of the `AnalysisOutput` Pydantic model.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `AnalysisOutput` schema.\n",
    "            This object MUST contain the full outputs from Task 1, Task 2, and Task 3 nested within the appropriate fields.\n",
    "            The `service_analysis` field will reflect the structure without the preliminary budget check.\n",
    "            The final output MUST be only the JSON representation of the populated `AnalysisOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.service_retrieval_agent(),\n",
    "            context=[\n",
    "                self._tasks[\"extract_requirements\"],\n",
    "                self._tasks[\"web_research\"],\n",
    "                self._tasks[\"service_retrieval\"],\n",
    "            ],\n",
    "            output_pydantic=AnalysisOutput,\n",
    "        )\n",
    "\n",
    "    def crew(self) -> Crew:\n",
    "        \"\"\"Assemble the crew with all agents and tasks.\"\"\"\n",
    "        return Crew(\n",
    "            agents=[\n",
    "                self.context_extraction_agent(),\n",
    "                self.web_search_agent(),\n",
    "                self.service_retrieval_agent(),\n",
    "            ],\n",
    "            tasks=[\n",
    "                self.extract_requirements_task(),\n",
    "                self.web_research_task(),\n",
    "                self.service_retrieval_task(),\n",
    "                self.consolidate_analysis_task(),\n",
    "            ],\n",
    "            process=Process.sequential,\n",
    "            verbose=False,  # Turn off verbose for Kaggle demo\n",
    "            step_callback=delay_before_task,\n",
    "            max_rpm=15,\n",
    "            memory=True,\n",
    "            embedder={\n",
    "                \"provider\": \"google\",\n",
    "                \"config\": {\n",
    "                    \"api_key\": GOOGLE_API_KEY,\n",
    "                    \"model\": \"models/text-embedding-004\",\n",
    "                },\n",
    "            },\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b24a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Context and Requirements Extraction Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Analyze the following client conversation transcript and extract key project details.\n",
      "            Focus on understanding the client's needs, goals, constraints, and any specific mentions\n",
      "            of technology, data, or deliverables.\n",
      "\n",
      "            CONVERSATION:\n",
      "            ---\n",
      "            agent: Hello! Thank you for contacting us. Could you please tell me your name and the company you represent?\n",
      "customer: Hi, my name is Jane Smith, and I'm from Wildfire Watch North America.\n",
      "agent: Nice to meet you, Jane. How can we help Wildfire Watch North America today?\n",
      "customer: We're looking for a quote for a geospatial project focused on wildfire monitoring across Canada and the US. We need to build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management.\n",
      "agent: Okay, that's a critical Geospatial AI project. What specific data sources are you considering for both the historical database and the ongoing monitoring?\n",
      "customer: We plan to use satellite imagery like MODIS, VIIRS, Landsat, and Sentinel, along with historical fire perimeter data, weather data, and potentially topographical data.\n",
      "agent: That's a robust set of sources. For the predictive aspect, what key factors are you hoping the AI model will help predict? Fire spread, ignition risk, or something else?\n",
      "customer: Primarily ignition risk and potential spread patterns. We also need near real-time monitoring capabilities.\n",
      "agent: What is the desired output for this system? A web dashboard for visualization, an API for integration, or detailed reports?\n",
      "customer: We need a web dashboard for our analysts and an API for integrating alerts into our existing systems. Regular risk assessment reports would also be valuable.\n",
      "agent: Do you have specific performance requirements for the predictive model, perhaps in terms of accuracy or lead time for predictions?\n",
      "customer: We're aiming for high accuracy in identifying high-risk zones, maybe around 85-90% precision, and timely alerts for new ignitions.\n",
      "agent: Understood. And what is your approximate budget for this initial phase, covering the database build, data gathering, and initial model development?\n",
      "customer: Our budget is around $40,000 for this phase.\n",
      "agent: Thank you, Jane. That gives us a clear picture. We'll start developing a proposal for Wildfire Watch North America.\n",
      "            ---\n",
      "\n",
      "            Extract the following: Client Name/Company, Project Type, Objectives, Required Features,\n",
      "            Data Sources, Technical Requirements (Accuracy, Outputs), Budget, Timeline hints, Potential Risks.\n",
      "            Also provide a concise Markdown summary of the project request and constraints.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Context and Requirements Extraction Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"client_name\": \"Jane Smith\",\n",
      "  \"client_company\": \"Wildfire Watch North America\",\n",
      "  \"project_type\": \"Geospatial AI project\",\n",
      "  \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "  \"data_sources\": [\n",
      "    \"MODIS satellite imagery\",\n",
      "    \"VIIRS satellite imagery\",\n",
      "    \"Landsat satellite imagery\",\n",
      "    \"Sentinel satellite imagery\",\n",
      "    \"Historical fire perimeter data\",\n",
      "    \"Weather data\",\n",
      "    \"Topographical data\"\n",
      "  ],\n",
      "  \"features_needed\": [\n",
      "    \"Near real-time monitoring capabilities\",\n",
      "    \"Predict ignition risk\",\n",
      "    \"Predict potential spread patterns\",\n",
      "    \"Alert integration into existing systems\"\n",
      "  ],\n",
      "  \"technical_requirements\": {\n",
      "    \"accuracy\": \"Aiming for 85-90% precision in identifying high-risk zones\",\n",
      "    \"outputs\": [\n",
      "      \"Web dashboard for analysts\",\n",
      "      \"API for integration\",\n",
      "      \"Regular risk assessment reports\"\n",
      "    ]\n",
      "  },\n",
      "  \"budget\": \"$40,000\",\n",
      "  \"timeline_expectation\": null,\n",
      "  \"potential_risks\": [\n",
      "    \"Achieving desired accuracy (85-90%)\",\n",
      "    \"Timely alerts for new ignitions\"\n",
      "  ],\n",
      "  \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a historical wildfire database, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The desired outputs include a web dashboard, an API for system integration, and regular risk assessment reports. The budget for the initial phase is $40,000, with a focus on high accuracy (85-90%) in identifying high-risk zones and providing timely alerts.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Based on the project requirements extracted in Task 1 (referencing the `project_type`, `main_objective`, `data_sources`, `technical_requirements` fields from the previous task's output),\n",
      "            conduct web research using the GoogleSearch tool. **Perform ONE focused search** combining the most critical aspects:\n",
      "            - Best practices for implementing similar geospatial AI projects (e.g., wildfire monitoring AI).\n",
      "            - Relevant technologies (specific libraries like TensorFlow/PyTorch/GeoPandas, platforms like Google Earth Engine/ArcGIS, algorithms for geospatial AI, satellite image processing).\n",
      "\n",
      "            Synthesize the findings from your search into the structured format required.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to perform a Google search to find information on best practices and relevant technologies for building a geospatial AI solution for wildfire monitoring and prediction, considering the project's objectives, data sources, and technical requirements. The search query should combine these aspects to get the most relevant results.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mGoogleSearch\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"best practices geospatial AI wildfire monitoring prediction satellite imagery TensorFlow PyTorch Google Earth Engine ArcGIS\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Here's a breakdown of best practices for geospatial AI in wildfire monitoring and prediction, considering satellite imagery, TensorFlow, PyTorch, Google Earth Engine, and ArcGIS:\n",
      "\n",
      "**1. Data Acquisition and Preprocessing:**\n",
      "\n",
      "*   **Google Earth Engine (GEE):** Use GEE for efficient retrieval of geospatial data, accessing a wide range of public datasets and real-time information via its cloud-based platform. GEE is particularly useful for processing and analyzing remotely sensed data.\n",
      "*   **Data Cleaning:** Implement AI-driven data cleaning techniques to detect and correct errors or anomalies (misalignment, missing values) in remote sensing data.\n",
      "*   **High-Resolution Imagery:** Utilize higher resolution satellite or drone imagery to increase accuracy. Sentinel-2 imagery is useful for mapping burned areas and assessing severity.\n",
      "*   **Data Augmentation:** Employ data augmentation techniques like SimpleGAN, SparseGAN, and CGAN combined with sliding windows to improve the performance of deep learning models, especially when working with limited data.\n",
      "\n",
      "**2. Tools and Platforms**\n",
      "\n",
      "*   **TensorFlow and PyTorch:** Use these deep learning frameworks for predictive modeling, leveraging their strengths to optimize performance. A modular framework supporting both can be beneficial.\n",
      "*   **ArcGIS:** Utilize ArcGIS for visualizing TIFF files and producing maps. Real-time GIS platforms like Esri's ArcGIS can be used for real-time data processing.\n",
      "*   **Real-time analytics platforms:** Use specialized GIS and AI platforms designed for real-time data processing.\n",
      "*   **Open-Source Software:** Consider open-source options like QGIS (for GIS) and TensorFlow or PyTorch (for AI) to reduce costs.\n",
      "\n",
      "**3. Model Development and Training**\n",
      "\n",
      "*   **Deep Learning:** Deep learning models can outperform traditional methods in predicting wildfires.\n",
      "*   **Model Selection:** Consider various algorithms like logistic regression, support vector machines, random forests, neural fuzzy systems, and decision trees. SVM is effective in dealing with complex problems with noisy or incomplete data.\n",
      "*   **U-Net Architecture:**  U-Net is frequently used for semantic segmentation tasks, such as identifying fire, smoke, and burn areas in satellite imagery.\n",
      "*   **Training outside Earth Engine:** Train your model outside of Earth Engine using frameworks like TensorFlow or PyTorch, especially if you're already familiar with them or if your dataset is very large.\n",
      "*   **TFRecord format:** Use the TFRecord data format for training in TensorFlow as it is optimized for this purpose.\n",
      "\n",
      "**4. Monitoring and Evaluation**\n",
      "\n",
      "*   **TensorBoard:** Integrate TensorBoard to enhance the user experience by providing powerful visualizations and efficient model tracking capabilities.\n",
      "*   **Validation:** During validation, calculate regression accuracy by scaling outputs to the original data's range and assessing whether predictions fall within a tolerance level to evaluate the localized precision of predictions.\n",
      "*   **Model Monitoring:** Implement phased approach aligns with best practice for model monitoring and debugging, ensuring convergence while avoiding model degradation through overtraining.\n",
      "\n",
      "**5. Deployment and Integration**\n",
      "\n",
      "*   **Cloud-based solutions:** Use cloud-based services to reduce the need for expensive on-premise hardware.\n",
      "*   **Earth Engine integration:** Use Earth Engine's `ee.Model` package to get predictions using data in Earth Engine and a trained model hosted on Google's Vertex AI. You can host your custom trained model in Vertex AI and perform inference directly in Earth Engine using `ee.Model.fromVertexAi`.\n",
      "*   **Real-Time Geospatial Data Analysis:** Integrate GIS with AI to analyze real-time data from various sources (sensors, drones, satellites).\n",
      "*    **Edge Computing:** Deploy edge computing to process data locally at the source (e.g., on sensors or drones) before sending it to central servers, reducing latency.\n",
      "\n",
      "**6. Specific Techniques and Applications**\n",
      "\n",
      "*   **Semantic Segmentation:** Use semantic segmentation techniques to identify fire, smoke, and burn areas.\n",
      "*   **Change Detection:** Employ change detection methods to analyze multi-temporal satellite images and identify changes over time.\n",
      "*   **Burned Area Monitoring:** Use Sentinel-2 and VIIRS data for high-resolution burned area monitoring.\n",
      "*   **Wildfire Risk Prediction:** Develop predictive models using geospatial raster data to assess wildfire hazard potential.\n",
      "\n",
      "**7. Collaboration and Cost-Effectiveness**\n",
      "\n",
      "*   **Collaborative Funding and Partnerships:** Governments and organizations can collaborate on shared infrastructure or public-private partnerships to reduce costs and improve resource allocation.\n",
      "\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGeospatial Technology & Market Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"Key findings emphasize the use of Google Earth Engine for data acquisition and preprocessing, TensorFlow/PyTorch for model development, and ArcGIS for visualization. Deep learning models, particularly U-Net, are effective for wildfire prediction. Real-time data integration and cloud-based solutions are crucial for deployment. Data augmentation techniques are important when working with limited data.\",\n",
      "  \"best_practices_insights\": \"Utilize Google Earth Engine for efficient data retrieval and preprocessing. Employ TensorFlow or PyTorch for deep learning model development, considering U-Net architecture for semantic segmentation. Integrate ArcGIS for visualization and real-time data processing. Implement data augmentation techniques to improve model performance with limited data. Consider cloud-based solutions for deployment and scalability. Use TensorBoard for model monitoring and debugging.\",\n",
      "  \"potential_challenges\": \"Achieving the desired accuracy (85-90%) can be challenging due to data quality and model complexity. Ensuring timely alerts for new ignitions requires efficient real-time data processing and model deployment. Handling large datasets from multiple sources (satellite imagery, weather data, etc.) can be computationally intensive. Integrating different systems and APIs may present compatibility issues.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Analyze the structured project requirements (output object from Task 1).\n",
      "            Use the 'VectorStoreSearch' tool to query the internal service catalog based on keywords derived from the requirements\n",
      "            (e.g., `project_type`, `features_needed`, `data_sources`, `technical_requirements`). Use a query like \"geospatial AI wildfire monitoring database model dashboard API\".\n",
      "\n",
      "            Identify the most relevant existing services from the tool's output. For each relevant service, extract its name, label, price, and development time.\n",
      "\n",
      "            Based on the findings:\n",
      "            1.  Select a combination of services that best address the client's needs to propose as a package.\n",
      "            2.  Calculate the total estimated *base* price and total estimated *base* development time (in hours) for this package by summing the values from the selected services.\n",
      "            3.  List any client requirements (from Task 1 output, e.g., `features_needed`, `technical_requirements.outputs`) that are NOT clearly covered by the selected standard services. These might require custom work.\n",
      "            4.  Write a brief Markdown explanation justifying the service choices, base calculations, and customization needs.\n",
      "            \u001b[00m\n",
      "Querying Vector Store with: geospatial AI wildfire monitoring database model dashboard API\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mOkay, I understand the task. I need to analyze the provided project requirements, use the `VectorStoreSearch` tool to find relevant internal services, select a service package, calculate the total price and development time, identify customization needs, and output the results in the specified Pydantic format.\n",
      "First, I'll use the `VectorStoreSearch` tool to find relevant services based on the project requirements. I'll use a query that includes keywords related to geospatial AI, wildfire monitoring, database, model, dashboard, and API.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mVectorStoreSearch\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"geospatial AI wildfire monitoring database model dashboard API\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Result (Score: 0.8233):\n",
      "  Name: GeospatialModelDeploymentMonitoring\n",
      "  Label: Geospatial AI Model Deployment & Monitoring\n",
      "  Price: $1000\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / devops\n",
      "  Description: Setup and configuration of infrastructure for deploying, monitoring (performance, drift, data quality), and managing the lifecycle of geospatial AI models in production environments.\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8500):\n",
      "  Name: AIIntegration\n",
      "  Label: AI Integration for Geospatial\n",
      "  Price: $900\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / ai\n",
      "  Description: Seamless integration with major AI providers (OpenAI, Google, Anthropic) customized for processing geospatial data with sophisticated content understanding\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8589):\n",
      "  Name: ChangeDetection\n",
      "  Label: AI-Powered Change Detection\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Temporal analysis of geospatial data to identify and classify changes over time\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8728):\n",
      "  Name: GeospatialLLM\n",
      "  Label: Geospatial Large Language Models\n",
      "  Price: $1400\n",
      "  Development Time: 40 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Fine-tuned LLMs for geospatial content understanding, query processing, and location-based insights generation\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8938):\n",
      "  Name: SpatialPredictiveModeling\n",
      "  Label: Spatial Predictive Modeling\n",
      "  Price: $1500\n",
      "  Development Time: 40 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: AI models that incorporate spatial relationships to predict future conditions or optimize resource allocation\n",
      "\n",
      "-----\n",
      "Result (Score: 0.8991):\n",
      "  Name: GeoDataWrangling\n",
      "  Label: AI-Assisted Geospatial Data Wrangling\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / geospatial\n",
      "  Description: Intelligent ETL pipelines for geospatial data using GeoPandas, Shapely, and ML-enhanced data cleaning\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9049):\n",
      "  Name: GeoWorkflows\n",
      "  Label: Intelligent Geospatial Workflows\n",
      "  Price: $950\n",
      "  Development Time: 28 hours\n",
      "  Category: automations / automation\n",
      "  Description: AI-optimized pipelines for processing, analyzing and visualizing geospatial data with smart validation and error handling\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9082):\n",
      "  Name: ReactDashboard\n",
      "  Label: Geospatial React Dashboard\n",
      "  Price: $1400\n",
      "  Development Time: 40 hours\n",
      "  Category: webDevelopment / frontend\n",
      "  Description: Interactive React-based dashboards for visualizing and interacting with geospatial data, featuring responsive design and modular components\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9183):\n",
      "  Name: SatelliteImageAnalysis\n",
      "  Label: Satellite Image Analysis with AI\n",
      "  Price: $1200\n",
      "  Development Time: 36 hours\n",
      "  Category: geospatialAI / ai\n",
      "  Description: Custom deep learning solutions for satellite imagery classification, segmentation, and anomaly detection\n",
      "\n",
      "-----\n",
      "Result (Score: 0.9189):\n",
      "  Name: MultiAgentSystems\n",
      "  Label: Geospatial Multi-Agent Systems\n",
      "  Price: $1400\n",
      "  Development Time: 44 hours\n",
      "  Category: automations / ai\n",
      "  Description: Orchestrated AI agent networks specifically designed for complex geospatial workflows with built-in spatial reasoning capabilities\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"recommended_services\": [\n",
      "    {\n",
      "      \"name\": \"GeoDataWrangling\",\n",
      "      \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"price\": 950.0,\n",
      "      \"development_time_hours\": 28\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SatelliteImageAnalysis\",\n",
      "      \"label\": \"Satellite Image Analysis with AI\",\n",
      "      \"price\": 1200.0,\n",
      "      \"development_time_hours\": 36\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SpatialPredictiveModeling\",\n",
      "      \"label\": \"Spatial Predictive Modeling\",\n",
      "      \"price\": 1500.0,\n",
      "      \"development_time_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ReactDashboard\",\n",
      "      \"label\": \"Geospatial React Dashboard\",\n",
      "      \"price\": 1400.0,\n",
      "      \"development_time_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"AIIntegration\",\n",
      "      \"label\": \"AI Integration for Geospatial\",\n",
      "      \"price\": 900.0,\n",
      "      \"development_time_hours\": 28\n",
      "    }\n",
      "  ],\n",
      "  \"service_package\": {\n",
      "    \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "    \"services_included\": [\n",
      "      \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"Satellite Image Analysis with AI\",\n",
      "      \"Spatial Predictive Modeling\",\n",
      "      \"Geospatial React Dashboard\",\n",
      "      \"AI Integration for Geospatial\"\n",
      "    ],\n",
      "    \"total_estimated_price\": 5950.0,\n",
      "    \"total_estimated_time_hours\": 172,\n",
      "    \"notes\": \"This package provides the core components for building a wildfire monitoring AI solution, including data wrangling, satellite image analysis, predictive modeling, a web dashboard, and API integration.\"\n",
      "  },\n",
      "  \"customization_notes\": [\n",
      "    \"Specific requirements for integrating the alert system into existing systems need to be defined.\",\n",
      "    \"Custom model tuning and optimization may be required to achieve the desired accuracy (85-90%).\",\n",
      "    \"The format and content of the regular risk assessment reports need to be defined.\"\n",
      "  ],\n",
      "  \"explanation\": \"The recommended service package includes GeoDataWrangling for data preparation, SatelliteImageAnalysis for processing satellite imagery, SpatialPredictiveModeling for building the AI model, ReactDashboard for the web dashboard, and AIIntegration for API access. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization may be needed for alert system integration, achieving the desired accuracy, and generating risk assessment reports according to specific requirements.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Consolidate the outputs from the previous three tasks (Requirements Extraction, Web Research, Service Analysis)\n",
      "            into a single structured `AnalysisOutput` object. You have access to the outputs of these tasks via context.\n",
      "            Retrieve the `RequirementsOutput` object from Task 1 context.\n",
      "            Retrieve the `WebResearchOutput` object from Task 2 context.\n",
      "            Retrieve the `ServiceAnalysisOutput` object (which no longer contains budget analysis) from Task 3 context (your own previous output).\n",
      "            Combine these three objects into the fields (`requirements`, `research`, `service_analysis`) of the `AnalysisOutput` Pydantic model.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mInternal Service Catalog & Pricing Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Alert integration into existing systems\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"Aiming for 85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard for analysts\",\n",
      "        \"API for integration\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy (85-90%)\",\n",
      "      \"Timely alerts for new ignitions\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a historical wildfire database, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The desired outputs include a web dashboard, an API for system integration, and regular risk assessment reports. The budget for the initial phase is $40,000, with a focus on high accuracy (85-90%) in identifying high-risk zones and providing timely alerts.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"Key findings emphasize the use of Google Earth Engine for data acquisition and preprocessing, TensorFlow/PyTorch for model development, and ArcGIS for visualization. Deep learning models, particularly U-Net, are effective for wildfire prediction. Real-time data integration and cloud-based solutions are crucial for deployment. Data augmentation techniques are important when working with limited data.\",\n",
      "    \"best_practices_insights\": \"Utilize Google Earth Engine for efficient data retrieval and preprocessing. Employ TensorFlow or PyTorch for deep learning model development, considering U-Net architecture for semantic segmentation. Integrate ArcGIS for visualization and real-time data processing. Implement data augmentation techniques to improve model performance with limited data. Consider cloud-based solutions for deployment and scalability. Use TensorBoard for model monitoring and debugging.\",\n",
      "    \"potential_challenges\": \"Achieving the desired accuracy (85-90%) can be challenging due to data quality and model complexity. Ensuring timely alerts for new ignitions requires efficient real-time data processing and model deployment. Handling large datasets from multiple sources (satellite imagery, weather data, etc.) can be computationally intensive. Integrating different systems and APIs may present compatibility issues.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for building a wildfire monitoring AI solution, including data wrangling, satellite image analysis, predictive modeling, a web dashboard, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Specific requirements for integrating the alert system into existing systems need to be defined.\",\n",
      "      \"Custom model tuning and optimization may be required to achieve the desired accuracy (85-90%).\",\n",
      "      \"The format and content of the regular risk assessment reports need to be defined.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes GeoDataWrangling for data preparation, SatelliteImageAnalysis for processing satellite imagery, SpatialPredictiveModeling for building the AI model, ReactDashboard for the web dashboard, and AIIntegration for API access. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization may be needed for alert system integration, achieving the desired accuracy, and generating risk assessment reports according to specific requirements.\"\n",
      "    }\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kickoff Crew 1\n",
    "analysis_crew = DataAnalysisCrew(vector_store=chroma_store, conversation=conversation)\n",
    "analysis_result = analysis_crew.crew().kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c5af4",
   "metadata": {},
   "source": [
    "### Crew #2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62a359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew 2: Quote Structuring & QA\n",
    "class QuoteGenerationCrew:\n",
    "    def __init__(self, analysis_output: AnalysisOutput) -> None:\n",
    "        self.llm = LLM(\n",
    "            model=\"gemini/gemini-2.0-flash\",\n",
    "            temperature=0,\n",
    "            api_key=GOOGLE_API_KEY,\n",
    "            num_retries=5,\n",
    "            request_timeout=80,\n",
    "            seed=42,\n",
    "        )\n",
    "        self.analysis_output = analysis_output\n",
    "        self.today_date_obj = date.today()\n",
    "        self.today_date_str = self.today_date_obj.strftime(\"%Y-%m-%d\")\n",
    "        self._tasks = {}\n",
    "        self.quote_calculator_tool = QuoteCalculatorTool()\n",
    "\n",
    "    # Agent #4 - Quote Structuring Agent\n",
    "    def quote_structuring_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Quote Data Structuring Specialist\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Synthesize information from the provided `AnalysisOutput` object (containing requirements, research, and service analysis)\n",
    "                to structure the detailed project quote data into the `QuoteDetailsOutput` Pydantic format.\n",
    "                Estimate adjusted development hours and overhead buffer based on project specifics.\n",
    "                Use the QuoteCalculator tool to calculate the final price.\n",
    "                Perform all necessary calculations for dates and milestones accurately.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are an expert in translating project analysis into structured quote data.\n",
    "            You take the consolidated analysis (requirements, research, service details, pricing, budget analysis)\n",
    "            and meticulously populate a detailed quote structure.\n",
    "            First, you estimate the realistic total development hours by starting from the sum of base service hours, and only moderately increasing it (typically by 10-40%) to account for project-specific customizations, integration, and full lifecycle (planning, testing, deployment, PM).\n",
    "            Avoid drastic upscaling unless the project is highly custom or risky. Justify any increase clearly.\n",
    "            Second, you estimate an appropriate overhead buffer percentage (between 0.10 and 0.20) based on project risks and complexity.\n",
    "            Third, you use the QuoteCalculator tool with these estimates and the base service prices to get the final project price.\n",
    "            Finally, you populate the quote summary, line items, and payment schedule, performing date/milestone calculations precisely. Your output is a clean Pydantic object.\n",
    "            \"\"\"),\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "            tools=[self.quote_calculator_tool],\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "    # Agent #5 - Quality Assurance Agent\n",
    "    def qa_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Quote Quality Assurance Engineer\",\n",
    "            goal=dedent(\"\"\"\\\n",
    "                Review the structured quote data (`QuoteDetailsOutput`) against the initial analysis (`AnalysisOutput`).\n",
    "                Check for accuracy in scope, deliverables, pricing, timeline, budget alignment, and consistency, based *only* on the provided inputs.\n",
    "                Output a structured QA review (`QAReviewOutput`) indicating 'approved' or 'needs_revision' with specific issues if any.\n",
    "            \"\"\"),\n",
    "            backstory=dedent(\"\"\"\\\n",
    "            You are a meticulous QA specialist focusing on data integrity in technical proposals.\n",
    "            You cross-reference the structured quote data against the original analysis data provided to you.\n",
    "            You ensure the quote accurately reflects the client's needs, aligns with the proposed services,\n",
    "            stays within budget, and is free of calculation errors or inconsistencies based *only* on the provided context.\n",
    "            You output a structured review Pydantic object. You do NOT have delegation capabilities in this setup.\n",
    "            \"\"\"),\n",
    "            allow_delegation=False,\n",
    "            verbose=True,\n",
    "            llm=self.llm,\n",
    "        )\n",
    "\n",
    "    # Task 4: Structure the quote data\n",
    "    def quote_structuring_task(self) -> Task:\n",
    "        analysis_json = self.analysis_output.model_dump_json(indent=2)\n",
    "        base_hours_sum = sum(\n",
    "            s.development_time_hours\n",
    "            for s in self.analysis_output.service_analysis.recommended_services\n",
    "        )\n",
    "\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Synthesize information from the provided `AnalysisOutput` JSON below to structure the project quote data.\n",
    "            Use the data within this JSON as the sole source for requirements, research, and service analysis.\n",
    "            **Assume today's date is {self.today_date_str} for all calculations.**\n",
    "\n",
    "            ANALYSIS OUTPUT:\n",
    "            ```json\n",
    "            {analysis_json}\n",
    "            ```\n",
    "\n",
    "            **Actions:**\n",
    "            1.  **Estimate Adjusted Hours & Justify:** Start with base hours ({base_hours_sum}).\n",
    "                - If the project is mostly standard, increase by 10-25% for lifecycle (planning, testing, deployment, PM).\n",
    "                - If there are moderate customizations or integrations, increase by 25-40%.\n",
    "                - Only increase more if the project is highly custom or risky, and justify clearly.\n",
    "                - Arrive at `estimated_total_hours`. Justify the increase (e.g., \"Increased base hours by X% for custom work and lifecycle activities...\").\n",
    "            2.  **Estimate Overhead Buffer & Justify:** Estimate `overhead_buffer_percentage` (0.10-0.20) based on risks/complexity. Justify briefly.\n",
    "            3.  **Calculate Price Breakdown:** Use `QuoteCalculator` tool with `estimated_total_hours`, `recommended_services` (from analysis output), `overhead_buffer_percentage`, `base_hourly_rate` (100). The tool will return a `QuoteCalculationDetails` object.\n",
    "            4.  **Extract Final Price:** Get the `final_total_price` from the `QuoteCalculationDetails` object returned by the tool. Use this for milestone calculations.\n",
    "            5.  **Generate Quote ID:** Create Quote ID: Q-{self.today_date_str}-001.\n",
    "            6.  **Extract Client Info:** Get `client_name`, `client_company`.\n",
    "            7.  **Determine Project Name:** Create a suitable `project_name`.\n",
    "            8.  **Calculate Dates:**\n",
    "                *   `today_obj = date.today()` \n",
    "                *   `quote_date_str = today_obj.strftime('%Y-%m-%d')`\n",
    "                *   `start_date_obj = today_obj + timedelta(days=7)`\n",
    "                *   `estimated_start_date = start_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `weeks_needed = math.ceil(estimated_total_hours / 40)` (Use `estimated_total_hours` from step 1)\n",
    "                *   `completion_date_obj = start_date_obj + timedelta(weeks=weeks_needed)`\n",
    "                *   `estimated_completion_date = completion_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `midpoint1_weeks = math.ceil(weeks_needed / 3)`\n",
    "                *   `midpoint1_date_obj = start_date_obj + timedelta(weeks=midpoint1_weeks)`\n",
    "                *   `midpoint1_date_str = midpoint1_date_obj.strftime('%Y-%m-%d')`\n",
    "                *   `midpoint2_weeks = math.ceil(weeks_needed * 2 / 3)`\n",
    "                *   `midpoint2_date_obj = start_date_obj + timedelta(weeks=midpoint2_weeks)`\n",
    "                *   `midpoint2_date_str = midpoint2_date_obj.strftime('%Y-%m-%d')`\n",
    "            9.  **Create Line Items:** Create `QuoteLineItem` objects from `recommended_services` (base details).\n",
    "            10.  **Create Payment Milestones (4 Milestones):**\n",
    "                *   Milestone 1: \"Project Kick-off\", 30%, amount = `total_price` * 0.30, due_date = `estimated_start_date`\n",
    "                *   Milestone 2: \"Mid-Project Review 1\", 30%, amount = `total_price` * 0.30, due_date = `midpoint1_date_str`\n",
    "                *   Milestone 3: \"Mid-Project Review 2\", 30%, amount = `total_price` * 0.30, due_date = `midpoint2_date_str`\n",
    "                *   Milestone 4: \"Project Completion & Handover\", 10%, amount = `total_price` * 0.10, due_date = `estimated_completion_date`\n",
    "                *   Create `PaymentMilestone` objects for each. Ensure amounts are calculated correctly (handle potential rounding if needed, though direct multiplication is usually fine).\n",
    "            11. **Populate Summary:** Populate `QuoteSummary` using extracted info, project name, quote ID, `quote_date_str` (from step 7), final `total_price`, `estimated_total_hours`, calculated dates (`estimated_start_date`, `estimated_completion_date`), the combined `estimation_justification`, **and the `overhead_buffer_percentage` estimated in step 2.**\n",
    "            12. **Assemble Output:** Combine `QuoteSummary`, `line_items`, and `payment_schedule` into `QuoteDetailsOutput`.\n",
    "\n",
    "            Ensure all estimations are justified, tool is called correctly, all dates use the current year ({self.today_date_obj.year}), milestones sum to 100%, and data mapping is accurate.\n",
    "            \"\"\",\n",
    "            expected_output=f\"\"\"\\\n",
    "            A Pydantic object conforming to the `QuoteDetailsOutput` schema.\n",
    "            - `quote_summary.quote_date` must be today's date ({self.today_date_str}).\n",
    "            - `quote_summary.estimated_start_date` and `quote_summary.estimated_completion_date` must be calculated correctly based on today's date and `total_estimated_hours`.\n",
    "            - `quote_summary.estimation_justification` must be populated.\n",
    "            - **`quote_summary.price_calculation_details` must be populated with the full `QuoteCalculationDetails` object returned by the QuoteCalculator tool.**\n",
    "            - **`quote_summary.total_price` must match `quote_summary.price_calculation_details.final_total_price`.**\n",
    "            - **`quote_summary.total_estimated_hours` must match `quote_summary.price_calculation_details.total_estimated_hours`.**\n",
    "            - `payment_schedule` must contain 4 milestones (30%, 30%, 30%, 10%) with correctly calculated amounts based on the final total price.\n",
    "            - All dates must reflect the current year ({self.today_date_obj.year}).\n",
    "\n",
    "            The final output MUST be only the JSON representation of this populated object.\n",
    "            \"\"\",\n",
    "            agent=self.quote_structuring_agent(),\n",
    "            output_pydantic=QuoteDetailsOutput,\n",
    "        )\n",
    "        self._tasks[\"quote_structuring\"] = task\n",
    "        return task\n",
    "\n",
    "    # Task 5: Review and QA the quote data\n",
    "    def qa_review_task(self) -> Task:\n",
    "        analysis_json = self.analysis_output.model_dump_json(indent=2)\n",
    "        task = Task(\n",
    "            description=f\"\"\"\\\n",
    "            Review the structured quote data (`QuoteDetailsOutput` object from Task 4 context) for quality and accuracy against the provided `AnalysisOutput` JSON below.\n",
    "            **You MUST use the provided `AnalysisOutput` JSON and the `QuoteDetailsOutput` from Task 4 context for your review.**\n",
    "\n",
    "            ANALYSIS OUTPUT (Source of Truth):\n",
    "            ```json\n",
    "            {analysis_json}\n",
    "            ```\n",
    "\n",
    "            QUOTE DETAILS (To Be Reviewed - from Task 4 context):\n",
    "            *(The `QuoteDetailsOutput` JSON from the previous task will be available here in the actual execution context)*\n",
    "\n",
    "            **Verify the following based *only* on comparing Task 4 output (`QuoteDetailsOutput`) against the `AnalysisOutput` JSON and the logic described for Task 4:**\n",
    "                - **Completeness:**\n",
    "                    - Does `line_items` reflect `recommended_services`?\n",
    "                    - **Is `quote_summary.price_calculation_details` present and fully populated?**\n",
    "                - **Plausibility & Justification:**\n",
    "                    - Is `quote_summary.total_estimated_hours` significantly higher than base hours (sum of `estimated_hours` in `line_items`)?\n",
    "                    - Is `estimation_justification` present and reasonable?\n",
    "                    - **Does `price_calculation_details.overhead_buffer_percentage` fall within the expected range (e.g., 0.10 to 0.20)?**\n",
    "                - **Internal Consistency:**\n",
    "                    - **Does `quote_summary.total_price` exactly match `quote_summary.price_calculation_details.final_total_price`?**\n",
    "                    - **Does `quote_summary.total_estimated_hours` exactly match `quote_summary.price_calculation_details.total_estimated_hours`?**\n",
    "                    - **Verify the calculation within `price_calculation_details`: Does `(details.subtotal_before_overhead * (1 + details.overhead_buffer_percentage))` approximately equal `details.final_total_price` (allowing for minor rounding)?**\n",
    "                    - **Verify subtotal: Does `details.sum_base_service_prices + details.additional_development_cost` approximately equal `details.subtotal_before_overhead`?**\n",
    "                - **Accuracy (Milestones/Dates):**\n",
    "                    - Does `payment_schedule` contain exactly 4 milestones?\n",
    "                    - Do percentages sum to 100?\n",
    "                    - Are amounts calculated correctly based on `quote_summary.total_price`?\n",
    "                    - Does `quote_summary.quote_date` match today's date ({self.today_date_str})?\n",
    "                    - Is `estimated_completion_date` calculated correctly based on `quote_summary.total_estimated_hours`? (Verify logic as before). Are dates in the correct year?\n",
    "                - **Budget Alignment:** (Check remains the same - compare `quote_summary.total_price` to parsed `budget`)\n",
    "                - **Consistency:** Are `client_name`, `client_company`, `project_name` consistent?\n",
    "                - **Clarity:** Are `line_items` descriptions reasonable? Is `payment_schedule` logical?\n",
    "\n",
    "            **Output Format:**\n",
    "            - If issues found (budget exceeded, date mismatch, milestone error, missing/poor justification, inconsistency), create `QAIssue` objects. Set `overall_status` to 'needs_revision'.\n",
    "            - If satisfactory, set `overall_status` to 'approved'.\n",
    "            - Optional `QAImprovementSuggestion`s.\n",
    "            - Brief Markdown `summary`.\n",
    "            - Base findings strictly on comparison.\n",
    "            \"\"\",\n",
    "            expected_output=\"\"\"\\\n",
    "            A Pydantic object conforming to the `QAReviewOutput` schema.\n",
    "            - `overall_status`: 'approved' or 'needs_revision'.\n",
    "                        - `issues`: List of `QAIssue` objects if status is 'needs_revision'. Issues must detail problems based *only* on the context (e.g., budget comparison failure, date calculation mismatch, milestone errors, **inconsistent price breakdown**, missing justification).\n",
    "            - `improvement_suggestions`: Optional list.\n",
    "            - `summary`: Brief Markdown summary.\n",
    "\n",
    "            The final output MUST be only the JSON representation of the populated `QAReviewOutput` object.\n",
    "            \"\"\",\n",
    "            agent=self.qa_agent(),\n",
    "            context=[self._tasks[\"quote_structuring\"]],\n",
    "            output_pydantic=QAReviewOutput,\n",
    "        )\n",
    "        return task\n",
    "\n",
    "    def crew(self) -> Crew:\n",
    "        \"\"\"Assemble the crew with all agents and tasks.\"\"\"\n",
    "        return Crew(\n",
    "            agents=[\n",
    "                self.quote_structuring_agent(),\n",
    "                self.qa_agent(),\n",
    "            ],\n",
    "            tasks=[self.quote_structuring_task(), self.qa_review_task()],\n",
    "            process=Process.sequential,\n",
    "            verbose=False,  # Turn off verbose for Kaggle demo\n",
    "            step_callback=delay_before_task,\n",
    "            max_rpm=15,\n",
    "            # memory=True,\n",
    "            # embedder={\n",
    "            #     \"provider\": \"google\",\n",
    "            #     \"config\": {\n",
    "            #         \"api_key\": GOOGLE_API_KEY,\n",
    "            #         \"model\": \"models/text-embedding-004\",\n",
    "            #     },\n",
    "            # },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250fb967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Synthesize information from the provided `AnalysisOutput` JSON below to structure the project quote data.\n",
      "            Use the data within this JSON as the sole source for requirements, research, and service analysis.\n",
      "            **Assume today's date is 2025-04-15 for all calculations.**\n",
      "\n",
      "            ANALYSIS OUTPUT:\n",
      "            ```json\n",
      "            {\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Alert integration into existing systems\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"Aiming for 85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard for analysts\",\n",
      "        \"API for integration\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy (85-90%)\",\n",
      "      \"Timely alerts for new ignitions\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a historical wildfire database, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The desired outputs include a web dashboard, an API for system integration, and regular risk assessment reports. The budget for the initial phase is $40,000, with a focus on high accuracy (85-90%) in identifying high-risk zones and providing timely alerts.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"Key findings emphasize the use of Google Earth Engine for data acquisition and preprocessing, TensorFlow/PyTorch for model development, and ArcGIS for visualization. Deep learning models, particularly U-Net, are effective for wildfire prediction. Real-time data integration and cloud-based solutions are crucial for deployment. Data augmentation techniques are important when working with limited data.\",\n",
      "    \"best_practices_insights\": \"Utilize Google Earth Engine for efficient data retrieval and preprocessing. Employ TensorFlow or PyTorch for deep learning model development, considering U-Net architecture for semantic segmentation. Integrate ArcGIS for visualization and real-time data processing. Implement data augmentation techniques to improve model performance with limited data. Consider cloud-based solutions for deployment and scalability. Use TensorBoard for model monitoring and debugging.\",\n",
      "    \"potential_challenges\": \"Achieving the desired accuracy (85-90%) can be challenging due to data quality and model complexity. Ensuring timely alerts for new ignitions requires efficient real-time data processing and model deployment. Handling large datasets from multiple sources (satellite imagery, weather data, etc.) can be computationally intensive. Integrating different systems and APIs may present compatibility issues.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for building a wildfire monitoring AI solution, including data wrangling, satellite image analysis, predictive modeling, a web dashboard, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Specific requirements for integrating the alert system into existing systems need to be defined.\",\n",
      "      \"Custom model tuning and optimization may be required to achieve the desired accuracy (85-90%).\",\n",
      "      \"The format and content of the regular risk assessment reports need to be defined.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes GeoDataWrangling for data preparation, SatelliteImageAnalysis for processing satellite imagery, SpatialPredictiveModeling for building the AI model, ReactDashboard for the web dashboard, and AIIntegration for API access. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization may be needed for alert system integration, achieving the desired accuracy, and generating risk assessment reports according to specific requirements.\"\n",
      "  }\n",
      "}\n",
      "            ```\n",
      "\n",
      "            **Actions:**\n",
      "            1.  **Estimate Adjusted Hours & Justify:** Start with base hours (172).\n",
      "                - If the project is mostly standard, increase by 10-25% for lifecycle (planning, testing, deployment, PM).\n",
      "                - If there are moderate customizations or integrations, increase by 25-40%.\n",
      "                - Only increase more if the project is highly custom or risky, and justify clearly.\n",
      "                - Arrive at `estimated_total_hours`. Justify the increase (e.g., \"Increased base hours by X% for custom work and lifecycle activities...\").\n",
      "            2.  **Estimate Overhead Buffer & Justify:** Estimate `overhead_buffer_percentage` (0.10-0.20) based on risks/complexity. Justify briefly.\n",
      "            3.  **Calculate Price Breakdown:** Use `QuoteCalculator` tool with `estimated_total_hours`, `recommended_services` (from analysis output), `overhead_buffer_percentage`, `base_hourly_rate` (100). The tool will return a `QuoteCalculationDetails` object.\n",
      "            4.  **Extract Final Price:** Get the `final_total_price` from the `QuoteCalculationDetails` object returned by the tool. Use this for milestone calculations.\n",
      "            5.  **Generate Quote ID:** Create Quote ID: Q-2025-04-15-001.\n",
      "            6.  **Extract Client Info:** Get `client_name`, `client_company`.\n",
      "            7.  **Determine Project Name:** Create a suitable `project_name`.\n",
      "            8.  **Calculate Dates:**\n",
      "                *   `today_obj = date.today()` \n",
      "                *   `quote_date_str = today_obj.strftime('%Y-%m-%d')`\n",
      "                *   `start_date_obj = today_obj + timedelta(days=7)`\n",
      "                *   `estimated_start_date = start_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `weeks_needed = math.ceil(estimated_total_hours / 40)` (Use `estimated_total_hours` from step 1)\n",
      "                *   `completion_date_obj = start_date_obj + timedelta(weeks=weeks_needed)`\n",
      "                *   `estimated_completion_date = completion_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `midpoint1_weeks = math.ceil(weeks_needed / 3)`\n",
      "                *   `midpoint1_date_obj = start_date_obj + timedelta(weeks=midpoint1_weeks)`\n",
      "                *   `midpoint1_date_str = midpoint1_date_obj.strftime('%Y-%m-%d')`\n",
      "                *   `midpoint2_weeks = math.ceil(weeks_needed * 2 / 3)`\n",
      "                *   `midpoint2_date_obj = start_date_obj + timedelta(weeks=midpoint2_weeks)`\n",
      "                *   `midpoint2_date_str = midpoint2_date_obj.strftime('%Y-%m-%d')`\n",
      "            9.  **Create Line Items:** Create `QuoteLineItem` objects from `recommended_services` (base details).\n",
      "            10.  **Create Payment Milestones (4 Milestones):**\n",
      "                *   Milestone 1: \"Project Kick-off\", 30%, amount = `total_price` * 0.30, due_date = `estimated_start_date`\n",
      "                *   Milestone 2: \"Mid-Project Review 1\", 30%, amount = `total_price` * 0.30, due_date = `midpoint1_date_str`\n",
      "                *   Milestone 3: \"Mid-Project Review 2\", 30%, amount = `total_price` * 0.30, due_date = `midpoint2_date_str`\n",
      "                *   Milestone 4: \"Project Completion & Handover\", 10%, amount = `total_price` * 0.10, due_date = `estimated_completion_date`\n",
      "                *   Create `PaymentMilestone` objects for each. Ensure amounts are calculated correctly (handle potential rounding if needed, though direct multiplication is usually fine).\n",
      "            11. **Populate Summary:** Populate `QuoteSummary` using extracted info, project name, quote ID, `quote_date_str` (from step 7), final `total_price`, `estimated_total_hours`, calculated dates (`estimated_start_date`, `estimated_completion_date`), the combined `estimation_justification`, **and the `overhead_buffer_percentage` estimated in step 2.**\n",
      "            12. **Assemble Output:** Combine `QuoteSummary`, `line_items`, and `payment_schedule` into `QuoteDetailsOutput`.\n",
      "\n",
      "            Ensure all estimations are justified, tool is called correctly, all dates use the current year (2025), milestones sum to 100%, and data mapping is accurate.\n",
      "            \u001b[00m\n",
      "Calculator Input: Hours=230, Services Base=5950.0, Buffer=0.15, Rate=100\n",
      "Calculated Details: {\n",
      "  \"sum_base_service_prices\": 5950.0,\n",
      "  \"total_estimated_hours\": 230,\n",
      "  \"base_hourly_rate\": 100,\n",
      "  \"additional_hours\": 230,\n",
      "  \"additional_development_cost\": 23000.0,\n",
      "  \"subtotal_before_overhead\": 28950.0,\n",
      "  \"overhead_buffer_percentage\": 0.15,\n",
      "  \"overhead_amount\": 4342.5,\n",
      "  \"final_total_price\": 33292.5\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mOkay, I understand the task. I need to create a detailed quote based on the provided analysis, calculate the price using the `QuoteCalculator` tool, and structure the data into a `QuoteDetailsOutput` Pydantic object. I need to pay close attention to the date calculations, milestone amounts, and justifications for estimations.\n",
      "First, I'll estimate the adjusted development hours and overhead buffer. Then, I'll use the `QuoteCalculator` tool to get the final price. After that, I'll calculate the dates, create line items and payment milestones, and finally assemble the `QuoteDetailsOutput` object.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mQuoteCalculator\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"estimated_total_hours\\\": 230, \\\"recommended_services\\\": [{\\\"name\\\": \\\"GeoDataWrangling\\\", \\\"label\\\": \\\"AI-Assisted Geospatial Data Wrangling\\\", \\\"price\\\": 950.0, \\\"development_time_hours\\\": 28}, {\\\"name\\\": \\\"SatelliteImageAnalysis\\\", \\\"label\\\": \\\"Satellite Image Analysis with AI\\\", \\\"price\\\": 1200.0, \\\"development_time_hours\\\": 36}, {\\\"name\\\": \\\"SpatialPredictiveModeling\\\", \\\"label\\\": \\\"Spatial Predictive Modeling\\\", \\\"price\\\": 1500.0, \\\"development_time_hours\\\": 40}, {\\\"name\\\": \\\"ReactDashboard\\\", \\\"label\\\": \\\"Geospatial React Dashboard\\\", \\\"price\\\": 1400.0, \\\"development_time_hours\\\": 40}, {\\\"name\\\": \\\"AIIntegration\\\", \\\"label\\\": \\\"AI Integration for Geospatial\\\", \\\"price\\\": 900.0, \\\"development_time_hours\\\": 28}], \\\"overhead_buffer_percentage\\\": 0.15, \\\"base_hourly_rate\\\": 100}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "sum_base_service_prices=5950.0 total_estimated_hours=230 base_hourly_rate=100 additional_hours=230 additional_development_cost=23000.0 subtotal_before_overhead=28950.0 overhead_buffer_percentage=0.15 overhead_amount=4342.5 final_total_price=33292.5\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Data Structuring Specialist\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"quote_summary\": {\n",
      "    \"project_name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"quote_id\": \"Q-2025-04-15-001\",\n",
      "    \"quote_date\": \"2025-04-15\",\n",
      "    \"total_price\": 33292.5,\n",
      "    \"total_estimated_hours\": 230,\n",
      "    \"estimated_start_date\": \"2025-04-22\",\n",
      "    \"estimated_completion_date\": \"2025-10-07\",\n",
      "    \"estimation_justification\": \"Increased base hours by 33% to account for project-specific customizations, integration of the alert system, custom model tuning to achieve the desired accuracy, defining the format and content of risk assessment reports, and full lifecycle activities (planning, testing, deployment, project management). An overhead buffer of 15% is applied to account for the project's complexity and potential risks associated with achieving the desired accuracy and timely alert delivery.\",\n",
      "    \"price_calculation_details\": {\n",
      "      \"sum_base_service_prices\": 5950.0,\n",
      "      \"total_estimated_hours\": 230,\n",
      "      \"base_hourly_rate\": 100,\n",
      "      \"additional_hours\": 58,\n",
      "      \"additional_development_cost\": 23000.0,\n",
      "      \"subtotal_before_overhead\": 28950.0,\n",
      "      \"overhead_buffer_percentage\": 0.15,\n",
      "      \"overhead_amount\": 4342.5,\n",
      "      \"final_total_price\": 33292.5\n",
      "    }\n",
      "  },\n",
      "  \"line_items\": [\n",
      "    {\n",
      "      \"service_name\": \"GeoDataWrangling\",\n",
      "      \"description\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "      \"price\": 950.0,\n",
      "      \"estimated_hours\": 28\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"SatelliteImageAnalysis\",\n",
      "      \"description\": \"Satellite Image Analysis with AI\",\n",
      "      \"price\": 1200.0,\n",
      "      \"estimated_hours\": 36\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"SpatialPredictiveModeling\",\n",
      "      \"description\": \"Spatial Predictive Modeling\",\n",
      "      \"price\": 1500.0,\n",
      "      \"estimated_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"ReactDashboard\",\n",
      "      \"description\": \"Geospatial React Dashboard\",\n",
      "      \"price\": 1400.0,\n",
      "      \"estimated_hours\": 40\n",
      "    },\n",
      "    {\n",
      "      \"service_name\": \"AIIntegration\",\n",
      "      \"description\": \"AI Integration for Geospatial\",\n",
      "      \"price\": 900.0,\n",
      "      \"estimated_hours\": 28\n",
      "    }\n",
      "  ],\n",
      "  \"payment_schedule\": [\n",
      "    {\n",
      "      \"milestone\": \"Project Kick-off\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-04-22\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Mid-Project Review 1\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-06-03\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Mid-Project Review 2\",\n",
      "      \"percentage\": 30,\n",
      "      \"amount\": 9987.75,\n",
      "      \"due_date\": \"2025-08-05\"\n",
      "    },\n",
      "    {\n",
      "      \"milestone\": \"Project Completion & Handover\",\n",
      "      \"percentage\": 10,\n",
      "      \"amount\": 3329.25,\n",
      "      \"due_date\": \"2025-10-07\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Quality Assurance Engineer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m            Review the structured quote data (`QuoteDetailsOutput` object from Task 4 context) for quality and accuracy against the provided `AnalysisOutput` JSON below.\n",
      "            **You MUST use the provided `AnalysisOutput` JSON and the `QuoteDetailsOutput` from Task 4 context for your review.**\n",
      "\n",
      "            ANALYSIS OUTPUT (Source of Truth):\n",
      "            ```json\n",
      "            {\n",
      "  \"requirements\": {\n",
      "    \"client_name\": \"Jane Smith\",\n",
      "    \"client_company\": \"Wildfire Watch North America\",\n",
      "    \"project_type\": \"Geospatial AI project\",\n",
      "    \"main_objective\": \"Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US.\",\n",
      "    \"data_sources\": [\n",
      "      \"MODIS satellite imagery\",\n",
      "      \"VIIRS satellite imagery\",\n",
      "      \"Landsat satellite imagery\",\n",
      "      \"Sentinel satellite imagery\",\n",
      "      \"Historical fire perimeter data\",\n",
      "      \"Weather data\",\n",
      "      \"Topographical data\"\n",
      "    ],\n",
      "    \"features_needed\": [\n",
      "      \"Near real-time monitoring capabilities\",\n",
      "      \"Predict ignition risk\",\n",
      "      \"Predict potential spread patterns\",\n",
      "      \"Alert integration into existing systems\"\n",
      "    ],\n",
      "    \"technical_requirements\": {\n",
      "      \"accuracy\": \"Aiming for 85-90% precision in identifying high-risk zones\",\n",
      "      \"outputs\": [\n",
      "        \"Web dashboard for analysts\",\n",
      "        \"API for integration\",\n",
      "        \"Regular risk assessment reports\"\n",
      "      ]\n",
      "    },\n",
      "    \"budget\": \"$40,000\",\n",
      "    \"timeline_expectation\": null,\n",
      "    \"potential_risks\": [\n",
      "      \"Achieving desired accuracy (85-90%)\",\n",
      "      \"Timely alerts for new ignitions\"\n",
      "    ],\n",
      "    \"summary\": \"Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a historical wildfire database, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The desired outputs include a web dashboard, an API for system integration, and regular risk assessment reports. The budget for the initial phase is $40,000, with a focus on high accuracy (85-90%) in identifying high-risk zones and providing timely alerts.\"\n",
      "  },\n",
      "  \"research\": {\n",
      "    \"summary\": \"Key findings emphasize the use of Google Earth Engine for data acquisition and preprocessing, TensorFlow/PyTorch for model development, and ArcGIS for visualization. Deep learning models, particularly U-Net, are effective for wildfire prediction. Real-time data integration and cloud-based solutions are crucial for deployment. Data augmentation techniques are important when working with limited data.\",\n",
      "    \"best_practices_insights\": \"Utilize Google Earth Engine for efficient data retrieval and preprocessing. Employ TensorFlow or PyTorch for deep learning model development, considering U-Net architecture for semantic segmentation. Integrate ArcGIS for visualization and real-time data processing. Implement data augmentation techniques to improve model performance with limited data. Consider cloud-based solutions for deployment and scalability. Use TensorBoard for model monitoring and debugging.\",\n",
      "    \"potential_challenges\": \"Achieving the desired accuracy (85-90%) can be challenging due to data quality and model complexity. Ensuring timely alerts for new ignitions requires efficient real-time data processing and model deployment. Handling large datasets from multiple sources (satellite imagery, weather data, etc.) can be computationally intensive. Integrating different systems and APIs may present compatibility issues.\"\n",
      "  },\n",
      "  \"service_analysis\": {\n",
      "    \"recommended_services\": [\n",
      "      {\n",
      "        \"name\": \"GeoDataWrangling\",\n",
      "        \"label\": \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"price\": 950.0,\n",
      "        \"development_time_hours\": 28\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SatelliteImageAnalysis\",\n",
      "        \"label\": \"Satellite Image Analysis with AI\",\n",
      "        \"price\": 1200.0,\n",
      "        \"development_time_hours\": 36\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"SpatialPredictiveModeling\",\n",
      "        \"label\": \"Spatial Predictive Modeling\",\n",
      "        \"price\": 1500.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"ReactDashboard\",\n",
      "        \"label\": \"Geospatial React Dashboard\",\n",
      "        \"price\": 1400.0,\n",
      "        \"development_time_hours\": 40\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"AIIntegration\",\n",
      "        \"label\": \"AI Integration for Geospatial\",\n",
      "        \"price\": 900.0,\n",
      "        \"development_time_hours\": 28\n",
      "      }\n",
      "    ],\n",
      "    \"service_package\": {\n",
      "      \"name\": \"Wildfire Monitoring AI - Phase 1\",\n",
      "      \"services_included\": [\n",
      "        \"AI-Assisted Geospatial Data Wrangling\",\n",
      "        \"Satellite Image Analysis with AI\",\n",
      "        \"Spatial Predictive Modeling\",\n",
      "        \"Geospatial React Dashboard\",\n",
      "        \"AI Integration for Geospatial\"\n",
      "      ],\n",
      "      \"total_estimated_price\": 5950.0,\n",
      "      \"total_estimated_time_hours\": 172,\n",
      "      \"notes\": \"This package provides the core components for building a wildfire monitoring AI solution, including data wrangling, satellite image analysis, predictive modeling, a web dashboard, and API integration.\"\n",
      "    },\n",
      "    \"customization_notes\": [\n",
      "      \"Specific requirements for integrating the alert system into existing systems need to be defined.\",\n",
      "      \"Custom model tuning and optimization may be required to achieve the desired accuracy (85-90%).\",\n",
      "      \"The format and content of the regular risk assessment reports need to be defined.\"\n",
      "    ],\n",
      "    \"explanation\": \"The recommended service package includes GeoDataWrangling for data preparation, SatelliteImageAnalysis for processing satellite imagery, SpatialPredictiveModeling for building the AI model, ReactDashboard for the web dashboard, and AIIntegration for API access. The total estimated base price for this package is $5950, and the total estimated base development time is 172 hours. Customization may be needed for alert system integration, achieving the desired accuracy, and generating risk assessment reports according to specific requirements.\"\n",
      "  }\n",
      "}\n",
      "            ```\n",
      "\n",
      "            QUOTE DETAILS (To Be Reviewed - from Task 4 context):\n",
      "            *(The `QuoteDetailsOutput` JSON from the previous task will be available here in the actual execution context)*\n",
      "\n",
      "            **Verify the following based *only* on comparing Task 4 output (`QuoteDetailsOutput`) against the `AnalysisOutput` JSON and the logic described for Task 4:**\n",
      "                - **Completeness:**\n",
      "                    - Does `line_items` reflect `recommended_services`?\n",
      "                    - **Is `quote_summary.price_calculation_details` present and fully populated?**\n",
      "                - **Plausibility & Justification:**\n",
      "                    - Is `quote_summary.total_estimated_hours` significantly higher than base hours (sum of `estimated_hours` in `line_items`)?\n",
      "                    - Is `estimation_justification` present and reasonable?\n",
      "                    - **Does `price_calculation_details.overhead_buffer_percentage` fall within the expected range (e.g., 0.10 to 0.20)?**\n",
      "                - **Internal Consistency:**\n",
      "                    - **Does `quote_summary.total_price` exactly match `quote_summary.price_calculation_details.final_total_price`?**\n",
      "                    - **Does `quote_summary.total_estimated_hours` exactly match `quote_summary.price_calculation_details.total_estimated_hours`?**\n",
      "                    - **Verify the calculation within `price_calculation_details`: Does `(details.subtotal_before_overhead * (1 + details.overhead_buffer_percentage))` approximately equal `details.final_total_price` (allowing for minor rounding)?**\n",
      "                    - **Verify subtotal: Does `details.sum_base_service_prices + details.additional_development_cost` approximately equal `details.subtotal_before_overhead`?**\n",
      "                - **Accuracy (Milestones/Dates):**\n",
      "                    - Does `payment_schedule` contain exactly 4 milestones?\n",
      "                    - Do percentages sum to 100?\n",
      "                    - Are amounts calculated correctly based on `quote_summary.total_price`?\n",
      "                    - Does `quote_summary.quote_date` match today's date (2025-04-15)?\n",
      "                    - Is `estimated_completion_date` calculated correctly based on `quote_summary.total_estimated_hours`? (Verify logic as before). Are dates in the correct year?\n",
      "                - **Budget Alignment:** (Check remains the same - compare `quote_summary.total_price` to parsed `budget`)\n",
      "                - **Consistency:** Are `client_name`, `client_company`, `project_name` consistent?\n",
      "                - **Clarity:** Are `line_items` descriptions reasonable? Is `payment_schedule` logical?\n",
      "\n",
      "            **Output Format:**\n",
      "            - If issues found (budget exceeded, date mismatch, milestone error, missing/poor justification, inconsistency), create `QAIssue` objects. Set `overall_status` to 'needs_revision'.\n",
      "            - If satisfactory, set `overall_status` to 'approved'.\n",
      "            - Optional `QAImprovementSuggestion`s.\n",
      "            - Brief Markdown `summary`.\n",
      "            - Base findings strictly on comparison.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mQuote Quality Assurance Engineer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "{\n",
      "  \"overall_status\": \"needs_revision\",\n",
      "  \"issues\": [\n",
      "    {\n",
      "      \"type\": \"BudgetExceeded\",\n",
      "      \"severity\": \"High\",\n",
      "      \"description\": \"The total price of the quote ($33292.5) is less than the budget specified in the analysis ($40,000). However, the additional development cost of $23000 seems high and requires justification. The estimation justification mentions customizations, integration, model tuning, and lifecycle activities, but the allocation of $23000 to these aspects needs further scrutiny to ensure it aligns with the project scope and complexity.\",\n",
      "      \"recommendation\": \"Review and justify the additional development cost of $23000. Provide a detailed breakdown of how this cost is allocated to customizations, integration, model tuning, and lifecycle activities. Ensure the cost is reasonable and aligns with the project scope and complexity.\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"InconsistentHours\",\n",
      "      \"severity\": \"Medium\",\n",
      "      \"description\": \"The total estimated hours (230) is significantly higher than the sum of the base service hours (28+36+40+40+28 = 172). The estimation justification mentions a 33% increase in base hours. While the justification is present, the magnitude of the increase warrants a closer look to ensure it's reasonable and well-supported by the project's requirements.\",\n",
      "      \"recommendation\": \"Re-evaluate the total estimated hours and provide a more detailed explanation for the 33% increase over the base service hours. Justify the additional hours with specific tasks and activities related to customizations, integration, model tuning, and lifecycle activities.\"\n",
      "    }\n",
      "  ],\n",
      "  \"improvement_suggestions\": [\n",
      "    {\n",
      "      \"area\": \"Estimation Justification\",\n",
      "      \"suggestion\": \"Provide a more detailed breakdown of the additional development cost and the increased hours, linking them to specific tasks and project requirements. Quantify the effort required for each customization, integration, model tuning, and lifecycle activity to justify the added cost and time.\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The quote requires revision. While the total price is within budget, the additional development cost and the increased hours need further justification. A more detailed breakdown of these aspects is necessary to ensure they align with the project scope and complexity.\"\n",
      "}\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Crew 1 output\n",
    "analysis_data: AnalysisOutput = analysis_result.pydantic\n",
    "\n",
    "# Kickoff Crew 2\n",
    "quote_crew = QuoteGenerationCrew(analysis_output=analysis_data)\n",
    "quote_result = quote_crew.crew().kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd2ce7",
   "metadata": {},
   "source": [
    "### SOW Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e847a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOW Generation Function\n",
    "\n",
    "# Define the SOW template\n",
    "SOW_TEMPLATE = dedent(\"\"\"\\\n",
    "    # Statement of Work\n",
    "\n",
    "    ## 1. Introduction\n",
    "    This Statement of Work (SOW) is made and entered into as of {today_date}, by and between {provider_name} (\"Provider\"), located at {provider_address}, and {client_company} (\"Client\"). This document outlines the scope, deliverables, timeline, and costs for the {project_name} project.\n",
    "\n",
    "    ## 2. Project Overview & Objectives\n",
    "    **Project Name:** {project_name} <br>\n",
    "    **Main Objective:** {main_objective} <br>\n",
    "    **Project Summary:**\n",
    "    *{project_summary}*\n",
    "\n",
    "    ## 3. Scope of Work\n",
    "    ### 3.1. In-Scope Services & Deliverables\n",
    "    The following services, based on the approved quote (ID: {quote_id}), are included:\n",
    "    {line_items_markdown}\n",
    "\n",
    "    Key features to be delivered include:\n",
    "    {features_markdown}\n",
    "\n",
    "    {customization_notes_section}\n",
    "\n",
    "    ### 3.2. Out-of-Scope\n",
    "    The following are explicitly excluded:\n",
    "    *   Ongoing maintenance beyond a standard 30-day warranty period post-completion.\n",
    "    *   Hardware procurement or setup.\n",
    "    *   Data acquisition or licensing costs unless specified in line items.\n",
    "    *   Training beyond basic handover documentation.\n",
    "    *   Work not explicitly listed in section 3.1.\n",
    "\n",
    "    ## 4. Technical Specifications\n",
    "    *   **Key Technologies:**\n",
    "        {key_technologies}\n",
    "    *   **Data Sources:**\n",
    "        {data_sources_list}\n",
    "    *   **Performance Requirements:** Accuracy target: {accuracy_target}.\n",
    "    *   **Outputs:**\n",
    "        {outputs_list}\n",
    "\n",
    "    ## 5. Deliverables & Acceptance Criteria\n",
    "    Major deliverables correspond to the services listed in 3.1.\n",
    "    {acceptance_criteria_markdown}\n",
    "\n",
    "    ## 6. Project Timeline\n",
    "    *   **Estimated Start Date:** {start_date}\n",
    "    *   **Estimated Completion Date:** {completion_date}\n",
    "    *   **Total Estimated Effort:** Approximately {total_hours} hours.\n",
    "\n",
    "    ## 7. Roles & Responsibilities\n",
    "    *   **Provider ({provider_name}):** Project management, development, testing, deployment, documentation.\n",
    "    *   **Client ({client_company}):** Provide timely access to data sources ({data_sources_list}), provide subject matter expertise, assign a primary point of contact ({client_name}), participate in reviews, provide timely feedback/approvals.\n",
    "\n",
    "    ## 8. Pricing & Payment Schedule\n",
    "    *   **Total Project Price:** ${total_price:.2f}\n",
    "\n",
    "    ### 8.1. Price Breakdown\n",
    "    {price_breakdown_markdown}\n",
    "\n",
    "    ### 8.2. Payment Schedule\n",
    "    | Milestone         | Percentage | Amount      | Due Date     |\n",
    "    |-------------------|------------|-------------|--------------|\n",
    "    {payment_schedule_markdown}\n",
    "\n",
    "    ## 9. Assumptions\n",
    "    *   Client will provide access to specified data sources within 1 week of project start.\n",
    "    *   Client point of contact will be available for weekly check-ins and provide feedback within 2 business days.\n",
    "    *   Scope is fixed as defined in Section 3.1.\n",
    "    *   Data quality from provided sources is sufficient for model training.\n",
    "    *   {add_assumptions_from_research}\n",
    "\n",
    "    ## 10. Change Management\n",
    "    Any requested changes to scope, deliverables, or timeline must be submitted via a formal Change Request process. Approved changes may impact cost and schedule.\n",
    "\n",
    "    ## 11. Reporting\n",
    "    Provider will provide brief weekly status updates via email and hold bi-weekly review meetings with the Client.\n",
    "\n",
    "    ## 12. Disclaimer\n",
    "    Disclaimer: This Statement of Work was generated with AI assistance based on the approved quote (Quote ID: {quote_id}). It requires final human review and approval by both Provider and Client before execution.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def generate_sow(\n",
    "    quote_details: QuoteDetailsOutput,\n",
    "    analysis: AnalysisOutput,\n",
    "    provider_info: Dict[str, str],\n",
    ") -> str:\n",
    "    \"\"\"Generates the SOW Markdown document from structured data.\"\"\"\n",
    "\n",
    "    # Prepare template data\n",
    "    qs = quote_details.quote_summary\n",
    "    req = analysis.requirements\n",
    "    sa = analysis.service_analysis\n",
    "    res = analysis.research\n",
    "\n",
    "    details = qs.price_calculation_details\n",
    "    additional_development_cost = details.additional_development_cost\n",
    "    overhead_amount = details.overhead_amount\n",
    "    buffer_percentage_used = details.overhead_buffer_percentage\n",
    "    total_estimated_hours = details.total_estimated_hours\n",
    "\n",
    "    # Format Price Breakdown Markdown\n",
    "    price_breakdown_lines = []\n",
    "    price_breakdown_lines.append(\n",
    "        \"| Item                                      | Estimated Cost | Notes                                    |\"\n",
    "    )\n",
    "    price_breakdown_lines.append(\n",
    "        \"|-------------------------------------------|----------------|------------------------------------------|\"\n",
    "    )\n",
    "\n",
    "    # Use line_items for service names/base prices as source of truth for included services\n",
    "    for item in quote_details.line_items:\n",
    "        price_to_display = item.price\n",
    "        price_breakdown_lines.append(\n",
    "            f\"| Standard Service: {item.service_name:<20} | ${price_to_display:<13.2f} | Base price for standard offering       |\"\n",
    "        )\n",
    "\n",
    "    if additional_development_cost > 0:\n",
    "        price_breakdown_lines.append(\n",
    "            f\"| Additional Development & Lifecycle        | ${additional_development_cost:<13.2f} | Custom work, testing, PM, etc. ({total_estimated_hours} hrs) |\"\n",
    "        )\n",
    "\n",
    "    # Display buffer percentage used in calculation\n",
    "    if overhead_amount > 0:\n",
    "        price_breakdown_lines.append(\n",
    "            f\"| Overhead & Contingency ({buffer_percentage_used * 100:.0f}%)           | ${overhead_amount:<13.2f} | Project complexity & risk buffer       |\"\n",
    "        )\n",
    "    price_breakdown_lines.append(\n",
    "        \"|-------------------------------------------|----------------|------------------------------------------|\"\n",
    "    )\n",
    "    price_breakdown_lines.append(\n",
    "        f\"| **Total Estimated Project Price**         | **${qs.total_price:<11.2f}** |                                          |\"\n",
    "    )\n",
    "\n",
    "    price_breakdown_md = \"\\n\".join(price_breakdown_lines)\n",
    "\n",
    "    # Prepare other sections\n",
    "    line_items_md = \"\\n\".join(\n",
    "        [\n",
    "            f\"*   **{item.service_name}**: {item.description} (Base: ${item.price:.2f}, {item.estimated_hours} hrs)\"\n",
    "            for item in quote_details.line_items\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    payment_schedule_md = \"\\n\".join(\n",
    "        [\n",
    "            f\"| {milestone.milestone:<17} | {milestone.percentage:<10}% | ${milestone.amount:<11.2f} | {milestone.due_date:<12} |\"\n",
    "            for milestone in quote_details.payment_schedule\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    key_tech = (\n",
    "        res.best_practices_insights\n",
    "        if res.best_practices_insights\n",
    "        else \"Standard web/cloud technologies as appropriate\"\n",
    "    )\n",
    "\n",
    "    add_assumptions = (\n",
    "        f\"Potential challenges identified during research include: {res.potential_challenges}\"\n",
    "        if res.potential_challenges\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    # Populate Template\n",
    "    sow_content = SOW_TEMPLATE.format(\n",
    "        today_date=date.today().strftime(\"%Y-%m-%d\"),\n",
    "        provider_name=provider_info.get(\"name\", \"Acme Inc.\"),\n",
    "        provider_address=provider_info.get(\n",
    "            \"address\", \"123 Business Street, City, Country\"\n",
    "        ),\n",
    "        client_company=qs.client_company or \"N/A\",\n",
    "        project_name=qs.project_name,\n",
    "        main_objective=req.main_objective or \"N/A\",\n",
    "        project_summary=req.summary or \"\",\n",
    "        quote_id=qs.quote_id,\n",
    "        line_items_markdown=line_items_md,\n",
    "        features_markdown=\"\\n\".join(\n",
    "            [f\"*   {feature}\" for feature in req.features_needed]\n",
    "        ),\n",
    "        customization_notes_section=(\n",
    "            f\"*Includes custom development considerations for: {', '.join(sa.customization_notes)}*\"\n",
    "            if sa.customization_notes\n",
    "            else \"\"\n",
    "        ),\n",
    "        key_technologies=key_tech.strip(),\n",
    "        data_sources_list=\", \".join(req.data_sources) if req.data_sources else \"N/A\",\n",
    "        accuracy_target=req.technical_requirements.accuracy\n",
    "        if req.technical_requirements\n",
    "        else \"N/A\",\n",
    "        outputs_list=\", \".join(req.technical_requirements.outputs)\n",
    "        if req.technical_requirements and req.technical_requirements.outputs\n",
    "        else \"N/A\",\n",
    "        acceptance_criteria_markdown=\"\\n\".join(\n",
    "            [\n",
    "                f\"*   **{item.service_name} Deliverable**: Corresponds to service {item.service_name}.\\n\"\n",
    "                f\"    *   **Acceptance Criteria:** Successfully delivers functionality related to '{req.features_needed[i] if i < len(req.features_needed) else 'N/A'}' as described.\"\n",
    "                for i, item in enumerate(quote_details.line_items)\n",
    "            ]\n",
    "        ).strip(),\n",
    "        start_date=qs.estimated_start_date,\n",
    "        completion_date=qs.estimated_completion_date,\n",
    "        total_hours=qs.total_estimated_hours,\n",
    "        client_name=qs.client_name or \"N/A\",\n",
    "        total_price=qs.total_price,\n",
    "        price_breakdown_markdown=price_breakdown_md,\n",
    "        payment_schedule_markdown=payment_schedule_md,\n",
    "        add_assumptions_from_research=add_assumptions.strip(),\n",
    "    )\n",
    "\n",
    "    return sow_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3b2e6",
   "metadata": {},
   "source": [
    "### Execution example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_sow(quote_result: Any, analysis_data: AnalysisOutput) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generates the final Statement of Work (SOW) if the quote passes QA.\n",
    "\n",
    "    Args:\n",
    "        quote_result: The output object from the QuoteGenerationCrew.\n",
    "        analysis_data: The output object from the DataAnalysisCrew.\n",
    "\n",
    "    Returns:\n",
    "        The generated SOW as a markdown string, or None if QA failed or an error occurred.\n",
    "    \"\"\"\n",
    "    provider_details = {\n",
    "        \"name\": \"Acme Inc.\",\n",
    "        \"address\": \"123 Your Street, ABCity, EDCountry\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Ensure quote_result and its tasks_output are valid\n",
    "        if not (\n",
    "            quote_result\n",
    "            and hasattr(quote_result, \"tasks_output\")\n",
    "            and len(quote_result.tasks_output) == 2\n",
    "        ):\n",
    "            raise ValueError(\"Invalid quote_result structure.\")\n",
    "\n",
    "        quote_details_task = quote_result.tasks_output[0]\n",
    "        qa_output_task = quote_result.tasks_output[-1]\n",
    "\n",
    "        # Check task outputs against pydantic models\n",
    "        if not (\n",
    "            hasattr(qa_output_task, \"pydantic\")\n",
    "            and isinstance(qa_output_task.pydantic, QAReviewOutput)\n",
    "            and hasattr(quote_details_task, \"pydantic\")\n",
    "            and isinstance(quote_details_task.pydantic, QuoteDetailsOutput)\n",
    "        ):\n",
    "            raise ValueError(\"Unexpected output types from quote generation tasks.\")\n",
    "\n",
    "        # qa_data: QAReviewOutput = qa_output_task.pydantic\n",
    "        quote_data: QuoteDetailsOutput = quote_details_task.pydantic\n",
    "\n",
    "        # if qa_data.overall_status == \"approved\":\n",
    "        final_sow = generate_sow(\n",
    "            quote_details=quote_data,\n",
    "            analysis=analysis_data,\n",
    "            provider_info=provider_details,\n",
    "        )\n",
    "        return final_sow\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during SOW generation: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188ac48",
   "metadata": {},
   "source": [
    "### Final SOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d54ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Statement of Work\n",
       "\n",
       "## 1. Introduction\n",
       "This Statement of Work (SOW) is made and entered into as of 2025-04-15, by and between Acme Inc. (\"Provider\"), located at 123 Your Street, ABCity, EDCountry, and Wildfire Watch North America (\"Client\"). This document outlines the scope, deliverables, timeline, and costs for the Wildfire Monitoring AI - Phase 1 project.\n",
       "\n",
       "## 2. Project Overview & Objectives\n",
       "**Project Name:** Wildfire Monitoring AI - Phase 1 <br>\n",
       "**Main Objective:** Build a comprehensive database of past wildfire events and gather data suitable for fine-tuning an AI model for monitoring and predictive risk management of wildfires across Canada and the US. <br>\n",
       "**Project Summary:**\n",
       "*Wildfire Watch North America is seeking a geospatial AI solution for wildfire monitoring and prediction across Canada and the US. The project involves building a historical wildfire database, gathering data from various sources (satellite imagery, weather data, etc.), and developing an AI model to predict ignition risk and spread patterns. The desired outputs include a web dashboard, an API for system integration, and regular risk assessment reports. The budget for the initial phase is $40,000, with a focus on high accuracy (85-90%) in identifying high-risk zones and providing timely alerts.*\n",
       "\n",
       "## 3. Scope of Work\n",
       "### 3.1. In-Scope Services & Deliverables\n",
       "The following services, based on the approved quote (ID: Q-2025-04-15-001), are included:\n",
       "*   **GeoDataWrangling**: AI-Assisted Geospatial Data Wrangling (Base: $950.00, 28 hrs)\n",
       "*   **SatelliteImageAnalysis**: Satellite Image Analysis with AI (Base: $1200.00, 36 hrs)\n",
       "*   **SpatialPredictiveModeling**: Spatial Predictive Modeling (Base: $1500.00, 40 hrs)\n",
       "*   **ReactDashboard**: Geospatial React Dashboard (Base: $1400.00, 40 hrs)\n",
       "*   **AIIntegration**: AI Integration for Geospatial (Base: $900.00, 28 hrs)\n",
       "\n",
       "Key features to be delivered include:\n",
       "*   Near real-time monitoring capabilities\n",
       "*   Predict ignition risk\n",
       "*   Predict potential spread patterns\n",
       "*   Alert integration into existing systems\n",
       "\n",
       "*Includes custom development considerations for: Specific requirements for integrating the alert system into existing systems need to be defined., Custom model tuning and optimization may be required to achieve the desired accuracy (85-90%)., The format and content of the regular risk assessment reports need to be defined.*\n",
       "\n",
       "### 3.2. Out-of-Scope\n",
       "The following are explicitly excluded:\n",
       "*   Ongoing maintenance beyond a standard 30-day warranty period post-completion.\n",
       "*   Hardware procurement or setup.\n",
       "*   Data acquisition or licensing costs unless specified in line items.\n",
       "*   Training beyond basic handover documentation.\n",
       "*   Work not explicitly listed in section 3.1.\n",
       "\n",
       "## 4. Technical Specifications\n",
       "*   **Key Technologies:**\n",
       "    Utilize Google Earth Engine for efficient data retrieval and preprocessing. Employ TensorFlow or PyTorch for deep learning model development, considering U-Net architecture for semantic segmentation. Integrate ArcGIS for visualization and real-time data processing. Implement data augmentation techniques to improve model performance with limited data. Consider cloud-based solutions for deployment and scalability. Use TensorBoard for model monitoring and debugging.\n",
       "*   **Data Sources:**\n",
       "    MODIS satellite imagery, VIIRS satellite imagery, Landsat satellite imagery, Sentinel satellite imagery, Historical fire perimeter data, Weather data, Topographical data\n",
       "*   **Performance Requirements:** Accuracy target: Aiming for 85-90% precision in identifying high-risk zones.\n",
       "*   **Outputs:**\n",
       "    Web dashboard for analysts, API for integration, Regular risk assessment reports\n",
       "\n",
       "## 5. Deliverables & Acceptance Criteria\n",
       "Major deliverables correspond to the services listed in 3.1.\n",
       "*   **GeoDataWrangling Deliverable**: Corresponds to service GeoDataWrangling.\n",
       "    *   **Acceptance Criteria:** Successfully delivers functionality related to 'Near real-time monitoring capabilities' as described.\n",
       "*   **SatelliteImageAnalysis Deliverable**: Corresponds to service SatelliteImageAnalysis.\n",
       "    *   **Acceptance Criteria:** Successfully delivers functionality related to 'Predict ignition risk' as described.\n",
       "*   **SpatialPredictiveModeling Deliverable**: Corresponds to service SpatialPredictiveModeling.\n",
       "    *   **Acceptance Criteria:** Successfully delivers functionality related to 'Predict potential spread patterns' as described.\n",
       "*   **ReactDashboard Deliverable**: Corresponds to service ReactDashboard.\n",
       "    *   **Acceptance Criteria:** Successfully delivers functionality related to 'Alert integration into existing systems' as described.\n",
       "*   **AIIntegration Deliverable**: Corresponds to service AIIntegration.\n",
       "    *   **Acceptance Criteria:** Successfully delivers functionality related to 'N/A' as described.\n",
       "\n",
       "## 6. Project Timeline\n",
       "*   **Estimated Start Date:** 2025-04-22\n",
       "*   **Estimated Completion Date:** 2025-10-07\n",
       "*   **Total Estimated Effort:** Approximately 230 hours.\n",
       "\n",
       "## 7. Roles & Responsibilities\n",
       "*   **Provider (Acme Inc.):** Project management, development, testing, deployment, documentation.\n",
       "*   **Client (Wildfire Watch North America):** Provide timely access to data sources (MODIS satellite imagery, VIIRS satellite imagery, Landsat satellite imagery, Sentinel satellite imagery, Historical fire perimeter data, Weather data, Topographical data), provide subject matter expertise, assign a primary point of contact (Jane Smith), participate in reviews, provide timely feedback/approvals.\n",
       "\n",
       "## 8. Pricing & Payment Schedule\n",
       "*   **Total Project Price:** $33292.50\n",
       "\n",
       "### 8.1. Price Breakdown\n",
       "| Item                                      | Estimated Cost | Notes                                    |\n",
       "|-------------------------------------------|----------------|------------------------------------------|\n",
       "| Standard Service: GeoDataWrangling     | $950.00        | Base price for standard offering       |\n",
       "| Standard Service: SatelliteImageAnalysis | $1200.00       | Base price for standard offering       |\n",
       "| Standard Service: SpatialPredictiveModeling | $1500.00       | Base price for standard offering       |\n",
       "| Standard Service: ReactDashboard       | $1400.00       | Base price for standard offering       |\n",
       "| Standard Service: AIIntegration        | $900.00        | Base price for standard offering       |\n",
       "| Additional Development & Lifecycle        | $23000.00      | Custom work, testing, PM, etc. (230 hrs) |\n",
       "| Overhead & Contingency (15%)           | $4342.50       | Project complexity & risk buffer       |\n",
       "|-------------------------------------------|----------------|------------------------------------------|\n",
       "| **Total Estimated Project Price**         | **$33292.50   ** |                                          |\n",
       "\n",
       "### 8.2. Payment Schedule\n",
       "| Milestone         | Percentage | Amount      | Due Date     |\n",
       "|-------------------|------------|-------------|--------------|\n",
       "| Project Kick-off  | 30        % | $9987.75     | 2025-04-22   |\n",
       "| Mid-Project Review 1 | 30        % | $9987.75     | 2025-06-03   |\n",
       "| Mid-Project Review 2 | 30        % | $9987.75     | 2025-08-05   |\n",
       "| Project Completion & Handover | 10        % | $3329.25     | 2025-10-07   |\n",
       "\n",
       "## 9. Assumptions\n",
       "*   Client will provide access to specified data sources within 1 week of project start.\n",
       "*   Client point of contact will be available for weekly check-ins and provide feedback within 2 business days.\n",
       "*   Scope is fixed as defined in Section 3.1.\n",
       "*   Data quality from provided sources is sufficient for model training.\n",
       "*   Potential challenges identified during research include: Achieving the desired accuracy (85-90%) can be challenging due to data quality and model complexity. Ensuring timely alerts for new ignitions requires efficient real-time data processing and model deployment. Handling large datasets from multiple sources (satellite imagery, weather data, etc.) can be computationally intensive. Integrating different systems and APIs may present compatibility issues.\n",
       "\n",
       "## 10. Change Management\n",
       "Any requested changes to scope, deliverables, or timeline must be submitted via a formal Change Request process. Approved changes may impact cost and schedule.\n",
       "\n",
       "## 11. Reporting\n",
       "Provider will provide brief weekly status updates via email and hold bi-weekly review meetings with the Client.\n",
       "\n",
       "## 12. Disclaimer\n",
       "Disclaimer: This Statement of Work was generated with AI assistance based on the approved quote (Quote ID: Q-2025-04-15-001). It requires final human review and approval by both Provider and Client before execution.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the SOW\n",
    "final_sow = create_final_sow(quote_result, analysis_data)\n",
    "\n",
    "# Display as markdown\n",
    "Markdown(final_sow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
